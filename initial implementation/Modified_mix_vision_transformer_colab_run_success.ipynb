{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY48Ap7wK2Sn"
      },
      "source": [
        "PNEUMOTHORAX SEGMENTATION USING SIIM-ACR DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMmEp3gqUyn1",
        "outputId": "5baca48a-f8e3-401b-df9b-a57f49a6398b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y_qWc-LUIJCv"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/qubvel/segmentation_models.pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sEfIEmqfADNu"
      },
      "outputs": [],
      "source": [
        "# !pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ECBIUxVDADNv"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from monai.networks.nets.swin_unetr import SwinTransformer as SwinViT\n",
        "# from monai.utils import ensure_tuple_rep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7HFiESzDWcY"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CUKZcMbBK2Ss"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from collections import defaultdict\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score\n",
        "import torchvision.models as models # NEW MARCH-JUNE EDIT\n",
        "\n",
        "try:\n",
        "    from itertools import ifilterfalse\n",
        "except ImportError:  # py3k\n",
        "    from itertools import filterfalse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pzbBvorxADNw"
      },
      "outputs": [],
      "source": [
        "def init_seed(SEED=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Un-DgSH9ADNx"
      },
      "outputs": [],
      "source": [
        "init_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfcsl-YW_ck9"
      },
      "source": [
        "## Config - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-vx6RpZ8_bba"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE         = 1024\n",
        "WHOSE_DIR        = \"Neeshanth\"\n",
        "if WHOSE_DIR == \"Aathesh\":\n",
        "    DIR              = \"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\mask\")\n",
        "    DATA_FRAME_PATH  = \"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\RLE_kfold.csv\"\n",
        "elif WHOSE_DIR == \"Neeshanth\":\n",
        "    DIR              = \"/content/drive/MyDrive/datasets\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"/content/drive/MyDrive/datasets/train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"/content/drive/MyDrive/datasets/mask\")\n",
        "    DATA_FRAME_PATH  = \"/content/drive/MyDrive/datasets/RLE_kfold.csv\"\n",
        "elif WHOSE_DIR == \"wonder_boys\":\n",
        "    DIR              = \"/content/drive/MyDrive/Colab_Notebooks/datasets/main\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"/content/drive/MyDrive/Colab_Notebooks/datasets/main/train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"/content/drive/MyDrive/Colab_Notebooks/datasets/main/mask\")\n",
        "    DATA_FRAME_PATH  = \"/content/drive/MyDrive/Colab_Notebooks/datasets/main/RLE_kfold.csv\"\n",
        "\n",
        "KFOLD_PATH       = \"\"\n",
        "TRAIN_BATCH_SIZE = 2\n",
        "VALID_BATCH_SIZE = 2\n",
        "BATCH_SIZE       = 2\n",
        "EPOCHS           = 50\n",
        "# Path for pretrained model weights\n",
        "TRAINING_MODEL_PATH = \"\"\n",
        "USE_SAMPLER      = True\n",
        "POSTIVE_PERC     = 0.8\n",
        "DEVICE           = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PRETRAINED       = False # False means we're using ImageNet weights, so essentially, it's pretrained & never from scratch!!!!!\n",
        "LEARNING_RATE    = 0.0001\n",
        "NUM_WORKERS      = 8\n",
        "USE_CRIT         = True\n",
        "FOLD_ID          = 4\n",
        "EVAL_METRICS      = [\"metric - it calculates dice coefficient.\"]\n",
        "\n",
        "# Regularization Settings\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "L2_WEIGHT_DECAY  = 0.000005\n",
        "GRADIENT_CLIPPING = True\n",
        "GRADIENT_CLIPPING_THRESHOLD = 0.1\n",
        "\n",
        "# IF U DON'T WANT TO CHANGE Learning rate from previous experiment then set this to True.\n",
        "OPTIMIZER_LOAD = False\n",
        "# U MUST VERIFY IF LR IS GETTING SET PROPERLY FOR THE SCHEDULER IN THE \"OPTIMIZER & SCHEDULER\" SECTION OF THE NOTEBOOK.\n",
        "\n",
        "# Learning Rate Scheduler Settings\n",
        "SCHEDULER        = \"ReduceLROnPlateau\"\n",
        "if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "    SCHEDULER_PARAMS = {'factor': 0.1, 'patience': 2, 'threshold': 0.0000001, 'min_lr': 0.0000001}\n",
        "elif SCHEDULER == \"CosineAnnealingWarmRestarts\":\n",
        "    SCHEDULER_PARAMS = {'T_0': 1, 'T_mult': 2, 'eta_min': 0.00000001}\n",
        "elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "    SCHEDULER_PARAMS = {'T_max': 8, 'eta_min': 0.0000001}\n",
        "\n",
        "# Thresholds for 1024x1024 is there along with div by 2 values and div by 4 values\n",
        "TRIPLET_THRESHOLDS = [  [0.6, 500.0, 0.35], [0.67, 500.0, 0.37], [0.75, 500.0, 0.3],\n",
        "                        [0.75, 500.0, 0.4], [0.75, 1000.0, 0.3], [0.75, 1000.0, 0.4],\n",
        "                        [0.6, 1000.0, 0.3], [0.6, 1000.0, 0.4], [0.6, 1500.0, 0.3],\n",
        "                        [0.6, 1500.0, 0.4], [0.6, 250.0, 0.35], [0.67, 250.0, 0.37],\n",
        "                        [0.75, 250.0, 0.3], [0.75, 250.0, 0.4], [0.75, 500.0, 0.3],\n",
        "                        [0.75, 500.0, 0.4], [0.6, 500.0, 0.3], [0.6, 500.0, 0.4],\n",
        "                        [0.6, 750.0, 0.3], [0.6, 750.0, 0.4], [0.6, 1000, 0.35],\n",
        "                        [0.67, 1000, 0.37], [0.75, 1000, 0.3], [0.75, 1000, 0.4],\n",
        "                        [0.75, 2000, 0.3], [0.75, 2000, 0.4], [0.6, 2000, 0.3],\n",
        "                        [0.6, 2000, 0.4], [0.6, 3000, 0.3], [0.6, 3000, 0.4]       ]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    \"-------------------------------SAVING losses & metrics-------------------------------------\"\n",
        "\"\"\"\n",
        "ACCOUNT         = \"Neeshanth\" # \"Neeshanth\" or \"wonder_boys\" or others\n",
        "PURPOSE         = \"Project-2\" # training/inference/hyperparameter-tuning or hpt OR ANYTHING MORE SPECIFIC\n",
        "EXP_NO          = \"1\" # hpt experiment with changed settings\n",
        "PHASE           = \"16_02_2025\" # Just date\n",
        "\n",
        "EFFECTIVE_BATCH_SIZE = 2 # accumulation_steps * BATCH_SIZE\n",
        "\n",
        "\n",
        "if ACCOUNT == \"Neeshanth\":\n",
        "    # Save Config.txt\n",
        "    CONFIG_FILE_LOC = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "if ACCOUNT == \"wonder_boys\":\n",
        "    # Save Config.txt file\n",
        "    CONFIG_FILE_LOC = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "if ACCOUNT == \"pc_aathesh\":\n",
        "    # Save Config.txt file\n",
        "    CONFIG_FILE_LOC = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\CHECKPOINTS\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_dice_score.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_all_loss_vals.csv\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class - CREATE A NEW FOLDER\n",
        "    model_checkpoint_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_best_checkpoint_thresholds.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A4XlffPf1jV"
      },
      "source": [
        "## Saving Config as .txt - TYPE ESSENTIAL DETAILS TO RECOVER THIS EXPERIMENT IN THE BELOW SNIPPET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB0Igy_Tf1jV"
      },
      "source": [
        "ESSENTIALS include current_notebook_loc_in_pc, previous_notebook_loc_in_pc, key_changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3likqSVNf1jV",
        "outputId": "2a39044a-4b6e-43e8-a9eb-ac93f41dc706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current IST date and time is: 04-04-2025 19:04:02\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# Define the IST timezone\n",
        "ist_timezone = pytz.timezone('Asia/Kolkata')\n",
        "\n",
        "# Get the current time in IST\n",
        "ist_time = datetime.now(ist_timezone)\n",
        "\n",
        "# Format the date and time to DD-MM-YYYY HH:MM:SS\n",
        "formatted_ist_time = ist_time.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "\n",
        "# Print the formatted IST time\n",
        "print(\"Current IST date and time is:\", formatted_ist_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bwNCwwVf1jV",
        "outputId": "4c4ea17b-ef11-459e-e964-154bcb89969f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyper-parameters have been saved to /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_1_16_02_2025/Config.txt\n"
          ]
        }
      ],
      "source": [
        "hyperparameters = {\n",
        "    'image_size': f\"{IMG_SIZE}x{IMG_SIZE}\",\n",
        "    'date': formatted_ist_time,\n",
        "    'account': ACCOUNT,\n",
        "    'purpose': PURPOSE,\n",
        "    'experiment_no': EXP_NO,\n",
        "    'key_changes': \"Path of current notebook:'C:/Users/nisha/Desktop/Research Project May-June/github rep/chxray/model1024_EXP2_retraining_from_0.8342_colab_bs_2.ipynb', Pretrained = 0.8342, OPTIMIZER_LOAD = False, IMG_SIZE = 1024, EARLY_STOPPING_PATIENCE = 12, epochs=50, NUM_WORKERS=0, GRADIENT_CLIPPING: 0.1, BATCH_SIZE=2, SCHEDULER='CosineAnnealingWarmRestarts', POSTIVE_PERC=0.6, LEARNING_RATE = 0.00001, L2_WEIGHT_DECAY  = 0.000005, eta_min=0.0000001\",\n",
        "    'checkpoint_used_loc': TRAINING_MODEL_PATH,\n",
        "    'is_optimizer_loaded': OPTIMIZER_LOAD,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'batch_size': EFFECTIVE_BATCH_SIZE,\n",
        "    'gradient_accumulation_steps': int(EFFECTIVE_BATCH_SIZE/BATCH_SIZE),\n",
        "    'num_epochs': EPOCHS,\n",
        "    'IsSamplerUsed': USE_SAMPLER,\n",
        "    'percentage_of_positive_samples': POSTIVE_PERC,\n",
        "    'optimizer': 'Adam',\n",
        "    'loss_function': 'ComboLoss',\n",
        "    'scheduler': SCHEDULER,\n",
        "    'scheduler_params': SCHEDULER_PARAMS,\n",
        "    'L2_Regularization_weight_decay':L2_WEIGHT_DECAY,\n",
        "    'early_stopping_patience': EARLY_STOPPING_PATIENCE,\n",
        "    'is_gradient_clipping_used': GRADIENT_CLIPPING,\n",
        "    'gradient_clipping_threshold': GRADIENT_CLIPPING_THRESHOLD,\n",
        "    'model': 'U-Net with ResNet34 encoder with scSE attention',\n",
        "    'training_phase': f\"Phase - {PHASE}\",\n",
        "    'fold_ID': FOLD_ID,\n",
        "    'GPU_name': torch.cuda.get_device_name(torch.cuda.current_device()),\n",
        "    'num_workers': NUM_WORKERS,\n",
        "    'weights_given_to_loss_functions': {'bce': 3, 'dice': 1, 'focal': 4},\n",
        "    'triplet_thresholds': TRIPLET_THRESHOLDS,\n",
        "    'evaluation_metrics': EVAL_METRICS,\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a formatted string\n",
        "hyperparameters_str = json.dumps(hyperparameters, indent=4)\n",
        "\n",
        "# Specify the file name\n",
        "file_name = CONFIG_FILE_LOC\n",
        "\n",
        "os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "# Write the string to a file\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(hyperparameters_str)\n",
        "\n",
        "print(f\"Hyper-parameters have been saved to {file_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xtZrBIJeLH_"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BjVO5UiC1z-h"
      },
      "outputs": [],
      "source": [
        "bce_losses = []\n",
        "dice_losses = []\n",
        "focal_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-GiUgprmeQ97"
      },
      "outputs": [],
      "source": [
        "eps = 1e-6\n",
        "\n",
        "\n",
        "def soft_dice_loss(outputs, targets, per_image=False, per_channel=False):\n",
        "    \"\"\"\n",
        "        If per_image = False, then the function calculates dice loss for a single image-mask pair.\n",
        "    \"\"\"\n",
        "    batch_size, n_channels = outputs.size(0), outputs.size(1)\n",
        "\n",
        "    eps = 1e-6\n",
        "    n_parts = 1\n",
        "    if per_image:\n",
        "        n_parts = batch_size\n",
        "    if per_channel:\n",
        "        n_parts = batch_size * n_channels\n",
        "\n",
        "    dice_target = targets.contiguous().view(n_parts, -1).float()\n",
        "    dice_output = outputs.contiguous().view(n_parts, -1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1)\n",
        "    loss = (1 - (2 * intersection + eps) / (union + eps)) # returns a tensor of size [8]\n",
        "    return loss.mean() # returns a tensor of size [1].\n",
        "\n",
        "def dice_metric(preds, trues, per_image=False, per_channel=False):\n",
        "    preds = preds.float()\n",
        "    return 1 - soft_dice_loss(preds, trues, per_image, per_channel)\n",
        "\n",
        "\n",
        "def jaccard(outputs, targets, per_image=False, non_empty=False, min_pixels=5):\n",
        "    batch_size = outputs.size()[0]\n",
        "    eps = 1e-3\n",
        "    if not per_image:\n",
        "        batch_size = 1\n",
        "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
        "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
        "    target_sum = torch.sum(dice_target, dim=1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    losses = 1 - (intersection + eps) / (torch.sum(dice_output + dice_target, dim=1) - intersection + eps)\n",
        "    if non_empty:\n",
        "        assert per_image == True\n",
        "        non_empty_images = 0\n",
        "        sum_loss = 0\n",
        "        for i in range(batch_size):\n",
        "            if target_sum[i] > min_pixels:\n",
        "                sum_loss += losses[i]\n",
        "                non_empty_images += 1\n",
        "        if non_empty_images == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return sum_loss / non_empty_images\n",
        "\n",
        "    return losses.mean()\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        dice_loss = soft_dice_loss(input, target, per_image=self.per_image)\n",
        "        dice_losses.append(dice_loss.item())\n",
        "        return dice_loss\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False, non_empty=False, apply_sigmoid=False,\n",
        "                 min_pixels=5):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "        self.non_empty = non_empty\n",
        "        self.apply_sigmoid = apply_sigmoid\n",
        "        self.min_pixels = min_pixels\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if self.apply_sigmoid:\n",
        "            input = torch.sigmoid(input)\n",
        "        return jaccard(input, target, per_image=self.per_image, non_empty=self.non_empty, min_pixels=self.min_pixels)\n",
        "\n",
        "class StableBCELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StableBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        bce_loss_with_logits = nn.BCEWithLogitsLoss(reduction='mean') # mean is taken across batches\n",
        "        bce_loss = bce_loss_with_logits(logits, target)\n",
        "        bce_losses.append(bce_loss.item())\n",
        "        return bce_loss # returns a tensor of size [1]\n",
        "\n",
        "\n",
        "class ComboLoss(nn.Module):\n",
        "    def __init__(self, weights, per_image=True, channel_weights=[1, 0.5, 0.5], channel_losses=None):\n",
        "        super().__init__()\n",
        "        self.weights = weights\n",
        "        self.bce = StableBCELoss()\n",
        "        self.dice = DiceLoss(per_image=True)\n",
        "        self.jaccard = JaccardLoss(per_image=True)\n",
        "        self.lovasz = LovaszLoss(per_image=per_image)\n",
        "        self.lovasz_sigmoid = LovaszLossSigmoid(per_image=per_image)\n",
        "        self.focal = FocalLoss2d()\n",
        "        self.mapping = {'bce': self.bce,\n",
        "                        'dice': self.dice,\n",
        "                        'focal': self.focal,\n",
        "                        'jaccard': self.jaccard,\n",
        "                        'lovasz': self.lovasz,\n",
        "                        'lovasz_sigmoid': self.lovasz_sigmoid}\n",
        "        self.expect_sigmoid = {'dice', 'focal', 'jaccard', 'lovasz_sigmoid'}\n",
        "        self.per_channel = {'dice', 'jaccard', 'lovasz_sigmoid'}\n",
        "        self.values = {}\n",
        "        self.channel_weights = channel_weights\n",
        "        self.channel_losses = channel_losses\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        loss = 0\n",
        "        weights = self.weights\n",
        "        sigmoid_input = torch.sigmoid(outputs)\n",
        "        for k, v in weights.items():\n",
        "            if not v:\n",
        "                continue\n",
        "            val = 0\n",
        "            if k in self.per_channel:\n",
        "                channels = targets.size(1)\n",
        "                for c in range(channels):\n",
        "                    if not self.channel_losses or k in self.channel_losses[c]:\n",
        "                        val += self.channel_weights[c] * self.mapping[k](sigmoid_input[:, c, ...] if k in self.expect_sigmoid else outputs[:, c, ...],\n",
        "                                               targets[:, c, ...])\n",
        "\n",
        "            else:\n",
        "                val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n",
        "\n",
        "            self.values[k] = val\n",
        "            loss += self.weights[k] * val\n",
        "        return loss.clamp(min=1e-5)\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    p = len(gt_sorted)\n",
        "    gts = gt_sorted.sum()\n",
        "    intersection = gts.float() - gt_sorted.float().cumsum(0)\n",
        "    union = gts.float() + (1 - gt_sorted).float().cumsum(0)\n",
        "    jaccard = 1. - intersection / union\n",
        "    if p > 1:  # cover 1-pixel case\n",
        "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
        "    return jaccard\n",
        "\n",
        "\n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
        "                    for log, lab in zip(logits, labels))\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "    if len(labels) == 0:\n",
        "        # only void pixels, the gradients should be 0\n",
        "        return logits.sum() * 0.\n",
        "    signs = 2. * labels.float() - 1.\n",
        "    errors = (1. - logits * Variable(signs))\n",
        "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
        "    perm = perm.data\n",
        "    gt_sorted = labels[perm]\n",
        "    grad = lovasz_grad(gt_sorted)\n",
        "    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = scores.view(-1)\n",
        "    labels = labels.view(-1)\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = (labels != ignore)\n",
        "    vscores = scores[valid]\n",
        "    vlabels = labels[valid]\n",
        "    return vscores, vlabels\n",
        "\n",
        "\n",
        "def lovasz_sigmoid(probas, labels, per_image=False, ignore=None):\n",
        "    \"\"\"\n",
        "    Multi-class Lovasz-Softmax loss\n",
        "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n",
        "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
        "      only_present: average only on classes present in ground truth\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class labels\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_sigmoid_flat(*flatten_binary_scores(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
        "                          for prob, lab in zip(probas, labels))\n",
        "    else:\n",
        "        loss = lovasz_sigmoid_flat(*flatten_binary_scores(probas, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def lovasz_sigmoid_flat(probas, labels):\n",
        "    \"\"\"\n",
        "    Multi-class Lovasz-Softmax loss\n",
        "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
        "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
        "      only_present: average only on classes present in ground truth\n",
        "    \"\"\"\n",
        "    fg = labels.float()\n",
        "    errors = (Variable(fg) - probas).abs()\n",
        "    errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
        "    perm = perm.data\n",
        "    fg_sorted = fg[perm]\n",
        "    loss = torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))\n",
        "    return loss\n",
        "\n",
        "def symmetric_lovasz(outputs, targets, ):\n",
        "    return (lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1 - targets)) / 2\n",
        "\n",
        "def mean(l, ignore_nan=False, empty=0):\n",
        "    \"\"\"\n",
        "    nanmean compatible with generators.\n",
        "    \"\"\"\n",
        "    l = iter(l)\n",
        "    if ignore_nan:\n",
        "        l = ifilterfalse(np.isnan, l)\n",
        "    try:\n",
        "        n = 1\n",
        "        acc = next(l)\n",
        "    except StopIteration:\n",
        "        if empty == 'raise':\n",
        "            raise ValueError('Empty mean')\n",
        "        return empty\n",
        "    for n, v in enumerate(l, 2):\n",
        "        acc += v\n",
        "    if n == 1:\n",
        "        return acc\n",
        "    return acc / n\n",
        "\n",
        "\n",
        "class LovaszLoss(nn.Module):\n",
        "    def __init__(self, ignore_index=255, per_image=True):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        return symmetric_lovasz(outputs, targets)\n",
        "\n",
        "class LovaszLossSigmoid(nn.Module):\n",
        "    def __init__(self, ignore_index=255, per_image=True):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        return lovasz_sigmoid(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n",
        "\n",
        "class FocalLoss2d(nn.Module):\n",
        "    def __init__(self, alpha=0.65, gamma=2,n_parts=BATCH_SIZE):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.n_parts = n_parts\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        n_parts = self.n_parts\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        eps = 1e-6\n",
        "        # non_ignored = targets.view(n_parts, -1) != self.ignore_index\n",
        "        targets = targets.view(n_parts, -1).float()\n",
        "        outputs = outputs.view(n_parts, -1)\n",
        "        # clamp just makes sure the values of the tensor is within the given range.\n",
        "        outputs = torch.clamp(outputs, eps, 1. - eps)\n",
        "        targets = torch.clamp(targets, eps, 1. - eps)\n",
        "        \"\"\" pt = predicted probability for the true class\n",
        "        when targets = 1, pt = outputs which means pt now has the predicted probability of positive class.\n",
        "        when tagets = 0, pt = 1 - outputs which means pt has the predicted probability of negative class.\n",
        "        \"\"\"\n",
        "        pt = (1 - targets) * (1 - outputs) + targets * outputs\n",
        "        alpha_t = self.alpha * targets + (1 - self.alpha)*(1 - targets)\n",
        "        pt_proc = -(alpha_t*((1. - pt) ** self.gamma * torch.log(pt))) # torch.log is natural log\n",
        "        focal_loss = pt_proc.mean(dim=1).mean()\n",
        "        focal_losses.append(focal_loss.item())\n",
        "        return focal_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-y8ovXEK2St"
      },
      "source": [
        "## Config - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SW4ViKfK2St"
      },
      "source": [
        "The [ComboLoss](https://github.com/sneddy/pneumothorax-segmentation/blob/master/unet_pipeline/Losses.py#L104) function used in CRITERION below also comes from the winning solution by Anuar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "g3__XL68K2St"
      },
      "outputs": [],
      "source": [
        "CRITERION        = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})\n",
        "\n",
        "# Use During Inference Stage to store images of predicted segmentation masks\n",
        "# PREDICTION_PATH  = \"/content/drive/MyDrive/Colab_Notebooks/datasets/archive_png_siim_acr/Predicted_masks/tests\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od_pZDUoK2Su"
      },
      "source": [
        "\n",
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JuSwxSqK2Su"
      },
      "source": [
        "General utility functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3LcOlxiwK2Sv"
      },
      "outputs": [],
      "source": [
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    fig,ax = plt.subplots(figsize=(10,6))\n",
        "    ax.imshow(img.permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LkS16buBK2Sv"
      },
      "outputs": [],
      "source": [
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    images = {k:v.numpy() for k,v in images.items() if isinstance(v, torch.Tensor)} #convert tensor to numpy\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    image, mask = images['image'], images['mask']\n",
        "    plt.imshow(image.transpose(1,2,0), vmin=0, vmax=1)\n",
        "    if mask.max()>0:\n",
        "        plt.imshow(mask.squeeze(0), alpha=0.25)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5KOb07Zlb3GN"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"Saving checkpoint...\")\n",
        "  torch.save(state, filename)\n",
        "  print(\"Checkpoint saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cIcBkgjOcPWQ",
        "outputId": "b22bddc0-4071-49cd-f7d0-beecd1470269"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef load_checkpoint(checkpoint):\\n  print(\"Loading checkpoint...\")\\n  model.load_state_dict(checkpoint[\\'state_dict\\'])\\n  optimizer.load_state_dict(checkpoint[\\'optimizer\\'])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\"\"\"\n",
        "def load_checkpoint(checkpoint):\n",
        "  print(\"Loading checkpoint...\")\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NrindR018hyO"
      },
      "outputs": [],
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item() # Calculates where two tensors are equal\n",
        "  acc = (correct / len(y_pred) ) * 100\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqxMQpqB5JFV"
      },
      "source": [
        "# ---------------------- DL Workflow -----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLEHf3QK2Sv"
      },
      "source": [
        "## 1. Data -> Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wbgxfpGK2Sv"
      },
      "source": [
        "### Create five-fold splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDyy5r93K2Sv",
        "outputId": "78e6a6c1-a128-4f96-d917-62a2b918afdc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8570, 2142)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# single fold training for now, rerun notebook to train for multi-fold\n",
        "DF       = pd.read_csv(DATA_FRAME_PATH)\n",
        "TRAIN_DF = DF.query(f'kfold!={FOLD_ID}').reset_index(drop=True)\n",
        "VAL_DF   = DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)\n",
        "len(TRAIN_DF), len(VAL_DF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njKNWpqFK2Sw"
      },
      "source": [
        "### Dataset and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9ywRGbXTK2Sw"
      },
      "outputs": [],
      "source": [
        "class Dataset():\n",
        "    def __init__(self, rle_df, image_base_dir, masks_base_dir, augmentation=None, mask_augmentation=None):\n",
        "        self.df                 = rle_df\n",
        "        self.image_base_dir     = image_base_dir\n",
        "        self.masks_base_dir     = masks_base_dir\n",
        "        self.image_ids          = rle_df.ImageId.values\n",
        "        self.augmentation       = augmentation\n",
        "        self.mask_augmentation  = mask_augmentation\n",
        "\n",
        "    def __image_ids__(self):\n",
        "        print(print(self.image_ids))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image_id  = self.image_ids[i]\n",
        "        img_path  = os.path.join(self.image_base_dir, Path(image_id+'.png'))\n",
        "        mask_path = os.path.join(self.masks_base_dir, Path(image_id+'.png'))\n",
        "        # image is grayscale but it is loaded as RGB for compatibility with model, data augmentations, loss function, etc.\n",
        "        image     = cv2.imread(img_path, 1)\n",
        "        # mask is loaded as grayscale then converted to a binary form but has 253/255, 254/255, etc values.\n",
        "        mask      = cv2.imread(mask_path, 0)\n",
        "\n",
        "        \"\"\"Normalizing input image and mask\"\"\"\n",
        "        image = (image / 255.0).astype(np.float32)\n",
        "        mask = (mask > 0).astype(np.float32)\n",
        "\n",
        "        # apply augmentations - DON'T APPLY ANY TRANSFORMATION TO ALREADY NORMALIZED MASK OTHER THAN ToTensorV2\n",
        "        if self.augmentation and self.mask_augmentation:\n",
        "            img_sample = {\"image\": image}\n",
        "            img_sample = self.augmentation(**img_sample)\n",
        "            image = img_sample['image']\n",
        "            mask_sample = {\"image\": mask}\n",
        "            mask_sample = self.mask_augmentation(**mask_sample)\n",
        "            mask = mask_sample[\"image\"]\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'mask' : mask,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxv5It8ZK2Sw",
        "outputId": "e3e76228-93f1-4c85-dfa8-a96dc9b18aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-4377b2a74077>:3: UserWarning: Argument(s) 'always_apply' are not valid for transform Resize\n",
            "  albu.Resize(height=IMG_SIZE, width=IMG_SIZE, always_apply=True, p=1),\n",
            "<ipython-input-21-4377b2a74077>:19: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform\n",
            "  albu.ElasticTransform(alpha=120, sigma=6.0, alpha_affine=None, p=0.5),\n",
            "<ipython-input-21-4377b2a74077>:21: UserWarning: Argument(s) 'shift_limit' are not valid for transform OpticalDistortion\n",
            "  albu.OpticalDistortion(distort_limit=(-2, 2), shift_limit=(-0.5, 0.5), p=0.5),\n",
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n",
            "<ipython-input-21-4377b2a74077>:38: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform\n",
            "  albu.ElasticTransform(alpha=120, sigma=6.0, alpha_affine=None, p=0.5),\n",
            "<ipython-input-21-4377b2a74077>:40: UserWarning: Argument(s) 'shift_limit' are not valid for transform OpticalDistortion\n",
            "  albu.OpticalDistortion(distort_limit=(-2, 2), shift_limit=(-0.5, 0.5), p=0.5),\n"
          ]
        }
      ],
      "source": [
        "# Test transforms\n",
        "TEST_TFMS = albu.Compose([\n",
        "    albu.Resize(height=IMG_SIZE, width=IMG_SIZE, always_apply=True, p=1),\n",
        "    albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
        "    ToTensorV2(),\n",
        "],\n",
        "    is_check_shapes=True,\n",
        "    p=1.0,\n",
        ")\n",
        "\n",
        "# New Train Transforms - Aggressive Augmentations to avoid overfitting.\n",
        "TFMS =  albu.Compose(\n",
        "    [\n",
        "        albu.OneOf([\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.OneOf([\n",
        "            albu.ElasticTransform(alpha=120, sigma=6.0, alpha_affine=None, p=0.5),\n",
        "            albu.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), p=0.5),\n",
        "            albu.OpticalDistortion(distort_limit=(-2, 2), shift_limit=(-0.5, 0.5), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.5),\n",
        "        albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "        albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1.0),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    p=1.0\n",
        ")\n",
        "\n",
        "MASK_TFMS =  albu.Compose(\n",
        "    [\n",
        "        albu.OneOf([\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.OneOf([\n",
        "            albu.ElasticTransform(alpha=120, sigma=6.0, alpha_affine=None, p=0.5),\n",
        "            albu.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), p=0.5),\n",
        "            albu.OpticalDistortion(distort_limit=(-2, 2), shift_limit=(-0.5, 0.5), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.5),\n",
        "        albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    p=1.0\n",
        ")\n",
        "\n",
        "mask_transform = albu.Compose([\n",
        "    albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "    ToTensorV2()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4DKpUR1QK2Sw"
      },
      "outputs": [],
      "source": [
        "# train dataset\n",
        "train_dataset = Dataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TFMS, MASK_TFMS)\n",
        "val_dataset   = Dataset(VAL_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TEST_TFMS, mask_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zno_0Q4eVHv",
        "outputId": "7e74e75e-eb4f-47b1-e45a-f361274b8c5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8570"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "train_dataset.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8SW1GpbQK2Sw",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# # plot one with mask\n",
        "# visualize(**train_dataset[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-dcysMmK2Sw"
      },
      "source": [
        "### Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Qx49nrt4K2Sw"
      },
      "outputs": [],
      "source": [
        "class PneumoSampler(Sampler):\n",
        "    def __init__(self, train_df, positive_perc=0.8):\n",
        "        assert positive_perc > 0, 'percentage of positive pneumothorax images must be greater then zero'\n",
        "        self.train_df = train_df\n",
        "        self.positive_perc = positive_perc\n",
        "        self.positive_idxs = self.train_df.query('has_mask==1').index.values\n",
        "        self.negative_idxs = self.train_df.query('has_mask!=1').index.values\n",
        "        self.n_positive = len(self.positive_idxs)\n",
        "        self.n_negative = int(self.n_positive * (1 - self.positive_perc) / self.positive_perc)\n",
        "\n",
        "    def __iter__(self):\n",
        "        negative_sample = np.random.choice(self.negative_idxs, size=self.n_negative)\n",
        "        shuffled = np.random.permutation(np.hstack((negative_sample, self.positive_idxs)))\n",
        "        return iter(shuffled.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_positive + self.n_negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kzJerUgtK2Sx"
      },
      "outputs": [],
      "source": [
        "SAMPLER = PneumoSampler(TRAIN_DF, positive_perc=POSTIVE_PERC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asd-jMF9K2Sx"
      },
      "source": [
        "### DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Saav3bv6K2Sx",
        "outputId": "01f8f417-0729-4f0e-9162-8e0a8817c51d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE,\n",
        "                              shuffle=True if not USE_SAMPLER else False,\n",
        "                              num_workers=NUM_WORKERS,\n",
        "                              sampler=SAMPLER if USE_SAMPLER else None)\n",
        "val_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9nGVmHlj6z5",
        "outputId": "c2b170c3-af27-40b0-b713-1985e931f9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7a32a1923a90>, <torch.utils.data.dataloader.DataLoader object at 0x7a32a215b510>)\n",
            "Length of train dataloader: 1190 batches of 2\n",
            "length of validation dataloader: 1071 batches of 2\n"
          ]
        }
      ],
      "source": [
        "print(f\"Dataloaders: {train_dataloader , val_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"length of validation dataloader: {len(val_dataloader)} batches of {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jmBiDQJwFTN"
      },
      "source": [
        "for batch_index, data in enumerate(train_dataloader):\n",
        "    for z in range(3):\n",
        "        print(\"Image: \", data['image'].shape)\n",
        "        print(\"Mask: \", data['mask'].shape)\n",
        "        print(\"data['mask'].unsqueeze(1)\", data['mask'].unsqueeze(1).shape)\n",
        "    break\n",
        "\n",
        "Output\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIIdw0tLK2S4"
      },
      "source": [
        "## 2. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20fGhStqikfl",
        "outputId": "aac7430f-847c-46f7-804c-fb3e0271a41b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "import math\n",
        "\n",
        "##############################\n",
        "# Advanced Modules & Utilities\n",
        "##############################\n",
        "\n",
        "class DynamicOverlapPatchEmbed(nn.Module):\n",
        "    def __init__(self, in_channels, embed_dim, kernel_size, stride, padding):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.norm = nn.LayerNorm(embed_dim, eps=1e-05)\n",
        "        # Use the embed_dim here instead of in_channels.\n",
        "        self.attn_branch = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(embed_dim, embed_dim, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, H, W]\n",
        "        # First project to embed_dim\n",
        "        x_proj = self.proj(x)  # [B, embed_dim, H_out, W_out]\n",
        "        # Use projected features for attention branch.\n",
        "        attn = self.attn_branch(x_proj)  # [B, embed_dim, 1, 1]\n",
        "        x_proj = x_proj * attn          # dynamic modulation\n",
        "        B, C, H, W = x_proj.shape\n",
        "        x_proj = x_proj.flatten(2).transpose(1, 2)  # [B, N, C]\n",
        "        x_proj = self.norm(x_proj)\n",
        "        return x_proj, (H, W)\n",
        "\n",
        "class WindowedGatedAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=2, window_size=16, attn_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.q = nn.Linear(dim, dim, bias=True)\n",
        "        self.kv = nn.Linear(dim, dim * 2, bias=True)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim, bias=True)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.gate = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, N, dim] where N is number of tokens\n",
        "        B, N, C = x.shape\n",
        "        # Assume tokens can be rearranged into a square grid.\n",
        "        H = W = int(math.sqrt(N))\n",
        "        assert H * W == N, \"Input token count is not a perfect square.\"\n",
        "\n",
        "        # Save original H and W to remove padding later\n",
        "        orig_H, orig_W = H, W\n",
        "\n",
        "        # Pad so that H and W are divisible by window_size\n",
        "        new_H = math.ceil(H / self.window_size) * self.window_size\n",
        "        new_W = math.ceil(W / self.window_size) * self.window_size\n",
        "        pad_H = new_H - H\n",
        "        pad_W = new_W - W\n",
        "\n",
        "        # Reshape tokens to grid: [B, H, W, C]\n",
        "        x_grid = x.reshape(B, H, W, C)\n",
        "        if pad_H > 0 or pad_W > 0:\n",
        "            # Pad last two dimensions (height and width)\n",
        "            # F.pad expects padding in the form (padW_left, padW_right, padH_top, padH_bottom)\n",
        "            x_grid = F.pad(x_grid, (0, 0, 0, pad_W, 0, pad_H))\n",
        "            H, W = new_H, new_W\n",
        "\n",
        "        # Split into windows: shape [B, num_windows_h, num_windows_w, window_size, window_size, C]\n",
        "        x_windows = x_grid.unfold(1, self.window_size, self.window_size).unfold(2, self.window_size, self.window_size)\n",
        "        num_windows_h, num_windows_w = x_windows.shape[1], x_windows.shape[2]\n",
        "        num_windows = num_windows_h * num_windows_w\n",
        "        x_windows = x_windows.contiguous().view(B * num_windows, self.window_size * self.window_size, C)\n",
        "\n",
        "        # Compute attention within each window.\n",
        "        q = self.q(x_windows).reshape(B * num_windows, -1, self.num_heads, C // self.num_heads)\n",
        "        q = q.permute(0, 2, 1, 3)  # [B*num_windows, heads, tokens_window, head_dim]\n",
        "        kv = self.kv(x_windows).reshape(B * num_windows, -1, 2, self.num_heads, C // self.num_heads)\n",
        "        k, v = kv[:,:,0], kv[:,:,1]\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B*num_windows, heads, tokens_window, tokens_window]\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        attn = attn * torch.sigmoid(self.gate)\n",
        "\n",
        "        out = attn @ v  # [B*num_windows, heads, tokens_window, head_dim]\n",
        "        out = out.transpose(1, 2).reshape(B * num_windows, self.window_size * self.window_size, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "\n",
        "        # Merge windows back to grid shape.\n",
        "        out = out.view(B, num_windows_h, num_windows_w, self.window_size, self.window_size, C)\n",
        "        out = out.permute(0,1,3,2,4,5).contiguous().view(B, H, W, C)\n",
        "        # Remove any padding and reshape back to tokens.\n",
        "        out = out[:, :orig_H, :orig_W, :].reshape(B, orig_H * orig_W, C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ModifiedBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4.0, drop=0.0, drop_path=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim, eps=1e-06)\n",
        "        self.attn = WindowedGatedAttention(dim, num_heads=num_heads)\n",
        "        self.drop_path = nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim, eps=1e-06)\n",
        "        hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class DynamicTokenPruning(nn.Module):\n",
        "    \"\"\"Soft token reweighting to reduce redundant tokens.\"\"\"\n",
        "    def __init__(self, dim, prune_ratio=0.2):\n",
        "        super().__init__()\n",
        "        self.prune_ratio = prune_ratio\n",
        "        self.score_fc = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, N, dim]\n",
        "        scores = self.score_fc(x).squeeze(-1)  # [B, N]\n",
        "        scores_norm = torch.sigmoid(scores)     # values in (0,1)\n",
        "        x = x * scores_norm.unsqueeze(-1)        # reweight tokens\n",
        "        return x, scores_norm\n",
        "\n",
        "class HierarchicalFusion(nn.Module):\n",
        "    \"\"\"Fuse high-resolution and low-resolution features via attention.\"\"\"\n",
        "    def __init__(self, high_dim, low_dim, fusion_dim):\n",
        "        super().__init__()\n",
        "        self.high_proj = nn.Conv2d(high_dim, fusion_dim, kernel_size=1)\n",
        "        self.low_proj = nn.Conv2d(low_dim, fusion_dim, kernel_size=1)\n",
        "        self.fuse_attn = nn.Sequential(\n",
        "            nn.Conv2d(fusion_dim * 2, fusion_dim, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(fusion_dim, fusion_dim, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, high_feat, low_feat):\n",
        "        # high_feat: [B, high_dim, H, W], low_feat: [B, low_dim, h, w]\n",
        "        low_feat_up = F.interpolate(low_feat, size=high_feat.shape[2:], mode='bilinear', align_corners=False)\n",
        "        high_proj = self.high_proj(high_feat)\n",
        "        low_proj = self.low_proj(low_feat_up)\n",
        "        fusion_input = torch.cat([high_proj, low_proj], dim=1)\n",
        "        weight = self.fuse_attn(fusion_input)\n",
        "        fused = weight * high_proj + (1 - weight) * low_proj\n",
        "        return fused\n",
        "\n",
        "##############################\n",
        "# Encoder: Modified MixVision Transformer Encoder\n",
        "##############################\n",
        "\n",
        "class ModifiedMixVisionTransformerEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super().__init__()\n",
        "        # Use dynamic patch embeddings.\n",
        "        self.patch_embed1 = DynamicOverlapPatchEmbed(in_channels, 64, kernel_size=7, stride=4, padding=3)\n",
        "        self.patch_embed2 = DynamicOverlapPatchEmbed(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.patch_embed3 = DynamicOverlapPatchEmbed(128, 320, kernel_size=3, stride=2, padding=1)\n",
        "        self.patch_embed4 = DynamicOverlapPatchEmbed(320, 512, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Stage 1: Two ModifiedBlocks; then a LayerNorm.\n",
        "        self.block1 = nn.Sequential(\n",
        "            ModifiedBlock(dim=64, num_heads=2),\n",
        "            ModifiedBlock(dim=64, num_heads=2)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(64, eps=1e-06)\n",
        "\n",
        "        # Stage 2: Two blocks with increased dimension and heads.\n",
        "        self.block2 = nn.Sequential(\n",
        "            ModifiedBlock(dim=128, num_heads=4),\n",
        "            ModifiedBlock(dim=128, num_heads=4)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(128, eps=1e-06)\n",
        "\n",
        "        # Stage 3:\n",
        "        self.block3 = nn.Sequential(\n",
        "            ModifiedBlock(dim=320, num_heads=5),\n",
        "            ModifiedBlock(dim=320, num_heads=5)\n",
        "        )\n",
        "        self.norm3 = nn.LayerNorm(320, eps=1e-06)\n",
        "\n",
        "        # Stage 4:\n",
        "        self.block4 = nn.Sequential(\n",
        "            ModifiedBlock(dim=512, num_heads=8),\n",
        "            ModifiedBlock(dim=512, num_heads=8)\n",
        "        )\n",
        "        self.norm4 = nn.LayerNorm(512, eps=1e-06)\n",
        "\n",
        "        # Add a dynamic token pruning layer after stage 1.\n",
        "        self.token_prune = DynamicTokenPruning(dim=64, prune_ratio=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Stage 1: Patch embedding and transformer blocks.\n",
        "        x, (H1, W1) = self.patch_embed1(x)   # x: [B, N1, 64]\n",
        "        x, _ = self.token_prune(x)\n",
        "        x = self.block1(x)\n",
        "        feat1 = self.norm1(x)  # Feature from stage 1\n",
        "\n",
        "        # Reshape tokens to spatial map for stage 2.\n",
        "        x2_input = feat1.transpose(1, 2).reshape(-1, 64, H1, W1)\n",
        "        x2, (H2, W2) = self.patch_embed2(x2_input)  # [B, N2, 128]\n",
        "        x2 = self.block2(x2)\n",
        "        feat2 = self.norm2(x2)\n",
        "\n",
        "        x3_input = feat2.transpose(1, 2).reshape(-1, 128, H2, W2)\n",
        "        x3, (H3, W3) = self.patch_embed3(x3_input)  # [B, N3, 320]\n",
        "        x3 = self.block3(x3)\n",
        "        feat3 = self.norm3(x3)\n",
        "\n",
        "        x4_input = feat3.transpose(1, 2).reshape(-1, 320, H3, W3)\n",
        "        x4, (H4, W4) = self.patch_embed4(x4_input)  # [B, N4, 512]\n",
        "        x4 = self.block4(x4)\n",
        "        feat4 = self.norm4(x4)\n",
        "\n",
        "        # Reshape token features to spatial maps for return\n",
        "        B = x.shape[0]\n",
        "        feat1_map = feat1.transpose(1, 2).reshape(B, 64, H1, W1)\n",
        "        feat2_map = feat2.transpose(1, 2).reshape(B, 128, H2, W2)\n",
        "        feat3_map = feat3.transpose(1, 2).reshape(B, 320, H3, W3)\n",
        "        feat4_map = feat4.transpose(1, 2).reshape(B, 512, H4, W4)\n",
        "\n",
        "        # Return features from all stages for decoder fusion.\n",
        "        return [feat1_map, feat2_map, feat3_map, feat4_map]\n",
        "\n",
        "##############################\n",
        "# Decoder: Advanced Unet Decoder\n",
        "##############################\n",
        "\n",
        "class AdvancedUnetDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # We assume encoder channels: feat1: 64, feat2: 128, feat3: 320, feat4: 512.\n",
        "        self.fuse3 = HierarchicalFusion(high_dim=320, low_dim=512, fusion_dim=256)\n",
        "        self.fuse2 = HierarchicalFusion(high_dim=128, low_dim=256, fusion_dim=128)\n",
        "        self.fuse1 = HierarchicalFusion(high_dim=64, low_dim=128, fusion_dim=64)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)\n",
        "        self.upconv2 = nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n",
        "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, feats):\n",
        "        # feats: list of encoder feature maps reshaped as [B, C, H, W]\n",
        "        feat1, feat2, feat3, feat4 = feats\n",
        "\n",
        "        # Instead of assuming specific dimensions, use dynamic upsampling\n",
        "        # Upsample feat4 to match feat3's resolution and fuse\n",
        "        fuse3 = self.fuse3(feat3, F.interpolate(feat4, size=feat3.shape[2:], mode='bilinear', align_corners=False))\n",
        "        up3 = self.upconv3(fuse3)\n",
        "\n",
        "        # Fuse with feat2\n",
        "        fuse2 = self.fuse2(F.interpolate(feat2, size=up3.shape[2:], mode='bilinear', align_corners=False), up3)\n",
        "        up2 = self.upconv2(fuse2)\n",
        "\n",
        "        # Fuse with feat1\n",
        "        fuse1 = self.fuse1(F.interpolate(feat1, size=up2.shape[2:], mode='bilinear', align_corners=False), up2)\n",
        "        up1 = self.upconv1(fuse1)\n",
        "\n",
        "        out = self.out_conv(up1)\n",
        "        return out\n",
        "\n",
        "##############################\n",
        "# Final Unet Model\n",
        "##############################\n",
        "\n",
        "class AdvancedUnet(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super().__init__()\n",
        "        self.encoder = ModifiedMixVisionTransformerEncoder(in_channels=in_channels)\n",
        "        self.decoder = AdvancedUnetDecoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get encoder features (tokens) and their spatial dimensions\n",
        "        feat1, (H1, W1) = self.encoder.patch_embed1(x)\n",
        "        feat1 = self.encoder.token_prune(feat1)[0]  # Get just the features, not the scores\n",
        "        feat1 = self.encoder.block1(feat1)\n",
        "        feat1 = self.encoder.norm1(feat1)\n",
        "\n",
        "        x2_input = feat1.transpose(1, 2).reshape(-1, 64, H1, W1)\n",
        "        feat2, (H2, W2) = self.encoder.patch_embed2(x2_input)\n",
        "        feat2 = self.encoder.block2(feat2)\n",
        "        feat2 = self.encoder.norm2(feat2)\n",
        "\n",
        "        x3_input = feat2.transpose(1, 2).reshape(-1, 128, H2, W2)\n",
        "        feat3, (H3, W3) = self.encoder.patch_embed3(x3_input)\n",
        "        feat3 = self.encoder.block3(feat3)\n",
        "        feat3 = self.encoder.norm3(feat3)\n",
        "\n",
        "        x4_input = feat3.transpose(1, 2).reshape(-1, 320, H3, W3)\n",
        "        feat4, (H4, W4) = self.encoder.patch_embed4(x4_input)\n",
        "        feat4 = self.encoder.block4(feat4)\n",
        "        feat4 = self.encoder.norm4(feat4)\n",
        "\n",
        "        # Reshape token features to spatial maps using the actual dimensions\n",
        "        B = feat1.shape[0]\n",
        "        feat1_map = feat1.transpose(1, 2).reshape(B, 64, H1, W1)\n",
        "        feat2_map = feat2.transpose(1, 2).reshape(B, 128, H2, W2)\n",
        "        feat3_map = feat3.transpose(1, 2).reshape(B, 320, H3, W3)\n",
        "        feat4_map = feat4.transpose(1, 2).reshape(B, 512, H4, W4)\n",
        "\n",
        "        # Pass to decoder\n",
        "        out = self.decoder([feat1_map, feat2_map, feat3_map, feat4_map])\n",
        "\n",
        "        # If needed, resize the output to match input resolution\n",
        "        if out.shape[2] != x.shape[2] or out.shape[3] != x.shape[3]:\n",
        "            out = F.interpolate(out, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return out\n",
        "\n",
        "##############################\n",
        "# Example usage\n",
        "##############################\n",
        "\n",
        "model = AdvancedUnet(in_channels=3)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out = model(x)\n",
        "print(out.shape)  # Expected output shape: [1, 1, 224, 224] (segmentation mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzze2VhIK2S5"
      },
      "source": [
        "### Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "TbTO94ePK2S5"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.0001):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.mode = mode\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        self.BEST_MODEL_PATH = \"\"\n",
        "        if self.mode == \"min\":\n",
        "            self.val_score = np.inf\n",
        "        else:\n",
        "            self.val_score = -np.inf\n",
        "\n",
        "    def __call__(self, epoch, epoch_score, model, optimizer, loss, model_path):\n",
        "        if self.mode == \"min\":\n",
        "            score = -1.0 * epoch_score\n",
        "        else:\n",
        "            score = np.copy(epoch_score)\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch, epoch_score, model, optimizer, loss, model_path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(\n",
        "                \"EarlyStopping counter: {} out of {}\".format(\n",
        "                    self.counter, self.patience\n",
        "                )\n",
        "            )\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch, epoch_score, model, optimizer, loss, model_path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch, epoch_score, model, optimizer, loss, model_path):\n",
        "        model_path = Path(model_path)\n",
        "        parent = model_path.parent\n",
        "        os.makedirs(parent, exist_ok=True)\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
        "            print(\n",
        "                \"Validation score improved ({} --> {}). Model saved at {}!\".format(\n",
        "                    self.val_score, epoch_score, model_path\n",
        "                )\n",
        "            )\n",
        "            torch.save({\"Epoch\": epoch, \"Train Loss\": loss, \"Validation Dice Score\": self.val_score, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, model_path)\n",
        "            self.BEST_MODEL_PATH = model_path\n",
        "            print(f\"Checkpoint saved on epoch - {epoch} with dice score - {epoch_score}\")\n",
        "        self.val_score = epoch_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOlM9jTEK2S5"
      },
      "source": [
        "### Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_amog1GHK2S5"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "xbn3pKTaK2S5"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(train_loader, model, optimizer, loss_fn, accumulation_steps=int(EFFECTIVE_BATCH_SIZE/BATCH_SIZE), device='cuda'):\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # Lists to store batch-to-batch progress details within the epoch while training\n",
        "    batch_count_train = []\n",
        "    batch_train_loss = []\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    if accumulation_steps > 1:\n",
        "      optimizer.zero_grad()\n",
        "    tk0 = tqdm(train_loader, total=len(train_loader))\n",
        "    for b_idx, data in enumerate(tk0):\n",
        "      # print(data['image'].shape) -> torch.Size([8, 3, 512, 512])\n",
        "      # print(data['mask'].shape) -> torch.Size([8, 1, 512, 512])\n",
        "      if (b_idx + 1) % accumulation_steps == 0:\n",
        "        batch_count_train.append(b_idx)\n",
        "\n",
        "      # moves image tensor and mask tensor to gpu\n",
        "      for key, value in data.items():\n",
        "        data[key] = value.to(\"cuda\")\n",
        "\n",
        "      if accumulation_steps == 1 and b_idx == 0:\n",
        "        optimizer.zero_grad()\n",
        "      out  = model(data['image']) # out.shape = torch.Size([8, 1, 512, 512])\n",
        "      loss = loss_fn(out, data['mask'].float()) # mask.shape = torch.Size([8, 1, 512, 512])\n",
        "      with torch.set_grad_enabled(True):\n",
        "        loss.backward()\n",
        "        if (b_idx + 1) % accumulation_steps == 0:\n",
        "          if GRADIENT_CLIPPING:\n",
        "            clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING_THRESHOLD)\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "      losses.update(loss.item(), train_loader.batch_size)\n",
        "      if (b_idx + 1) % accumulation_steps == 0:\n",
        "        batch_train_loss.append(loss.item())\n",
        "      tk0.set_postfix(loss=losses.avg, learning_rate=optimizer.param_groups[0]['lr'])\n",
        "    return losses.avg, batch_count_train, batch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "QCUdPtqF-unk"
      },
      "outputs": [],
      "source": [
        "criterion = CRITERION\n",
        "es = EarlyStopping(patience=EARLY_STOPPING_PATIENCE, mode='max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKL6S336-unk",
        "outputId": "47417315-684d-4d0c-8a1d-57a5076aae7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ComboLoss(\n",
              "  (bce): StableBCELoss()\n",
              "  (dice): DiceLoss()\n",
              "  (jaccard): JaccardLoss()\n",
              "  (lovasz): LovaszLoss()\n",
              "  (lovasz_sigmoid): LovaszLossSigmoid()\n",
              "  (focal): FocalLoss2d()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K136b3eqK2S5"
      },
      "source": [
        "### Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RY2NxC-unk"
      },
      "source": [
        "#### Mask Binarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "V4my02Uf-unk"
      },
      "outputs": [],
      "source": [
        "class MaskBinarization():\n",
        "    def __init__(self):\n",
        "        self.thresholds = 0.5\n",
        "    def transform(self, predicted):\n",
        "        yield predicted > self.thresholds\n",
        "\n",
        "class SimpleMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, score_thresholds):\n",
        "        super().__init__()\n",
        "        self.thresholds = score_thresholds\n",
        "    def transform(self, predicted):\n",
        "        for thr in self.thresholds:\n",
        "            yield predicted > thr\n",
        "\n",
        "class DupletMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, duplets, with_channels=True):\n",
        "        super().__init__()\n",
        "        self.thresholds = duplets\n",
        "        self.dims = (2,3) if with_channels else (1,2)\n",
        "    def transform(self, predicted):\n",
        "        for score_threshold, area_threshold in self.thresholds:\n",
        "            mask = predicted > score_threshold\n",
        "            mask[mask.sum(dim=self.dims) < area_threshold] = 0\n",
        "            yield mask\n",
        "\n",
        "class TripletMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, triplets, with_channels=True):\n",
        "        super().__init__()\n",
        "        self.thresholds = triplets\n",
        "        self.dims = (2,3) if with_channels else (1,2) # dims should be HxW, basically it should ignore batch_size and no_of_channels in the general format of BxCxHxW\n",
        "    def transform(self, predicted):\n",
        "        for top_score_threshold, area_threshold, bottom_score_threshold in self.thresholds:\n",
        "            clf_mask = (predicted > top_score_threshold).float()\n",
        "            pred_mask = (predicted > bottom_score_threshold).float()\n",
        "            pred_mask[clf_mask.sum(dim=self.dims) < area_threshold] = 0\n",
        "            yield pred_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joGnfZgw-unl"
      },
      "source": [
        "#### Metric used in validation and evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "zlWOcwQdK2S5"
      },
      "outputs": [],
      "source": [
        "def metric(probability, truth):\n",
        "    if probability.shape[0] == truth.shape[0]: # checking for batch size mismatches in the code for image & mask\n",
        "        batch_size = probability.shape[0]\n",
        "    with torch.no_grad():\n",
        "        probability = probability.view(batch_size, -1) # probability's size is [8, 1*512*512]\n",
        "        truth = truth.view(batch_size, -1)             # truth's size is [8, 1*512*512]\n",
        "        assert(probability.shape == truth.shape)\n",
        "\n",
        "        p = probability.float() # prob_preds already comes in binarized.\n",
        "        t = (truth > 0.5).float()\n",
        "\n",
        "        t_sum = t.sum(-1) # t_sum size is 8 # Each value in the vector represents the sum of all pixels in one mask\n",
        "        p_sum = p.sum(-1) # p_sum size is 8 # Each value in the vector represents the sum of all elements in one pred_probs\n",
        "        neg_index = torch.nonzero(t_sum == 0) # indices of masks which are negative.\n",
        "        pos_index = torch.nonzero(t_sum >= 1) # indices of masks which are positive.\n",
        "\n",
        "        dice_neg = (p_sum == 0).float() # tensor of size 8\n",
        "        \"\"\"\n",
        "        if t_sum = torch.tensor([0.0, 1000.0, 0.0, 600.0, 720.0, 420.0, 0.0, 0.0]), then dice_neg = tensor([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0])\n",
        "        \"\"\"\n",
        "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1)) # tensor of size 8\n",
        "\n",
        "        dice_neg = dice_neg[neg_index] # selects elements of dice_neg acc to the indices in neg_index, it can have more than one element.\n",
        "        dice_pos = dice_pos[pos_index] # similar to the above code line.\n",
        "        dice = torch.cat([dice_pos, dice_neg])\n",
        "\n",
        "        num_neg = len(neg_index) # no. of negative masks in a batch\n",
        "        num_pos = len(pos_index) # no. of positive masks in a batch\n",
        "\n",
        "    return dice.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "u3ovO3f9XHQt"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_best_thresholds(epoch, b_idx, best_metric, best_threshold, filepath=Path(save_best_thresholds_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch,\n",
        "        'batch_number': b_idx,\n",
        "        'best_metric': best_metric,\n",
        "        'best_threshold': best_threshold\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "7opE5TuMRCvg"
      },
      "outputs": [],
      "source": [
        "epoch_count_thr = []\n",
        "batch_indices = []\n",
        "best_metrics_list = []\n",
        "best_thresholds_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ww6ftfKmK2S5"
      },
      "outputs": [],
      "source": [
        "def evaluate(valid_loader, model, epoch, device=DEVICE, metric=metric, loss_fn=criterion):\n",
        "    losses = AverageMeter()\n",
        "    combolosses = AverageMeter()\n",
        "    metrics = defaultdict(float)\n",
        "    # Lists to store batch-to-batch progress details within the epoch while training\n",
        "    batch_count_val = []\n",
        "    batch_val_loss_values = []\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    tk0 = tqdm(valid_loader, total=len(valid_loader))\n",
        "    with torch.inference_mode():\n",
        "        for b_idx, data in enumerate(tk0):\n",
        "          batch_count_val.append(b_idx)\n",
        "          for key, value in data.items():\n",
        "            data[key] = value.to(device)\n",
        "          out   = model(data['image']) # Raw model output\n",
        "          loss = loss_fn(out, data['mask']).cpu()\n",
        "          pred_probs   = torch.sigmoid(out) # Prediction probabilities, this is already done in ComboLoss's forward function.\n",
        "          # dice and jaccard expect prediction probabilities whereas bce expects logits (raw model outputs). It's mentioned inside the class too.\n",
        "          # This is a mistake, you can't pass prediction probability into a evaluation metric like this.\n",
        "          # You must pass the mask cuz u are comparing the actual mask(target) and the predicted mask not predicted probabilities.\n",
        "          # So, 1st apply mask binarization to the prediction probabilities.\n",
        "          binarizer_fn = TripletMaskBinarization(TRIPLET_THRESHOLDS)\n",
        "          mask_generator = binarizer_fn.transform(pred_probs)\n",
        "          used_thresholds = binarizer_fn.thresholds\n",
        "          # ------------------------------------- Tests Required ---------------------------------------------------------\n",
        "          for current_thr, current_mask in zip(used_thresholds, mask_generator):\n",
        "              current_metric = metric(current_mask, data['mask']).item()\n",
        "              current_thr = tuple(current_thr)\n",
        "              metrics[current_thr] = (metrics[current_thr] * b_idx + current_metric) / (b_idx + 1)\n",
        "\n",
        "          best_threshold = max(metrics, key=metrics.get)\n",
        "          best_metric = metrics[best_threshold]\n",
        "          tk0.set_description('score: {:.5f} on {}'.format(best_metric, best_threshold))\n",
        "          dice = best_metric\n",
        "\n",
        "          epoch_count_thr.append(epoch)\n",
        "          batch_indices.append(b_idx)\n",
        "          best_metrics_list.append(best_metric)\n",
        "          best_thresholds_list.append(best_threshold)\n",
        "          save_best_thresholds(epoch_count_thr, batch_indices, best_metrics_list, best_thresholds_list)\n",
        "\n",
        "          # if .item() is used then .cpu() is NOT required. .item() will itself return a float value.\n",
        "          losses.update(dice, valid_loader.batch_size)\n",
        "          combolosses.update(loss.item(), valid_loader.batch_size)\n",
        "          batch_val_loss_values.append(loss.item())\n",
        "          tk0.set_postfix(dice_score=losses.avg, val_loss=combolosses.avg)\n",
        "    return losses.avg, batch_count_val, batch_val_loss_values, combolosses.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE7TO5J9XHQt"
      },
      "source": [
        "### Optimizer & Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Fn0XAwlyK2S6"
      },
      "outputs": [],
      "source": [
        "if PRETRAINED:\n",
        "  checkpoint = torch.load(TRAINING_MODEL_PATH)\n",
        "  model.to(DEVICE)\n",
        "  model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay= L2_WEIGHT_DECAY)\n",
        "  if OPTIMIZER_LOAD:\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "  # Manually set the new learning rate for this stage of training as loading optimizer's state dict will\n",
        "  # load parameters that was there while saving the previous checkpoint but loading the optimizer's state dict is\n",
        "  # crucial\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = LEARNING_RATE\n",
        "\n",
        "  if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=SCHEDULER_PARAMS[\"factor\"], patience=SCHEDULER_PARAMS[\"patience\"], threshold=SCHEDULER_PARAMS[\"threshold\"], min_lr=SCHEDULER_PARAMS[\"min_lr\"])\n",
        "  elif SCHEDULER == \"CosineAnnealingWarmRestarts\":\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=SCHEDULER_PARAMS[\"T_0\"], T_mult=SCHEDULER_PARAMS[\"T_mult\"], eta_min=SCHEDULER_PARAMS[\"eta_min\"])\n",
        "  elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=SCHEDULER_PARAMS[\"T_max\"], eta_min=SCHEDULER_PARAMS[\"eta_min\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gK69cmOlRCvg"
      },
      "outputs": [],
      "source": [
        "if not PRETRAINED:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay= L2_WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=SCHEDULER_PARAMS[\"factor\"], patience=SCHEDULER_PARAMS[\"patience\"], threshold=SCHEDULER_PARAMS[\"threshold\"], min_lr=SCHEDULER_PARAMS[\"min_lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqlFcFDSRCvh",
        "outputId": "26783e6e-67a2-402c-95cb-c0bd9e4c17db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New learning rate: 0.0001\n"
          ]
        }
      ],
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "    print(f\"New learning rate: {param_group['lr']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ig7E3afMOi-6"
      },
      "outputs": [],
      "source": [
        "# initial_lr_scheduler = scheduler.base_lrs[0]  # Assuming a single learning rate for all parameters\n",
        "# print(\"Initial learning rate of scheduler:\", initial_lr_scheduler)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOkP2HYpvsqr"
      },
      "source": [
        "### GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ypne222KBjPe"
      },
      "outputs": [],
      "source": [
        "# # memory footprint support libraries/code\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "# def printm():\n",
        "#  process = psutil.Process(os.getpid())\n",
        "#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "# printm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtHHjC45vsqr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcACWXgAXHQu"
      },
      "source": [
        "#### Saving Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "AzIGIAe2FmJQ"
      },
      "outputs": [],
      "source": [
        "def store_batch_training_details(epoch, batch_count_train, batch_train_loss):\n",
        "  # Directory where you want to save the CSV file\n",
        "  directory = store_batch_training_details_path\n",
        "\n",
        "  # Ensure the directory exists\n",
        "  os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "  # File path\n",
        "  file_path = os.path.join(directory, name_of_batch_training_details_csv + f\"{epoch}.csv\")\n",
        "\n",
        "  # Write to CSV file\n",
        "  with open(file_path, \"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      # Write header\n",
        "      writer.writerow([\"Batch Count Train\", \"Batch Train Loss\"])\n",
        "      # Write data rows\n",
        "      for batch_count, train_loss in zip(batch_count_train, batch_train_loss):\n",
        "          writer.writerow([batch_count, train_loss])\n",
        "\n",
        "  print(f\"CSV file for epoch-{epoch}has been created at: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "f5-0shprvsqr"
      },
      "outputs": [],
      "source": [
        "def store_batch_validation_details(epoch, batch_count_val, batch_val_score_values):\n",
        "  # Directory where you want to save the CSV file\n",
        "  directory = store_batch_validation_details_path\n",
        "\n",
        "  # Ensure the directory exists\n",
        "  os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "  # File path\n",
        "  file_path = os.path.join(directory, name_of_batch_validation_details_csv + f\"{epoch}.csv\")\n",
        "\n",
        "  # Write to CSV file\n",
        "  with open(file_path, \"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      # Write header\n",
        "      writer.writerow([\"Batch Count Validation\", \"Batch Validation Loss\"])\n",
        "      # Write data rows\n",
        "      for batch_count, batch_val_score in zip(batch_count_val, batch_val_score_values):\n",
        "          writer.writerow([batch_count, batch_val_score])\n",
        "\n",
        "  print(f\"CSV file for validation, epoch-{epoch}has been created at: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ijkQ3VlU-unm"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_progress(epoch_count, loss_values, val_loss_values, filepath= Path(save_progress_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch_count,\n",
        "        'train_loss': loss_values,\n",
        "        'val_loss': [None] * (len(epoch_count) - len(val_loss_values)) + val_loss_values\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "PTogqmQgWkZC"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_dice_score(epoch_count, val_score_values, filepath= Path(save_dice_score_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch_count,\n",
        "        'val_dice_scores': [None] * (len(epoch_count) - len(val_score_values)) + val_score_values\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "ntFd7ioHXHQv"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_losses(bce_losses=bce_losses, dice_losses=dice_losses, focal_losses=focal_losses, filepath= Path(save_3losses_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'bce': bce_losses,\n",
        "        'dice': dice_losses,\n",
        "        'focal': focal_losses\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMFmkSOtXHQv"
      },
      "source": [
        "#### Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bPiRNBuK2S6",
        "outputId": "93493d88-837d-47d1-fb27-18e1d6a76cf4",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1190 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "  5%|▌         | 61/1190 [00:41<11:18,  1.66it/s, learning_rate=0.0001, loss=2.11]"
          ]
        }
      ],
      "source": [
        "till_epoch = EPOCHS\n",
        "model.to(DEVICE)\n",
        "model.train()\n",
        "# Lists to store epoch-to-epoch progress details\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "val_score_values = []\n",
        "val_loss_values = []\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, till_epoch+1):\n",
        "    epoch_count.append(epoch)\n",
        "    loss, batch_count_train, batch_train_loss = train_one_epoch(train_dataloader, model, optimizer, criterion)\n",
        "    loss_values.append(loss)\n",
        "    store_batch_training_details(epoch, batch_count_train, batch_train_loss)\n",
        "    if not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step()\n",
        "    # Storing Training details for plotting curves and inference\n",
        "    print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}\")\n",
        "\n",
        "    \"\"\"-------------------VALIDATION---------------------\"\"\"\n",
        "    dice, batch_count_val, batch_val_score_values, val_loss = evaluate(val_dataloader, model, epoch=epoch) # Evaluates model performance by calculating the dice coefficient\n",
        "    val_score_values.append(dice)\n",
        "    val_loss_values.append(val_loss)\n",
        "    # Storing Validation details for plotting curves and inference\n",
        "    store_batch_validation_details(epoch, batch_count_val, batch_val_score_values)\n",
        "    print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}, VAL DICE: {dice}\")\n",
        "\n",
        "    save_progress(epoch_count, loss_values, val_loss_values)\n",
        "    save_dice_score(epoch_count, val_score_values)\n",
        "    save_losses()\n",
        "# \"\"\"\" If it is necessary to save model's state dict as checkpoint, do the essential changes\n",
        "#      in Early Stopping class.\"\"\"\n",
        "\n",
        "    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step(dice)\n",
        "\n",
        "    es(epoch, dice, model, optimizer, loss, model_path= model_checkpoint_path + f\"_epoch{epoch}_bst_model{IMG_SIZE}_fold{FOLD_ID}_{np.round(dice,4)}.tar\")\n",
        "    if es.early_stop:\n",
        "        print('\\n\\n -------------- EARLY STOPPING -------------- \\n\\n')\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJFz9wAhzL4k"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgLZEbqwyAb4"
      },
      "source": [
        "20 mins for a single epoch of training and validation together with T4 GPU. If the no. of epoches is 50 and early stopping is 10, let's see how many hours it takes to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ptGWY9E2cn"
      },
      "source": [
        "Visualizations for the predictions!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KC5W67ovsqs"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")221d\n",
        "52\n",
        "# fig, ax = plt.subplots()awjk\n",
        "506\n",
        "132\n",
        "54\n",
        "# ax.plot(epoch_count, loss_values)\n",
        "# ax.set_title(\"Training Loss Curve [EPOCHS]\", fontsize=20)\n",
        "# ax.set_xlabel(\"epoch number\", fontsize=14)\n",
        "# ax.set_ylabel(\"loss value\", fontsize=14)\n",
        "# ax.tick_params(axis='both', labelsize=14)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8TB_fU0f1jh"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")\n",
        "# fig, ax = plt.subplots()\n",
        "\n",
        "# ax.plot(epoch_count, val_loss_values)\n",
        "# ax.set_title(\"Validation Loss Curve [EPOCHS]\", fontsize=20)\n",
        "# ax.set_xlabel(\"epoch number\", fontsize=14)\n",
        "# ax.set_ylabel(\"loss value\", fontsize=14)\n",
        "# ax.tick_params(axis='both', labelsize=14)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ2c6YzKSuFU"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(epoch_count, loss_values)\n",
        "# plt.plot(epoch_count, val_loss_values)\n",
        "# plt.title(\"Training and Validation Loss Curves\", fontsize=20)\n",
        "# plt.xlabel(\"epoch number\", fontsize=14)\n",
        "# plt.ylabel(\"loss value\", fontsize=14)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2BXTyFrPxzy"
      },
      "outputs": [],
      "source": [
        "# Load data from CSV into a DataFrame\n",
        "file_path = save_progress_path  # Replace with your CSV file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Plotting the line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['epoch'], df['train_loss'], marker='o', linestyle='-', color='b', label='Train Loss')\n",
        "plt.plot(df['epoch'], df['val_loss'], marker='o', linestyle='-', color='r', label='Validation Loss')\n",
        "\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOQs9Sy2VBHR"
      },
      "source": [
        "# Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_QmgkWSGPsu"
      },
      "outputs": [],
      "source": [
        "# checkpoint = torch.load(es.BEST_MODEL_PATH)\n",
        "# print(es.best_score)\n",
        "# model.to(DEVICE)\n",
        "# model.load_state_dict(checkpoint[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnM0qtplSwyW"
      },
      "outputs": [],
      "source": [
        "# model.to(DEVICE)\n",
        "# model.eval()\n",
        "# idx = 1995\n",
        "# mask = val_dataset[idx][\"mask\"].unsqueeze(0)\n",
        "# print(\"Mask Shape: \",mask.shape)\n",
        "# image = val_dataset[idx][\"image\"].unsqueeze(0)\n",
        "# print(\"Image Shape: \",image.shape)\n",
        "# raw_output = model(image.to(DEVICE))\n",
        "# print(\"raw output shape: \", raw_output.shape)\n",
        "# print(\"------ Raw Output ------\")\n",
        "# print(raw_output)\n",
        "# print(\"------- Pred Probs -------\")\n",
        "# pred_probs = torch.sigmoid(raw_output)\n",
        "# print(pred_probs)\n",
        "# print(\"------- How does the Mask look like? ------\")\n",
        "# print(mask)\n",
        "# print(\"-------- Predicted Segmentation mask --------\")\n",
        "# # binarizer_fn = TripletMaskBinarization(triplets=[[0.7, 600, 0.3]])\n",
        "# # mask = binarizer_fn.transform(mask).float()\n",
        "# # print(segmentation_mask)\n",
        "# segmentation_mask = (pred_probs > 0.4).float()\n",
        "# print(segmentation_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cALrPDiZFyhu"
      },
      "outputs": [],
      "source": [
        "# mask.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prdvPG3XZiuL"
      },
      "outputs": [],
      "source": [
        "# segmentation_mask.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAzD4mVaTYJg"
      },
      "outputs": [],
      "source": [
        "# plt.style.use(\"classic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak3eAHrDdY_V"
      },
      "outputs": [],
      "source": [
        "# first_channel_tensor = image[0, 0, :, :]\n",
        "# print(first_channel_tensor.shape)  # torch.Size([512, 512])\n",
        "# second_channel_tensor = image[0, 1, :, :]\n",
        "# print(second_channel_tensor.shape)  # torch.Size([512, 512])\n",
        "# third_channel_tensor = image[0, 2, :, :]\n",
        "# print(third_channel_tensor.shape)  # torch.Size([512, 512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRJzqWT1drZv"
      },
      "outputs": [],
      "source": [
        "# plt.title(\"First Channel Image\")\n",
        "# plt.imshow(first_channel_tensor.detach().cpu().numpy(), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS4Lttr2TLad"
      },
      "outputs": [],
      "source": [
        "# plt.imshow(mask.squeeze().detach().cpu().numpy(), cmap = 'gray')\n",
        "# print(mask.squeeze().shape)\n",
        "# plt.title(\"Mask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X9nSoL1TNw_"
      },
      "outputs": [],
      "source": [
        "# print(segmentation_mask.squeeze().shape)\n",
        "# plt.imshow(segmentation_mask.squeeze().detach().cpu().numpy(), cmap='gray')\n",
        "# plt.title(\"Segmentation Mask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi-kZiELUV79"
      },
      "outputs": [],
      "source": [
        "# print(type(pred_probs), type(mask))\n",
        "# print(pred_probs.shape, mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKAY7_pFh_Vt"
      },
      "outputs": [],
      "source": [
        "# print(segmentation_mask.squeeze().squeeze().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm9Qm1_FiFbS"
      },
      "outputs": [],
      "source": [
        "# print(mask.squeeze().squeeze().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kiIFEw2TPZn"
      },
      "outputs": [],
      "source": [
        "# # \"\"\"\n",
        "# # Both methods aim to capture the overlap between predicted positive regions and actual positive regions in the ground truth.\n",
        "# # The dice_metric approach leverages the full range of predicted probabilities, while the  metric function relies on a binary classification based on a chosen threshold.\n",
        "# # \"\"\"\n",
        "# # Dice = metric(pred_probs.detach().cpu(), mask).item()\n",
        "# # print(f\"Dice coefficient: {Dice}\")\n",
        "# # dice_metric_score = dice_metric(pred_probs.detach().cpu(), mask)\n",
        "# # print(f\"Dice coefficient: {dice_metric_score}\")\n",
        "\n",
        "# \"\"\"\n",
        "# Both methods aim to capture the overlap between predicted positive regions and actual positive regions in the ground truth.\n",
        "# The dice_metric approach leverages the full range of predicted probabilities, while the  metric function relies on a binary classification based on a chosen threshold.\n",
        "# \"\"\"\n",
        "# Dice = metric(segmentation_mask.detach().cpu(), mask).item()\n",
        "# print(f\"Dice coefficient: {Dice}\")\n",
        "# dice_metric_score = dice_metric(segmentation_mask.squeeze().squeeze().detach().cpu(), mask.squeeze().squeeze(), per_image=False)\n",
        "# print(f\"Dice coefficient: {dice_metric_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLi3ZH-8ALZM"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P0OtlxdALZN"
      },
      "outputs": [],
      "source": [
        "# from torchvision.transforms import Resize\n",
        "# from torchvision.utils import make_grid\n",
        "\n",
        "\n",
        "# def preprocess_image(image, target_size):\n",
        "#     transform = Resize(target_size)\n",
        "#     image = transform(image)\n",
        "#     return image\n",
        "\n",
        "# def generate_predictions(model, images):\n",
        "#     with torch.inference_mode():\n",
        "#         predictions = model(images.to(DEVICE))\n",
        "#         predictions = torch.sigmoid(predictions)\n",
        "#         return predictions\n",
        "\n",
        "\n",
        "# def visualize_predictions(images, target_masks, preds):\n",
        "#     images = images.cpu() # shape: [8, 3, 1024, 1024] or [8, 3, 512, 512]\n",
        "#     target_masks = target_masks.cpu() # shape: [8, 1, 1024, 1024] or [8, 1, 512, 512]\n",
        "#     preds = preds.cpu() # shape: [8, 1, 1024, 1024] or [8, 1, 512, 512]\n",
        "\n",
        "#     fig, axes = plt.subplots(2, 3, figsize=(20, 30))\n",
        "\n",
        "#     for i in range(2):\n",
        "#         # axes[i, 0].imshow(images[i].permute(1, 2, 0))\n",
        "#         axes[i, 0].imshow(images[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 0].set_title(\"Input Image\")\n",
        "#         # axes[i, 0].axis(\"off\")\n",
        "\n",
        "#         axes[i, 1].imshow(target_masks[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 1].set_title(\"Target Mask\")\n",
        "#         # axes[i, 1].axis(\"off\")\n",
        "\n",
        "#         axes[i, 2].imshow(preds[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 2].set_title(\"Model 512 prediction\")\n",
        "#         # axes[i, 2].axis(\"off\")\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPbHLrDvALZN"
      },
      "outputs": [],
      "source": [
        "# \"\"\"----------------------------- Inference --------------------------------\"\"\"\n",
        "\n",
        "# metrics512 = defaultdict(float)\n",
        "# for b_idx, data in enumerate(val_dataloader):\n",
        "#     batch_images = data[\"image\"] # shape: [8, 3, 1024, 1024]\n",
        "#     images_resized_512 = preprocess_image(batch_images, (512, 512))\n",
        "\n",
        "#     # Generate predictions\n",
        "#     preds = generate_predictions(model, images_resized_512)  # shape: [1, 1, 512, 512]\n",
        "#     print(\"preds - Shape: \", preds.shape, \" || device: \", preds.get_device())\n",
        "#     seg_mask_ensemble_512 = preds > 0.4\n",
        "#     # mask preprocessing\n",
        "#     batch_masks = data[\"mask\"]\n",
        "#     masks_resized_512 = preprocess_image(batch_masks, (512, 512))\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"---------------------------- Dice Scores -----------------------------------\")\n",
        "\n",
        "\n",
        "#     dice_scores_512 = dice_metric(seg_mask_ensemble_512, masks_resized_512.to(DEVICE), per_image=True)\n",
        "#     print(\"dice_scores_512 - Shape: \", dice_scores_512.shape, \" || type: \", type(dice_scores_512))\n",
        "\n",
        "#     # dice_scores_1024 = dice_metric(combined_pred, batch_images.to(DEVICE), per_image=True)\n",
        "#     # print(\"dice_scores_1024 - Shape: \", dice_scores_1024.shape, \" || type: \", type(dice_scores_1024))\n",
        "\n",
        "#     print(\"Dice scores of predictions on the 512x512 scale\\n\", dice_scores_512)\n",
        "#     print(\"Dice score of the batch 512x512 scale: \", dice_scores_512.mean())\n",
        "\n",
        "#     print(\"---------------------------- SCALE = 1024x1024 -----------------------------------\")\n",
        "#     visualize_predictions(batch_images, batch_masks, seg_mask_ensemble_512)\n",
        "#     print(\"-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-\")\n",
        "#     break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Y7HFiESzDWcY",
        "Xfcsl-YW_ck9",
        "3A4XlffPf1jV",
        "7xtZrBIJeLH_",
        "A-y8ovXEK2St",
        "od_pZDUoK2Su",
        "EqxMQpqB5JFV",
        "BNLEHf3QK2Sv",
        "6-dcysMmK2Sw",
        "Vzze2VhIK2S5",
        "kOlM9jTEK2S5",
        "K136b3eqK2S5",
        "pOkP2HYpvsqr",
        "LcACWXgAXHQu",
        "gJFz9wAhzL4k",
        "iOQs9Sy2VBHR",
        "cLi3ZH-8ALZM"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}