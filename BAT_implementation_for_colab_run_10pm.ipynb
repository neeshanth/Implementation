{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY48Ap7wK2Sn"
      },
      "source": [
        "PNEUMOTHORAX SEGMENTATION USING SIIM-ACR DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMmEp3gqUyn1",
        "outputId": "47ed6349-5f46-4235-91ad-3573c18e673b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_qWc-LUIJCv"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/qubvel/segmentation_models.pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz4WJSBg9UOk",
        "outputId": "eab674b8-48ff-423a-82bd-618ae2f10d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEfIEmqfADNu"
      },
      "outputs": [],
      "source": [
        "# !pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECBIUxVDADNv"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from monai.networks.nets.swin_unetr import SwinTransformer as SwinViT\n",
        "# from monai.utils import ensure_tuple_rep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7HFiESzDWcY"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUKZcMbBK2Ss"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from collections import defaultdict\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score\n",
        "import torchvision.models as models # NEW MARCH-JUNE EDIT\n",
        "\n",
        "try:\n",
        "    from itertools import ifilterfalse\n",
        "except ImportError:  # py3k\n",
        "    from itertools import filterfalse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzbBvorxADNw"
      },
      "outputs": [],
      "source": [
        "def init_seed(SEED=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un-DgSH9ADNx"
      },
      "outputs": [],
      "source": [
        "init_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfcsl-YW_ck9"
      },
      "source": [
        "## Config - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vx6RpZ8_bba"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE         = 512 # This is efficient and better for the completion of the project\n",
        "WHOSE_DIR        = \"Neeshanth\"\n",
        "\n",
        "if WHOSE_DIR == \"Aathesh\":\n",
        "    DIR              = \"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\mask\")\n",
        "    DATA_FRAME_PATH  = \"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\RLE_kfold.csv\"\n",
        "elif WHOSE_DIR == \"Neeshanth\":\n",
        "    DIR              = \"/content/drive/MyDrive/datasets\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"/content/drive/MyDrive/datasets/train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"/content/drive/MyDrive/datasets/mask\")\n",
        "    DATA_FRAME_PATH  = \"/content/drive/MyDrive/datasets/RLE_kfold.csv\"\n",
        "\n",
        "elif WHOSE_DIR == \"Kousik\":\n",
        "    DIR              = r\"C:\\Users\\kousi\\Downloads\\datasets\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(r\"C:\\Users\\kousi\\Downloads\\datasets\\train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(r\"C:\\Users\\kousi\\Downloads\\datasets\\mask\")\n",
        "    DATA_FRAME_PATH  = r\"C:\\Users\\kousi\\Downloads\\datasets\\RLE_kfold.csv\"\n",
        "\n",
        "elif WHOSE_DIR == \"wonder_boys\":\n",
        "    DIR              = \"/content/drive/MyDrive/Colab_Notebooks/datasets/main\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"/content/drive/MyDrive/Colab_Notebooks/datasets/main/train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"/content/drive/MyDrive/Colab_Notebooks/datasets/main/mask\")\n",
        "    DATA_FRAME_PATH  = \"/content/drive/MyDrive/Colab_Notebooks/datasets/main/RLE_kfold.csv\"\n",
        "\n",
        "KFOLD_PATH       = \"\"\n",
        "TRAIN_BATCH_SIZE = 2\n",
        "VALID_BATCH_SIZE = 2\n",
        "BATCH_SIZE       = 2\n",
        "EPOCHS           = 50\n",
        "# Path for pretrained model weights\n",
        "TRAINING_MODEL_PATH = \"\"\n",
        "USE_SAMPLER      = True\n",
        "POSTIVE_PERC     = 0.8\n",
        "DEVICE           = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PRETRAINED       = False # False means we're using ImageNet weights, so essentially, it's pretrained & never from scratch!!!!!\n",
        "LEARNING_RATE    = 0.0001\n",
        "NUM_WORKERS      = 8\n",
        "USE_CRIT         = True\n",
        "FOLD_ID          = 4\n",
        "EVAL_METRICS      = [\"metric - it calculates dice coefficient.\"]\n",
        "\n",
        "# Regularization Settings\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "L2_WEIGHT_DECAY  = 0.000005\n",
        "GRADIENT_CLIPPING = True\n",
        "GRADIENT_CLIPPING_THRESHOLD = 0.1\n",
        "\n",
        "# IF U DON'T WANT TO CHANGE Learning rate from previous experiment then set this to True.\n",
        "OPTIMIZER_LOAD = False\n",
        "# U MUST VERIFY IF LR IS GETTING SET PROPERLY FOR THE SCHEDULER IN THE \"OPTIMIZER & SCHEDULER\" SECTION OF THE NOTEBOOK.\n",
        "\n",
        "# Learning Rate Scheduler Settings\n",
        "SCHEDULER        = \"ReduceLROnPlateau\"\n",
        "if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "    SCHEDULER_PARAMS = {'factor': 0.1, 'patience': 1, 'threshold': 0.0000001, 'min_lr': 0.0000001} # patience changed from 2 to 1 on 12-04-2025\n",
        "elif SCHEDULER == \"CosineAnnealingWarmRestarts\":\n",
        "    SCHEDULER_PARAMS = {'T_0': 1, 'T_mult': 2, 'eta_min': 0.00000001}\n",
        "elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "    SCHEDULER_PARAMS = {'T_max': 8, 'eta_min': 0.0000001}\n",
        "\n",
        "# Thresholds for 1024x1024 is there along with div by 2 values and div by 4 values\n",
        "TRIPLET_THRESHOLDS = [  [0.6, 500.0, 0.35], [0.67, 500.0, 0.37], [0.75, 500.0, 0.3],\n",
        "                        [0.75, 500.0, 0.4], [0.75, 1000.0, 0.3], [0.75, 1000.0, 0.4],\n",
        "                        [0.6, 1000.0, 0.3], [0.6, 1000.0, 0.4], [0.6, 1500.0, 0.3],\n",
        "                        [0.6, 1500.0, 0.4], [0.6, 250.0, 0.35], [0.67, 250.0, 0.37],\n",
        "                        [0.75, 250.0, 0.3], [0.75, 250.0, 0.4], [0.75, 500.0, 0.3],\n",
        "                        [0.75, 500.0, 0.4], [0.6, 500.0, 0.3], [0.6, 500.0, 0.4],\n",
        "                        [0.6, 750.0, 0.3], [0.6, 750.0, 0.4], [0.6, 1000, 0.35],\n",
        "                        [0.67, 1000, 0.37], [0.75, 1000, 0.3], [0.75, 1000, 0.4],\n",
        "                        [0.75, 2000, 0.3], [0.75, 2000, 0.4], [0.6, 2000, 0.3],\n",
        "                        [0.6, 2000, 0.4], [0.6, 3000, 0.3], [0.6, 3000, 0.4]       ]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    \"-------------------------------SAVING losses & metrics-------------------------------------\"\n",
        "\"\"\"\n",
        "ACCOUNT         = \"Neeshanth\" # \"Neeshanth\" or \"wonder_boys\" or others\n",
        "PURPOSE         = \"Project-2\" # training/inference/hyperparameter-tuning or hpt OR ANYTHING MORE SPECIFIC\n",
        "EXP_NO          = \"first_run\" # hpt experiment with changed settings\n",
        "PHASE           = \"12_04_2025\" # Just date\n",
        "\n",
        "EFFECTIVE_BATCH_SIZE = 2 # accumulation_steps * BATCH_SIZE\n",
        "\n",
        "if ACCOUNT == \"Neeshanth\":\n",
        "    # Save Config.txt\n",
        "    CONFIG_FILE_LOC = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "if ACCOUNT == \"Kousik\":\n",
        "    # Save Config.txt\n",
        "    CONFIG_FILE_LOC = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch-wise comboloss during training\n",
        "    store_batch_training_details_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\"  # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch-wise comboloss during validation\n",
        "    store_batch_validation_details_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch-wise train and val losses to monitor overfitting\n",
        "    save_progress_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch-wise dice coefficient\n",
        "    save_dice_score_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save BCE, dice & focal losses separately after training is done\n",
        "    save_3losses_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "\n",
        "if ACCOUNT == \"wonder_boys\":\n",
        "    # Save Config.txt file\n",
        "    CONFIG_FILE_LOC = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "if ACCOUNT == \"pc_aathesh\":\n",
        "    # Save Config.txt file\n",
        "    CONFIG_FILE_LOC = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\CHECKPOINTS\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_dice_score.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_all_loss_vals.csv\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class - CREATE A NEW FOLDER\n",
        "    model_checkpoint_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_best_checkpoint_thresholds.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A4XlffPf1jV"
      },
      "source": [
        "## Saving Config as .txt - TYPE ESSENTIAL DETAILS TO RECOVER THIS EXPERIMENT IN THE BELOW SNIPPET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB0Igy_Tf1jV"
      },
      "source": [
        "ESSENTIALS include current_notebook_loc_in_pc, previous_notebook_loc_in_pc, key_changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3likqSVNf1jV",
        "outputId": "fe863d7e-d070-423e-eb11-6026e463af07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current IST date and time is: 13-04-2025 21:57:57\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# Define the IST timezone\n",
        "ist_timezone = pytz.timezone('Asia/Kolkata')\n",
        "\n",
        "# Get the current time in IST\n",
        "ist_time = datetime.now(ist_timezone)\n",
        "\n",
        "# Format the date and time to DD-MM-YYYY HH:MM:SS\n",
        "formatted_ist_time = ist_time.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "\n",
        "# Print the formatted IST time\n",
        "print(\"Current IST date and time is:\", formatted_ist_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bwNCwwVf1jV",
        "outputId": "908a2ae0-0912-455a-89b6-36b87743a1e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyper-parameters have been saved to /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_first_run_12_04_2025/Config.txt\n"
          ]
        }
      ],
      "source": [
        "hyperparameters = {\n",
        "    'image_size': f\"{IMG_SIZE}x{IMG_SIZE}\",\n",
        "    'date': formatted_ist_time,\n",
        "    'account': ACCOUNT,\n",
        "    'purpose': PURPOSE,\n",
        "    'experiment_no': EXP_NO,\n",
        "    'key_changes': \"\" ,\n",
        "    'checkpoint_used_loc': TRAINING_MODEL_PATH,\n",
        "    'is_optimizer_loaded': OPTIMIZER_LOAD,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'batch_size': EFFECTIVE_BATCH_SIZE,\n",
        "    'gradient_accumulation_steps': int(EFFECTIVE_BATCH_SIZE/BATCH_SIZE),\n",
        "    'num_epochs': EPOCHS,\n",
        "    'IsSamplerUsed': USE_SAMPLER,\n",
        "    'percentage_of_positive_samples': POSTIVE_PERC,\n",
        "    'optimizer': 'Adam',\n",
        "    'loss_function': 'ComboLoss',\n",
        "    'scheduler': SCHEDULER,\n",
        "    'scheduler_params': SCHEDULER_PARAMS,\n",
        "    'L2_Regularization_weight_decay':L2_WEIGHT_DECAY,\n",
        "    'early_stopping_patience': EARLY_STOPPING_PATIENCE,\n",
        "    'is_gradient_clipping_used': GRADIENT_CLIPPING,\n",
        "    'gradient_clipping_threshold': GRADIENT_CLIPPING_THRESHOLD,\n",
        "    'model': 'Hybrid Vision Transformer',\n",
        "    'training_phase': f\"Phase - {PHASE}\",\n",
        "    'fold_ID': FOLD_ID,\n",
        "    'GPU_name': torch.cuda.get_device_name(torch.cuda.current_device()),\n",
        "    'num_workers': NUM_WORKERS,\n",
        "    'weights_given_to_loss_functions': {'bce': 3, 'dice': 1, 'focal': 4},\n",
        "    'triplet_thresholds': TRIPLET_THRESHOLDS,\n",
        "    'evaluation_metrics': EVAL_METRICS,\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a formatted string\n",
        "hyperparameters_str = json.dumps(hyperparameters, indent=4)\n",
        "\n",
        "# Specify the file name\n",
        "file_name = CONFIG_FILE_LOC\n",
        "\n",
        "os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "# Write the string to a file\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(hyperparameters_str)\n",
        "\n",
        "print(f\"Hyper-parameters have been saved to {file_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xtZrBIJeLH_"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjVO5UiC1z-h"
      },
      "outputs": [],
      "source": [
        "bce_losses = []\n",
        "dice_losses = []\n",
        "focal_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MgSVyz79UOs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from scipy.ndimage import distance_transform_edt as distance\n",
        "from skimage import segmentation as skimage_seg\n",
        "\n",
        "\n",
        "def dice_loss(score, target):\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * target)\n",
        "    y_sum = torch.sum(target * target)\n",
        "    z_sum = torch.sum(score * score)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def dice_loss1(score, target):\n",
        "    # non-square\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * target)\n",
        "    y_sum = torch.sum(target)\n",
        "    z_sum = torch.sum(score)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def iou_loss(score, target):\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    tp_sum = torch.sum(score * target)\n",
        "    fp_sum = torch.sum(score * (1 - target))\n",
        "    fn_sum = torch.sum((1 - score) * target)\n",
        "    loss = (tp_sum + smooth) / (tp_sum + fp_sum + fn_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def entropy_loss(p, C=2):\n",
        "    ## p N*C*W*H*D\n",
        "    y1 = -1 * torch.sum(p * torch.log(p + 1e-6), dim=1) / torch.tensor(\n",
        "        np.log(C)).cuda()\n",
        "    ent = torch.mean(y1)\n",
        "\n",
        "    return ent\n",
        "\n",
        "\n",
        "def softmax_dice_loss(input_logits, target_logits):\n",
        "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_softmax = F.softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "    n = input_logits.shape[1]\n",
        "    dice = 0\n",
        "    for i in range(0, n):\n",
        "        dice += dice_loss1(input_softmax[:, i], target_softmax[:, i])\n",
        "    mean_dice = dice / n\n",
        "\n",
        "    return mean_dice\n",
        "\n",
        "\n",
        "def entropy_loss_map(p, C=2):\n",
        "    ent = -1 * torch.sum(p * torch.log(p + 1e-6), dim=1,\n",
        "                         keepdim=True) / torch.tensor(np.log(C)).cuda()\n",
        "    return ent\n",
        "\n",
        "\n",
        "def softmax_mse_loss(input_logits, target_logits):\n",
        "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_softmax = F.softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "\n",
        "    mse_loss = (input_softmax - target_softmax)**2\n",
        "    return mse_loss\n",
        "\n",
        "\n",
        "def softmax_kl_loss(input_logits, target_logits):\n",
        "    \"\"\"Takes softmax on both sides and returns KL divergence\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_log_softmax = F.log_softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "\n",
        "    # return F.kl_div(input_log_softmax, target_softmax)\n",
        "    kl_div = F.kl_div(input_log_softmax, target_softmax, reduction='none')\n",
        "    # mean_kl_div = torch.mean(0.2*kl_div[:,0,...]+0.8*kl_div[:,1,...])\n",
        "    return kl_div\n",
        "\n",
        "\n",
        "def symmetric_mse_loss(input1, input2):\n",
        "    \"\"\"Like F.mse_loss but sends gradients to both directions\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to both input1 and input2.\n",
        "    \"\"\"\n",
        "    assert input1.size() == input2.size()\n",
        "    return torch.mean((input1 - input2)**2)\n",
        "\n",
        "\n",
        "def compute_sdf01(segmentation):\n",
        "    \"\"\"\n",
        "    compute the signed distance map of binary mask\n",
        "    input: segmentation, shape = (batch_size, class, x, y, z)\n",
        "    output: the Signed Distance Map (SDM)\n",
        "    sdm(x) = 0; x in segmentation boundary\n",
        "             -inf|x-y|; x in segmentation\n",
        "             +inf|x-y|; x out of segmentation\n",
        "    \"\"\"\n",
        "    # print(type(segmentation), segmentation.shape)\n",
        "\n",
        "    segmentation = segmentation.astype(np.uint8)\n",
        "\n",
        "    if len(segmentation.shape) == 4:  # 3D image\n",
        "        segmentation = np.expand_dims(segmentation, 1)\n",
        "    normalized_sdf = np.zeros(segmentation.shape)\n",
        "    if segmentation.shape[1] == 1:\n",
        "        dis_id = 0\n",
        "    else:\n",
        "        dis_id = 1\n",
        "    for b in range(segmentation.shape[0]):  # batch size\n",
        "        for c in range(dis_id, segmentation.shape[1]):  # class_num\n",
        "            # ignore background\n",
        "            posmask = segmentation[b][c]\n",
        "            if np.max(posmask) == 0:\n",
        "                continue\n",
        "            negmask = ~posmask\n",
        "            posdis = distance(posmask)\n",
        "            negdis = distance(negmask)\n",
        "            boundary = skimage_seg.find_boundaries(\n",
        "                posmask, mode='inner').astype(np.uint8)\n",
        "            sdf = negdis / np.max(negdis) / 2 - posdis / np.max(\n",
        "                posdis) / 2 + 0.5\n",
        "            sdf[boundary > 0] = 0.5\n",
        "            normalized_sdf[b][c] = sdf\n",
        "    return normalized_sdf\n",
        "\n",
        "\n",
        "def compute_sdf1_1(segmentation):\n",
        "    \"\"\"\n",
        "    compute the signed distance map of binary mask\n",
        "    input: segmentation, shape = (batch_size, class, x, y, z)\n",
        "    output: the Signed Distance Map (SDM)\n",
        "    sdm(x) = 0; x in segmentation boundary\n",
        "             -inf|x-y|; x in segmentation\n",
        "             +inf|x-y|; x out of segmentation\n",
        "    \"\"\"\n",
        "    # print(type(segmentation), segmentation.shape)\n",
        "\n",
        "    segmentation = segmentation.astype(np.uint8)\n",
        "    if len(segmentation.shape) == 4:  # 3D image\n",
        "        segmentation = np.expand_dims(segmentation, 1)\n",
        "    normalized_sdf = np.zeros(segmentation.shape)\n",
        "    if segmentation.shape[1] == 1:\n",
        "        dis_id = 0\n",
        "    else:\n",
        "        dis_id = 1\n",
        "    for b in range(segmentation.shape[0]):  # batch size\n",
        "        for c in range(dis_id, segmentation.shape[1]):  # class_num\n",
        "            # ignore background\n",
        "            posmask = segmentation[b][c]\n",
        "            if np.max(posmask) == 0:\n",
        "                continue\n",
        "            negmask = ~posmask\n",
        "            posdis = distance(posmask)\n",
        "            negdis = distance(negmask)\n",
        "            boundary = skimage_seg.find_boundaries(\n",
        "                posmask, mode='inner').astype(np.uint8)\n",
        "            sdf = negdis / np.max(negdis) - posdis / np.max(posdis)\n",
        "            sdf[boundary > 0] = 0\n",
        "            normalized_sdf[b][c] = sdf\n",
        "    return normalized_sdf\n",
        "\n",
        "\n",
        "def compute_fore_dist(segmentation):\n",
        "    \"\"\"\n",
        "    compute the foreground of binary mask\n",
        "    input: segmentation, shape = (batch_size, class, x, y, z)\n",
        "    output: the Signed Distance Map (SDM)\n",
        "    sdm(x) = 0; x in segmentation boundary\n",
        "             -inf|x-y|; x in segmentation\n",
        "             +inf|x-y|; x out of segmentation\n",
        "    \"\"\"\n",
        "    # print(type(segmentation), segmentation.shape)\n",
        "\n",
        "    segmentation = segmentation.astype(np.uint8)\n",
        "    if len(segmentation.shape) == 4:  # 3D image\n",
        "        segmentation = np.expand_dims(segmentation, 1)\n",
        "    normalized_sdf = np.zeros(segmentation.shape)\n",
        "    if segmentation.shape[1] == 1:\n",
        "        dis_id = 0\n",
        "    else:\n",
        "        dis_id = 1\n",
        "    for b in range(segmentation.shape[0]):  # batch size\n",
        "        for c in range(dis_id, segmentation.shape[1]):  # class_num\n",
        "            # ignore background\n",
        "            posmask = segmentation[b][c]\n",
        "            posdis = distance(posmask)\n",
        "            normalized_sdf[b][c] = posdis / np.max(posdis)\n",
        "    return normalized_sdf\n",
        "\n",
        "\n",
        "def sum_tensor(inp, axes, keepdim=False):\n",
        "    axes = np.unique(axes).astype(int)\n",
        "    if keepdim:\n",
        "        for ax in axes:\n",
        "            inp = inp.sum(int(ax), keepdim=True)\n",
        "    else:\n",
        "        for ax in sorted(axes, reverse=True):\n",
        "            inp = inp.sum(int(ax))\n",
        "    return inp\n",
        "\n",
        "\n",
        "def AAAI_sdf_loss(net_output, gt):\n",
        "    \"\"\"\n",
        "    net_output: net logits; shape=(batch_size, class, x, y, z)\n",
        "    gt: ground truth; (shape (batch_size, 1, x, y, z) OR (batch_size, x, y, z))\n",
        "    \"\"\"\n",
        "    smooth = 1e-5\n",
        "    axes = tuple(range(2, len(net_output.size())))\n",
        "    shp_x = net_output.shape\n",
        "    shp_y = gt.shape\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if len(shp_x) != len(shp_y):\n",
        "            gt = gt.view((shp_y[0], 1, *shp_y[1:]))\n",
        "\n",
        "        if all([i == j for i, j in zip(net_output.shape, gt.shape)]):\n",
        "            # if this is the case then gt is probably already a one hot encoding\n",
        "            y_onehot = gt\n",
        "        else:\n",
        "            gt = gt.long()\n",
        "            y_onehot = torch.zeros(shp_x)\n",
        "            if net_output.device.type == \"cuda\":\n",
        "                y_onehot = y_onehot.cuda(net_output.device.index)\n",
        "            y_onehot.scatter_(1, gt, 1)\n",
        "        gt_sdm_npy = compute_sdf1_1(y_onehot.cpu().numpy())\n",
        "        if net_output.device.type == \"cuda\":\n",
        "            gt_sdm = torch.from_numpy(gt_sdm_npy).float().cuda(\n",
        "                net_output.device.index)\n",
        "        else:\n",
        "            gt_sdm = torch.from_numpy(gt_sdm_npy).float()\n",
        "    intersect = sum_tensor(net_output * gt_sdm, axes, keepdim=False)\n",
        "    pd_sum = sum_tensor(net_output**2, axes, keepdim=False)\n",
        "    gt_sum = sum_tensor(gt_sdm**2, axes, keepdim=False)\n",
        "    L_product = (intersect + smooth) / (intersect + pd_sum + gt_sum)\n",
        "    # print('L_product.shape', L_product.shape) (4,2)\n",
        "    L_SDF_AAAI = -L_product.mean() + torch.norm(net_output - gt_sdm,\n",
        "                                                1) / torch.numel(net_output)\n",
        "\n",
        "    return L_SDF_AAAI\n",
        "\n",
        "\n",
        "def sdf_kl_loss(net_output, gt):\n",
        "    \"\"\"\n",
        "    net_output: net logits; shape=(batch_size, class, x, y, z)\n",
        "    gt: ground truth; (shape (batch_size, 1, x, y, z) OR (batch_size, x, y, z))\n",
        "    \"\"\"\n",
        "    smooth = 1e-5\n",
        "    axes = tuple(range(2, len(net_output.size())))\n",
        "    shp_x = net_output.shape\n",
        "    shp_y = gt.shape\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if len(shp_x) != len(shp_y):\n",
        "            gt = gt.view((shp_y[0], 1, *shp_y[1:]))\n",
        "\n",
        "        if all([i == j for i, j in zip(net_output.shape, gt.shape)]):\n",
        "            # if this is the case then gt is probably already a one hot encoding\n",
        "            y_onehot = gt\n",
        "        else:\n",
        "            gt = gt.long()\n",
        "            y_onehot = torch.zeros(shp_x)\n",
        "            if net_output.device.type == \"cuda\":\n",
        "                y_onehot = y_onehot.cuda(net_output.device.index)\n",
        "            y_onehot.scatter_(1, gt, 1)\n",
        "        # print('y_onehot.shape', y_onehot.shape)\n",
        "        gt_sdf_npy = compute_sdf(y_onehot.cpu().numpy())\n",
        "        gt_sdf = torch.from_numpy(gt_sdf_npy + smooth).float().cuda(\n",
        "            net_output.device.index)\n",
        "    # print('net_output, gt_sdf', net_output.shape, gt_sdf.shape)\n",
        "    # exit()\n",
        "    sdf_kl_loss = F.kl_div(net_output,\n",
        "                           gt_sdf[:, 1:2, ...],\n",
        "                           reduction='batchmean')\n",
        "\n",
        "    return sdf_kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GiUgprmeQ97"
      },
      "outputs": [],
      "source": [
        "eps = 1e-6\n",
        "\n",
        "\n",
        "def soft_dice_loss(outputs, targets, per_image=False, per_channel=False):\n",
        "    \"\"\"\n",
        "        If per_image = False, then the function calculates dice loss for a single image-mask pair.\n",
        "    \"\"\"\n",
        "    batch_size, n_channels = outputs.size(0), outputs.size(1)\n",
        "\n",
        "    eps = 1e-6\n",
        "    n_parts = 1\n",
        "    if per_image:\n",
        "        n_parts = batch_size\n",
        "    if per_channel:\n",
        "        n_parts = batch_size * n_channels\n",
        "\n",
        "    dice_target = targets.contiguous().view(n_parts, -1).float()\n",
        "    dice_output = outputs.contiguous().view(n_parts, -1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1)\n",
        "    loss = (1 - (2 * intersection + eps) / (union + eps)) # returns a tensor of size [8]\n",
        "    return loss.mean() # returns a tensor of size [1].\n",
        "\n",
        "def dice_metric(preds, trues, per_image=False, per_channel=False):\n",
        "    preds = preds.float()\n",
        "    return 1 - soft_dice_loss(preds, trues, per_image, per_channel)\n",
        "\n",
        "\n",
        "def jaccard(outputs, targets, per_image=False, non_empty=False, min_pixels=5):\n",
        "    batch_size = outputs.size()[0]\n",
        "    eps = 1e-3\n",
        "    if not per_image:\n",
        "        batch_size = 1\n",
        "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
        "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
        "    target_sum = torch.sum(dice_target, dim=1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    losses = 1 - (intersection + eps) / (torch.sum(dice_output + dice_target, dim=1) - intersection + eps)\n",
        "    if non_empty:\n",
        "        assert per_image == True\n",
        "        non_empty_images = 0\n",
        "        sum_loss = 0\n",
        "        for i in range(batch_size):\n",
        "            if target_sum[i] > min_pixels:\n",
        "                sum_loss += losses[i]\n",
        "                non_empty_images += 1\n",
        "        if non_empty_images == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return sum_loss / non_empty_images\n",
        "\n",
        "    return losses.mean()\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        dice_loss = soft_dice_loss(input, target, per_image=self.per_image)\n",
        "        dice_losses.append(dice_loss.item())\n",
        "        return dice_loss\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False, non_empty=False, apply_sigmoid=False,\n",
        "                 min_pixels=5):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "        self.non_empty = non_empty\n",
        "        self.apply_sigmoid = apply_sigmoid\n",
        "        self.min_pixels = min_pixels\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if self.apply_sigmoid:\n",
        "            input = torch.sigmoid(input)\n",
        "        return jaccard(input, target, per_image=self.per_image, non_empty=self.non_empty, min_pixels=self.min_pixels)\n",
        "\n",
        "class StableBCELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StableBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        bce_loss_with_logits = nn.BCEWithLogitsLoss(reduction='mean') # mean is taken across batches\n",
        "        bce_loss = bce_loss_with_logits(logits, target)\n",
        "        bce_losses.append(bce_loss.item())\n",
        "        return bce_loss # returns a tensor of size [1]\n",
        "\n",
        "\n",
        "class ComboLoss(nn.Module):\n",
        "    def __init__(self, weights, per_image=True, channel_weights=[1, 0.5, 0.5], channel_losses=None):\n",
        "        super().__init__()\n",
        "        self.weights = weights\n",
        "        self.bce = StableBCELoss()\n",
        "        self.dice = DiceLoss(per_image=True)\n",
        "        self.jaccard = JaccardLoss(per_image=True)\n",
        "        self.lovasz = LovaszLoss(per_image=per_image)\n",
        "        self.lovasz_sigmoid = LovaszLossSigmoid(per_image=per_image)\n",
        "        self.focal = FocalLoss2d()\n",
        "        self.mapping = {'bce': self.bce,\n",
        "                        'dice': self.dice,\n",
        "                        'focal': self.focal,\n",
        "                        'jaccard': self.jaccard,\n",
        "                        'lovasz': self.lovasz,\n",
        "                        'lovasz_sigmoid': self.lovasz_sigmoid}\n",
        "        self.expect_sigmoid = {'dice', 'focal', 'jaccard', 'lovasz_sigmoid'}\n",
        "        self.per_channel = {'dice', 'jaccard', 'lovasz_sigmoid'}\n",
        "        self.values = {}\n",
        "        self.channel_weights = channel_weights\n",
        "        self.channel_losses = channel_losses\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        loss = 0\n",
        "        weights = self.weights\n",
        "        sigmoid_input = torch.sigmoid(outputs)\n",
        "        for k, v in weights.items():\n",
        "            if not v:\n",
        "                continue\n",
        "            val = 0\n",
        "            if k in self.per_channel:\n",
        "                channels = targets.size(1)\n",
        "                for c in range(channels):\n",
        "                    if not self.channel_losses or k in self.channel_losses[c]:\n",
        "                        val += self.channel_weights[c] * self.mapping[k](sigmoid_input[:, c, ...] if k in self.expect_sigmoid else outputs[:, c, ...],\n",
        "                                               targets[:, c, ...])\n",
        "\n",
        "            else:\n",
        "                val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n",
        "\n",
        "            self.values[k] = val\n",
        "            loss += self.weights[k] * val\n",
        "        return loss.clamp(min=1e-5)\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    p = len(gt_sorted)\n",
        "    gts = gt_sorted.sum()\n",
        "    intersection = gts.float() - gt_sorted.float().cumsum(0)\n",
        "    union = gts.float() + (1 - gt_sorted).float().cumsum(0)\n",
        "    jaccard = 1. - intersection / union\n",
        "    if p > 1:  # cover 1-pixel case\n",
        "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
        "    return jaccard\n",
        "\n",
        "\n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
        "                    for log, lab in zip(logits, labels))\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "    if len(labels) == 0:\n",
        "        # only void pixels, the gradients should be 0\n",
        "        return logits.sum() * 0.\n",
        "    signs = 2. * labels.float() - 1.\n",
        "    errors = (1. - logits * Variable(signs))\n",
        "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
        "    perm = perm.data\n",
        "    gt_sorted = labels[perm]\n",
        "    grad = lovasz_grad(gt_sorted)\n",
        "    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = scores.view(-1)\n",
        "    labels = labels.view(-1)\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = (labels != ignore)\n",
        "    vscores = scores[valid]\n",
        "    vlabels = labels[valid]\n",
        "    return vscores, vlabels\n",
        "\n",
        "\n",
        "def lovasz_sigmoid(probas, labels, per_image=False, ignore=None):\n",
        "    \"\"\"\n",
        "    Multi-class Lovasz-Softmax loss\n",
        "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n",
        "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
        "      only_present: average only on classes present in ground truth\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class labels\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_sigmoid_flat(*flatten_binary_scores(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
        "                          for prob, lab in zip(probas, labels))\n",
        "    else:\n",
        "        loss = lovasz_sigmoid_flat(*flatten_binary_scores(probas, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def lovasz_sigmoid_flat(probas, labels):\n",
        "    \"\"\"\n",
        "    Multi-class Lovasz-Softmax loss\n",
        "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
        "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
        "      only_present: average only on classes present in ground truth\n",
        "    \"\"\"\n",
        "    fg = labels.float()\n",
        "    errors = (Variable(fg) - probas).abs()\n",
        "    errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
        "    perm = perm.data\n",
        "    fg_sorted = fg[perm]\n",
        "    loss = torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))\n",
        "    return loss\n",
        "\n",
        "def symmetric_lovasz(outputs, targets, ):\n",
        "    return (lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1 - targets)) / 2\n",
        "\n",
        "def mean(l, ignore_nan=False, empty=0):\n",
        "    \"\"\"\n",
        "    nanmean compatible with generators.\n",
        "    \"\"\"\n",
        "    l = iter(l)\n",
        "    if ignore_nan:\n",
        "        l = ifilterfalse(np.isnan, l)\n",
        "    try:\n",
        "        n = 1\n",
        "        acc = next(l)\n",
        "    except StopIteration:\n",
        "        if empty == 'raise':\n",
        "            raise ValueError('Empty mean')\n",
        "        return empty\n",
        "    for n, v in enumerate(l, 2):\n",
        "        acc += v\n",
        "    if n == 1:\n",
        "        return acc\n",
        "    return acc / n\n",
        "\n",
        "\n",
        "class LovaszLoss(nn.Module):\n",
        "    def __init__(self, ignore_index=255, per_image=True):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        return symmetric_lovasz(outputs, targets)\n",
        "\n",
        "class LovaszLossSigmoid(nn.Module):\n",
        "    def __init__(self, ignore_index=255, per_image=True):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        return lovasz_sigmoid(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n",
        "\n",
        "class FocalLoss2d(nn.Module):\n",
        "    def __init__(self, alpha=0.65, gamma=2,n_parts=BATCH_SIZE):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.n_parts = n_parts\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        n_parts = self.n_parts\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        eps = 1e-6\n",
        "        # non_ignored = targets.view(n_parts, -1) != self.ignore_index\n",
        "        targets = targets.view(n_parts, -1).float()\n",
        "        outputs = outputs.view(n_parts, -1)\n",
        "        # clamp just makes sure the values of the tensor is within the given range.\n",
        "        outputs = torch.clamp(outputs, eps, 1. - eps)\n",
        "        targets = torch.clamp(targets, eps, 1. - eps)\n",
        "        \"\"\" pt = predicted probability for the true class\n",
        "        when targets = 1, pt = outputs which means pt now has the predicted probability of positive class.\n",
        "        when tagets = 0, pt = 1 - outputs which means pt has the predicted probability of negative class.\n",
        "        \"\"\"\n",
        "        pt = (1 - targets) * (1 - outputs) + targets * outputs\n",
        "        alpha_t = self.alpha * targets + (1 - self.alpha)*(1 - targets)\n",
        "        pt_proc = -(alpha_t*((1. - pt) ** self.gamma * torch.log(pt))) # torch.log is natural log\n",
        "        focal_loss = pt_proc.mean(dim=1).mean()\n",
        "        focal_losses.append(focal_loss.item())\n",
        "        return focal_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-y8ovXEK2St"
      },
      "source": [
        "## Config - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQG4nEiq9UOu",
        "outputId": "12aadbd0-258e-4533-acb7-4608e268fa3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(arch='BAT', gpu='1', net_layer=50, seg_loss=0, pre=0, trans=1, point_pred=1, ppl=6, cross=0)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--arch', type=str, default='BAT')\n",
        "parser.add_argument('--gpu', type=str, default='1')\n",
        "parser.add_argument('--net_layer', type=int, default=50)\n",
        "# parser.add_argument('--dataset', type=str, default='isic2016')\n",
        "# parser.add_argument('--exp_name', type=str, default='')\n",
        "# parser.add_argument('--fold', type=str, default='0')\n",
        "# parser.add_argument('--lr_seg', type=float, default=1e-4)  #0.0003\n",
        "# parser.add_argument('--n_epochs', type=int, default=200)  #100\n",
        "# parser.add_argument('--bt_size', type=int, default=8)  #36\n",
        "parser.add_argument('--seg_loss', type=int, default=0, choices=[0, 1])\n",
        "# parser.add_argument('--aug', type=int, default=1)\n",
        "# parser.add_argument('--patience', type=int, default=10)  #50\n",
        "\n",
        "# pre-train\n",
        "parser.add_argument('--pre', type=int, default=0)\n",
        "\n",
        "# transformer\n",
        "parser.add_argument('--trans', type=int, default=1)\n",
        "\n",
        "# point constrain\n",
        "parser.add_argument('--point_pred', type=int, default=1)\n",
        "parser.add_argument('--ppl', type=int, default=6)\n",
        "\n",
        "# cross-scale framework\n",
        "parser.add_argument('--cross', type=int, default=0)\n",
        "\n",
        "parse_config , unknown = parser.parse_known_args()\n",
        "print(parse_config )\n",
        "\n",
        "# if parse_config.arch == 'BAT':\n",
        "#     parse_config.exp_name += '_{}_{}_{}_e{}'.format(parse_config.trans,\n",
        "#                                                     parse_config.point_pred,\n",
        "#                                                     parse_config.cross,\n",
        "#                                                     parse_config.ppl)\n",
        "# exp_name = parse_config.dataset + '/' + parse_config.exp_name + '_loss_' + str(\n",
        "#     parse_config.seg_loss) + '_aug_' + str(parse_config.aug) + '/fold_' + str(\n",
        "#         parse_config.fold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz72Jtwr9UOu"
      },
      "outputs": [],
      "source": [
        "def ce_loss(pred, gt):\n",
        "    pred = torch.clamp(pred, 1e-6, 1 - 1e-6)\n",
        "    return (-gt * torch.log(pred) - (1 - gt) * torch.log(1 - pred)).mean()\n",
        "\n",
        "\n",
        "def structure_loss(pred, mask):\n",
        "    \"\"\"            TransFuse train loss        \"\"\"\n",
        "    \"\"\"            Without sigmoid             \"\"\"\n",
        "    weit = 1 + 5 * torch.abs(\n",
        "        F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
        "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduction='none')\n",
        "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
        "\n",
        "    pred = torch.sigmoid(pred)\n",
        "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
        "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
        "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
        "    return (wbce + wiou).mean()\n",
        "\n",
        "\n",
        "def focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.6,  #0.8\n",
        "    gamma: float = 2,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    p = inputs\n",
        "    ce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"mean\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t)**gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "CRITERION = [focal_loss, ce_loss][parse_config.seg_loss]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SW4ViKfK2St"
      },
      "source": [
        "The [ComboLoss](https://github.com/sneddy/pneumothorax-segmentation/blob/master/unet_pipeline/Losses.py#L104) function used in CRITERION below also comes from the winning solution by Anuar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3__XL68K2St"
      },
      "outputs": [],
      "source": [
        "# CRITERION        = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})\n",
        "\n",
        "# # Use During Inference Stage to store images of predicted segmentation masks\n",
        "# # PREDICTION_PATH  = \"/content/drive/MyDrive/Colab_Notebooks/datasets/archive_png_siim_acr/Predicted_masks/tests\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od_pZDUoK2Su"
      },
      "source": [
        "\n",
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JuSwxSqK2Su"
      },
      "source": [
        "General utility functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LcOlxiwK2Sv"
      },
      "outputs": [],
      "source": [
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    fig,ax = plt.subplots(figsize=(10,6))\n",
        "    ax.imshow(img.permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkS16buBK2Sv"
      },
      "outputs": [],
      "source": [
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    images = {k:v.numpy() for k,v in images.items() if isinstance(v, torch.Tensor)} #convert tensor to numpy\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    image, mask = images['image'], images['mask']\n",
        "    plt.imshow(image.transpose(1,2,0), vmin=0, vmax=1)\n",
        "    if mask.max()>0:\n",
        "        plt.imshow(mask.squeeze(0), alpha=0.25)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KOb07Zlb3GN"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"Saving checkpoint...\")\n",
        "  torch.save(state, filename)\n",
        "  print(\"Checkpoint saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIcBkgjOcPWQ",
        "outputId": "0195719a-1559-4bb6-a99f-593c7aafd4a0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef load_checkpoint(checkpoint):\\n  print(\"Loading checkpoint...\")\\n  model.load_state_dict(checkpoint[\\'state_dict\\'])\\n  optimizer.load_state_dict(checkpoint[\\'optimizer\\'])\\n'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "def load_checkpoint(checkpoint):\n",
        "  print(\"Loading checkpoint...\")\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrindR018hyO"
      },
      "outputs": [],
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item() # Calculates where two tensors are equal\n",
        "  acc = (correct / len(y_pred) ) * 100\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqxMQpqB5JFV"
      },
      "source": [
        "# ---------------------- DL Workflow -----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLEHf3QK2Sv"
      },
      "source": [
        "## 1. Data -> Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wbgxfpGK2Sv"
      },
      "source": [
        "### Create five-fold splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDyy5r93K2Sv",
        "outputId": "0b047d95-4754-4470-a00b-50bd8cd81419"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8570, 2142)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# single fold training for now, rerun notebook to train for multi-fold\n",
        "DF       = pd.read_csv(DATA_FRAME_PATH)\n",
        "TRAIN_DF = DF.query(f'kfold!={FOLD_ID}').reset_index(drop=True)\n",
        "VAL_DF   = DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)\n",
        "len(TRAIN_DF), len(VAL_DF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njKNWpqFK2Sw"
      },
      "source": [
        "### Dataset and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ywRGbXTK2Sw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Key-Patch Map Generation\n",
        "# =============================================================================\n",
        "\n",
        "def resize_and_clip(img, target_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Resize image or mask to target_size using nearest neighbor interpolation\n",
        "    and clip its pixel values to [0, 255].\n",
        "    \"\"\"\n",
        "    resized = cv2.resize(img, target_size, interpolation=cv2.INTER_NEAREST)\n",
        "    resized = np.clip(resized, 0, 255)\n",
        "    return resized\n",
        "\n",
        "def create_circular_mask(h, w, center, radius):\n",
        "    \"\"\"\n",
        "    Create a boolean mask with a circle of given center and radius.\n",
        "    \"\"\"\n",
        "    Y, X = np.ogrid[:h, :w]\n",
        "    dist_from_center = np.sqrt((X - center[0])**2 + (Y - center[1])**2)\n",
        "    mask = dist_from_center <= radius\n",
        "    return mask\n",
        "\n",
        "def draw_msra_gaussian(heatmap, center, sigma):\n",
        "    \"\"\"\n",
        "    Draw a gaussian blob onto the heatmap centered at `center` with standard deviation `sigma`.\n",
        "    \"\"\"\n",
        "    tmp_size = sigma * 3\n",
        "    mu_x = int(center[0] + 0.5)\n",
        "    mu_y = int(center[1] + 0.5)\n",
        "    h, w = heatmap.shape[0], heatmap.shape[1]\n",
        "    ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
        "    br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
        "    if ul[0] >= w or ul[1] >= h or br[0] < 0 or br[1] < 0:\n",
        "        return heatmap\n",
        "    size = 2 * tmp_size + 1\n",
        "    x = np.arange(0, size, 1, np.float32)\n",
        "    y = x[:, np.newaxis]\n",
        "    x0 = y0 = size // 2\n",
        "    g = np.exp(-((x - x0)**2 + (y - y0)**2) / (2 * sigma**2))\n",
        "    g_x = max(0, -ul[0]), min(br[0], w) - ul[0]\n",
        "    g_y = max(0, -ul[1]), min(br[1], h) - ul[1]\n",
        "    img_x = max(0, ul[0]), min(br[0], w)\n",
        "    img_y = max(0, ul[1]), min(br[1], h)\n",
        "    heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]] = np.maximum(\n",
        "        heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]],\n",
        "        g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
        "    )\n",
        "    return heatmap\n",
        "\n",
        "def compute_key_patch_map(mask_orig, R=8, N=5):\n",
        "    \"\"\"\n",
        "    Compute the key-patch (ground truth point) map for a given mask.\n",
        "\n",
        "    This function:\n",
        "      1. Resizes the input mask (assumed to be 1024x1024) to 512x512.\n",
        "      2. Thresholds it to obtain a binary mask.\n",
        "      3. Downsamples the binary mask further for contour extraction.\n",
        "      4. Finds contours and, for each contour, selects keypoints based on neighborhood overlap statistics.\n",
        "      5. Draws a gaussian on a heatmap for each selected keypoint.\n",
        "\n",
        "    Returns the 512x512 generated heatmap.\n",
        "    \"\"\"\n",
        "    # Resize the original mask to 512x512\n",
        "    mask_resized = resize_and_clip(mask_orig, (512, 512))\n",
        "\n",
        "    # Threshold mask to binary (pixels > 127 become foreground)\n",
        "    _, mask_bin = cv2.threshold(mask_resized, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Downsample for contour extraction: using factor 2 (512 -> 256)\n",
        "    mask_small = cv2.resize(mask_bin, (mask_bin.shape[1] // 2, mask_bin.shape[0] // 2), interpolation=cv2.INTER_NEAREST)\n",
        "    mask_small = np.uint8(mask_small)\n",
        "\n",
        "    # Extract contours\n",
        "    contours, _ = cv2.findContours(mask_small, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "    # Initialize heatmap of size 512x512\n",
        "    point_heatmap = np.zeros((512, 512), dtype=np.float32)\n",
        "\n",
        "    for contour in contours:\n",
        "        points = contour[:, 0]  # Each point in contour is (x, y)\n",
        "        # Scale points to the 512x512 coordinate system\n",
        "        points = points * 2\n",
        "        points_number = points.shape[0]\n",
        "        if points_number < 20:\n",
        "            continue\n",
        "\n",
        "        # Adjust parameters for pneumothorax segmentation:\n",
        "        if points_number < 150:\n",
        "            radius = R\n",
        "            neighbor_points_n_oneside = N\n",
        "        elif points_number < 300:\n",
        "            radius = R + 4\n",
        "            neighbor_points_n_oneside = N + 5\n",
        "        else:\n",
        "            radius = R + 7\n",
        "            neighbor_points_n_oneside = N + 10\n",
        "\n",
        "        # Calculate the overlap statistic for each point in the contour.\n",
        "        stds = []\n",
        "        for i in range(points_number):\n",
        "            pt = points[i]\n",
        "            mask_circ = create_circular_mask(512, 512, (pt[0], pt[1]), radius)\n",
        "            overlap = np.sum(mask_circ * (mask_resized > 0)) / (np.pi * radius * radius)\n",
        "            stds.append(overlap)\n",
        "        stds = np.array(stds)\n",
        "\n",
        "        # Select keypoints by comparing each point's overlap with its neighbors.\n",
        "        for i in range(points_number):\n",
        "            neighbor_idx = np.concatenate([\n",
        "                np.arange(-neighbor_points_n_oneside, 0),\n",
        "                np.arange(1, neighbor_points_n_oneside + 1)\n",
        "            ]) + i\n",
        "            # Handle wrap-around indexing\n",
        "            neighbor_idx[neighbor_idx < 0] += points_number\n",
        "            neighbor_idx[neighbor_idx >= points_number] -= points_number\n",
        "\n",
        "            if stds[i] < np.min(stds[neighbor_idx]) or stds[i] > np.max(stds[neighbor_idx]):\n",
        "                point_heatmap = draw_msra_gaussian(point_heatmap, (points[i, 0], points[i, 1]), sigma=R)\n",
        "\n",
        "    return point_heatmap\n",
        "\n",
        "# =============================================================================\n",
        "# Modified Dataset Class\n",
        "# =============================================================================\n",
        "\n",
        "class Dataset():\n",
        "    def __init__(self, rle_df, image_base_dir, masks_base_dir, augmentation=None, mask_augmentation=None):\n",
        "        self.df                 = rle_df\n",
        "        self.image_base_dir     = image_base_dir\n",
        "        self.masks_base_dir     = masks_base_dir\n",
        "        self.image_ids          = rle_df.ImageId.values\n",
        "        self.augmentation       = augmentation\n",
        "        self.mask_augmentation  = mask_augmentation\n",
        "\n",
        "    def __image_ids__(self):\n",
        "        print(self.image_ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image_id  = self.image_ids[i]\n",
        "        img_path  = os.path.join(self.image_base_dir, Path(image_id + '.png'))\n",
        "        mask_path = os.path.join(self.masks_base_dir, Path(image_id + '.png'))\n",
        "\n",
        "        # Load image and mask using OpenCV.\n",
        "        # Image is loaded in color (RGB) for compatibility with models and augmentations.\n",
        "        image     = cv2.imread(img_path, 1)\n",
        "        # Mask is loaded in grayscale.\n",
        "        mask      = cv2.imread(mask_path, 0)\n",
        "\n",
        "        # Normalize image (scale to [0, 1]) and convert mask to binary.\n",
        "        image = (image / 255.0).astype(np.float32)\n",
        "        mask = (mask > 0).astype(np.float32)\n",
        "\n",
        "        # Generate the ground truth key-patch map (\"point\") on the fly.\n",
        "        # The compute function internally resizes the original mask (from 1024x1024 to 512x512) and generates the heatmap.\n",
        "        point = compute_key_patch_map(mask * 255)\n",
        "        # Note: We multiply mask by 255 because when read as grayscale it might be in {0,1} (after normalization).\n",
        "        # If your input mask is still in [0,255] scale before thresholding, adjust accordingly.\n",
        "\n",
        "        # Apply augmentations (if provided) to the image and mask.\n",
        "        if self.augmentation and self.mask_augmentation:\n",
        "            img_sample = {\"image\": image}\n",
        "            img_sample = self.augmentation(**img_sample)\n",
        "            image = img_sample['image']\n",
        "            mask_sample = {\"image\": mask}\n",
        "            mask_sample = self.mask_augmentation(**mask_sample)\n",
        "            mask = mask_sample[\"image\"]\n",
        "            # Note: We are not applying augmentation to the computed point map.\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'mask' : mask,\n",
        "            'point': torch.unsqueeze(torch.Tensor(point), axis=0)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxv5It8ZK2Sw",
        "outputId": "81222e29-1ad2-4f4a-96f8-17635f8098f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Test transforms\n",
        "TEST_TFMS = albu.Compose([\n",
        "    albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1),\n",
        "    albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
        "    ToTensorV2(),\n",
        "],\n",
        "    is_check_shapes=True,\n",
        "    p=1.0,\n",
        ")\n",
        "\n",
        "# New Train Transforms - Aggressive Augmentations to avoid overfitting.\n",
        "TFMS =  albu.Compose(\n",
        "    [\n",
        "        albu.OneOf([\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.OneOf([\n",
        "            albu.ElasticTransform(alpha=120, sigma=6.0, p=0.5),\n",
        "            albu.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), p=0.5),\n",
        "            albu.OpticalDistortion(distort_limit=(-2, 2), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, p=0.5),\n",
        "        albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "        albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1.0),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    p=1.0\n",
        ")\n",
        "\n",
        "MASK_TFMS =  albu.Compose(\n",
        "    [\n",
        "        albu.OneOf([\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.OneOf([\n",
        "            albu.ElasticTransform(alpha=120, sigma=6.0, p=0.5),\n",
        "            albu.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), p=0.5),\n",
        "            albu.OpticalDistortion(distort_limit=(-2, 2), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, p=0.5),\n",
        "        albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    p=1.0\n",
        ")\n",
        "\n",
        "mask_transform = albu.Compose([\n",
        "    albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "    ToTensorV2()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DKpUR1QK2Sw"
      },
      "outputs": [],
      "source": [
        "# train dataset\n",
        "train_dataset = Dataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TFMS, MASK_TFMS)\n",
        "val_dataset   = Dataset(VAL_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TEST_TFMS, mask_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zno_0Q4eVHv",
        "outputId": "3a33d27a-3452-4bcf-b3d6-5f482bb442bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8570"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SW1GpbQK2Sw"
      },
      "outputs": [],
      "source": [
        "# # plot one with mask\n",
        "# visualize(**train_dataset[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-dcysMmK2Sw"
      },
      "source": [
        "### Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx49nrt4K2Sw"
      },
      "outputs": [],
      "source": [
        "class PneumoSampler(Sampler):\n",
        "    def __init__(self, train_df, positive_perc=0.8):\n",
        "        assert positive_perc > 0, 'percentage of positive pneumothorax images must be greater then zero'\n",
        "        self.train_df = train_df\n",
        "        self.positive_perc = positive_perc\n",
        "        self.positive_idxs = self.train_df.query('has_mask==1').index.values\n",
        "        self.negative_idxs = self.train_df.query('has_mask!=1').index.values\n",
        "        self.n_positive = len(self.positive_idxs)\n",
        "        self.n_negative = int(self.n_positive * (1 - self.positive_perc) / self.positive_perc)\n",
        "\n",
        "    def __iter__(self):\n",
        "        negative_sample = np.random.choice(self.negative_idxs, size=self.n_negative)\n",
        "        shuffled = np.random.permutation(np.hstack((negative_sample, self.positive_idxs)))\n",
        "        return iter(shuffled.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_positive + self.n_negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzJerUgtK2Sx"
      },
      "outputs": [],
      "source": [
        "SAMPLER = PneumoSampler(TRAIN_DF, positive_perc=POSTIVE_PERC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asd-jMF9K2Sx"
      },
      "source": [
        "### DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Saav3bv6K2Sx",
        "outputId": "b8e50c74-1eee-4e4d-9eef-a8f85e035b12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE,\n",
        "                              shuffle=True if not USE_SAMPLER else False,\n",
        "                              num_workers=NUM_WORKERS,\n",
        "                              sampler=SAMPLER if USE_SAMPLER else None)\n",
        "val_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9nGVmHlj6z5",
        "outputId": "a1383131-4b72-45da-af36-4daeb518aad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7fe78c131490>, <torch.utils.data.dataloader.DataLoader object at 0x7fe78ba1e910>)\n",
            "Length of train dataloader: 1190 batches of 2\n",
            "length of validation dataloader: 1071 batches of 2\n"
          ]
        }
      ],
      "source": [
        "print(f\"Dataloaders: {train_dataloader , val_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"length of validation dataloader: {len(val_dataloader)} batches of {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jmBiDQJwFTN"
      },
      "source": [
        "for batch_index, data in enumerate(train_dataloader):\n",
        "    for z in range(3):\n",
        "        print(\"Image: \", data['image'].shape)\n",
        "        print(\"Mask: \", data['mask'].shape)\n",
        "        print(\"data['mask'].unsqueeze(1)\", data['mask'].unsqueeze(1).shape)\n",
        "    break\n",
        "\n",
        "Output\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPWAK3BK9UPJ"
      },
      "source": [
        "## NonBlockND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJdpZrHW9UPJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class _NonLocalBlockND(nn.Module):\n",
        "    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n",
        "        \"\"\"\n",
        "        :param in_channels:\n",
        "        :param inter_channels:\n",
        "        :param dimension:\n",
        "        :param sub_sample:\n",
        "        :param bn_layer:\n",
        "        \"\"\"\n",
        "\n",
        "        super(_NonLocalBlockND, self).__init__()\n",
        "\n",
        "        assert dimension in [1, 2, 3]\n",
        "\n",
        "        self.dimension = dimension\n",
        "        self.sub_sample = sub_sample\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.inter_channels = inter_channels\n",
        "\n",
        "        if self.inter_channels is None:\n",
        "            self.inter_channels = in_channels // 2\n",
        "            if self.inter_channels == 0:\n",
        "                self.inter_channels = 1\n",
        "\n",
        "        if dimension == 3:\n",
        "            conv_nd = nn.Conv3d\n",
        "            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "            bn = nn.BatchNorm3d\n",
        "        elif dimension == 2:\n",
        "            conv_nd = nn.Conv2d\n",
        "            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "            bn = nn.BatchNorm2d\n",
        "        else:\n",
        "            conv_nd = nn.Conv1d\n",
        "            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n",
        "            bn = nn.BatchNorm1d\n",
        "\n",
        "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                         kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        if bn_layer:\n",
        "            self.W = nn.Sequential(\n",
        "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
        "                        kernel_size=1, stride=1, padding=0),\n",
        "                bn(self.in_channels)\n",
        "            )\n",
        "            nn.init.constant_(self.W[1].weight, 0)\n",
        "            nn.init.constant_(self.W[1].bias, 0)\n",
        "        else:\n",
        "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
        "                             kernel_size=1, stride=1, padding=0)\n",
        "            nn.init.constant_(self.W.weight, 0)\n",
        "            nn.init.constant_(self.W.bias, 0)\n",
        "\n",
        "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                             kernel_size=1, stride=1, padding=0)\n",
        "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                           kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        if sub_sample:\n",
        "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
        "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
        "\n",
        "    def forward(self, x, y, return_nl_map=False):\n",
        "        \"\"\"\n",
        "        :param x: (b, c, h, w)\n",
        "        :param y: (b, c, 1)\n",
        "        :param return_nl_map: if True return z, nl_map, else only return z.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        h, w = x.shape[2:]\n",
        "\n",
        "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
        "        #g_x = g_x.permute(0, 2, 1)\n",
        "\n",
        "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
        "        theta_x = theta_x.permute(0, 2, 1)\n",
        "\n",
        "        #phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
        "        phi_x = self.phi(y.unsqueeze(-1)).view(batch_size, self.inter_channels, -1)\n",
        "        f = torch.matmul(theta_x, phi_x)\n",
        "        #f_div_C = F.softmax(f, dim=-1)\n",
        "        f_div_C = torch.sigmoid(f)\n",
        "        f_div_C = f_div_C.permute(0,2,1).contiguous()\n",
        "\n",
        "        #y = torch.matmul(f_div_C, g_x)\n",
        "        #y = y.permute(0, 2, 1).contiguous()\n",
        "        y = g_x * f_div_C\n",
        "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
        "        W_y = self.W(y)\n",
        "        z = W_y + x\n",
        "\n",
        "        if return_nl_map:\n",
        "            return z, f_div_C.view(batch_size, 1, h, w)\n",
        "        return z\n",
        "\n",
        "class NONLocalBlock2D(_NonLocalBlockND):\n",
        "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
        "        super(NONLocalBlock2D, self).__init__(in_channels,\n",
        "                                              inter_channels=inter_channels,\n",
        "                                              dimension=2, sub_sample=sub_sample,\n",
        "                                              bn_layer=bn_layer,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld_aLxHH9UPK"
      },
      "source": [
        "## BAG - Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAar3gnt9UPK"
      },
      "source": [
        "### process_point.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKkD4dhS9UPK"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import skimage.draw\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def create_circular_mask(h, w, center, radius):\n",
        "    Y, X = np.ogrid[:h, :w]\n",
        "    dist_from_center = np.sqrt((X - center[0])**2 + (Y - center[1])**2)\n",
        "    mask = dist_from_center <= radius\n",
        "    return mask\n",
        "\n",
        "\n",
        "def NMS(heatmap, kernel=13):\n",
        "    hmax = F.max_pool2d(heatmap, kernel, stride=1, padding=(kernel - 1) // 2)\n",
        "    keep = (hmax == heatmap).float()\n",
        "    return heatmap * keep, hmax, keep\n",
        "\n",
        "\n",
        "def draw_msra_gaussian(heatmap, center, sigma):\n",
        "    tmp_size = sigma * 3\n",
        "    mu_x = int(center[0] + 0.5)\n",
        "    mu_y = int(center[1] + 0.5)\n",
        "    w, h = heatmap.shape[0], heatmap.shape[1]\n",
        "    ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
        "    br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
        "    if ul[0] >= h or ul[1] >= w or br[0] < 0 or br[1] < 0:\n",
        "        return heatmap\n",
        "    size = 2 * tmp_size + 1\n",
        "    x = np.arange(0, size, 1, np.float32)\n",
        "    y = x[:, np.newaxis]\n",
        "    x0 = y0 = size // 2\n",
        "    g = np.exp(-((x - x0)**2 + (y - y0)**2) / (2 * sigma**2))\n",
        "    g_x = max(0, -ul[0]), min(br[0], h) - ul[0]\n",
        "    g_y = max(0, -ul[1]), min(br[1], w) - ul[1]\n",
        "    img_x = max(0, ul[0]), min(br[0], h)\n",
        "    img_y = max(0, ul[1]), min(br[1], w)\n",
        "    heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]] = np.maximum(\n",
        "        heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]], g[g_y[0]:g_y[1],\n",
        "                                                         g_x[0]:g_x[1]])\n",
        "    return heatmap\n",
        "\n",
        "\n",
        "def kpm_gen(label_path, R, N):\n",
        "    label = np.load(label_path)\n",
        "    #     label = label[0]\n",
        "    label_ori = label.copy()\n",
        "    label = label[::4, ::4]\n",
        "    label = np.uint8(label * 255)\n",
        "    contours, hierarchy = cv2.findContours(label, cv2.RETR_LIST,\n",
        "                                           cv2.CHAIN_APPROX_NONE)\n",
        "    contour_len = len(contours)\n",
        "\n",
        "    label = np.repeat(label[..., np.newaxis], 3, axis=-1)\n",
        "    draw_label = cv2.drawContours(label.copy(), contours, -1, (0, 0, 255), 1)\n",
        "\n",
        "    point_file = []\n",
        "    if contour_len == 0:\n",
        "        point_heatmap = np.zeros((512, 512))\n",
        "    else:\n",
        "        point_heatmap = np.zeros((512, 512))\n",
        "        for contour in contours:\n",
        "            stds = []\n",
        "            points = contour[:, 0]  # (N,2)\n",
        "            points = points * 4\n",
        "            points_number = contour.shape[0]\n",
        "            if points_number < 30:\n",
        "                continue\n",
        "\n",
        "            if points_number < 100:\n",
        "                radius = 6\n",
        "                neighbor_points_n_oneside = 3\n",
        "            elif points_number < 200:\n",
        "                radius = 10\n",
        "                neighbor_points_n_oneside = 15\n",
        "            elif points_number < 300:\n",
        "                radius = 10\n",
        "                neighbor_points_n_oneside = 20\n",
        "            elif points_number < 350:\n",
        "                radius = 15\n",
        "                neighbor_points_n_oneside = 30\n",
        "            else:\n",
        "                radius = 10\n",
        "                neighbor_points_n_oneside = 40\n",
        "\n",
        "            for i in range(points_number):\n",
        "                current_point = points[i]\n",
        "                mask = create_circular_mask(512, 512, points[i], radius)\n",
        "                overlap_area = np.sum(\n",
        "                    mask * label_ori) / (np.pi * radius * radius)\n",
        "                stds.append(overlap_area)\n",
        "            print(\"stds len: \", len(stds))\n",
        "\n",
        "            # show\n",
        "            selected_points = []\n",
        "            stds = np.array(stds)\n",
        "            neighbor_points = []\n",
        "            for i in range(len(points)):\n",
        "                current_point = points[i]\n",
        "                neighbor_points_index = np.concatenate([\n",
        "                    np.arange(-neighbor_points_n_oneside, 0),\n",
        "                    np.arange(1, neighbor_points_n_oneside + 1)\n",
        "                ]) + i\n",
        "                neighbor_points_index[np.where(\n",
        "                    neighbor_points_index < 0)[0]] += len(points)\n",
        "                neighbor_points_index[np.where(\n",
        "                    neighbor_points_index > len(points) - 1)[0]] -= len(points)\n",
        "                if stds[i] < np.min(\n",
        "                        stds[neighbor_points_index]) or stds[i] > np.max(\n",
        "                            stds[neighbor_points_index]):\n",
        "                    #                     print(points[i])\n",
        "                    point_heatmap = draw_msra_gaussian(\n",
        "                        point_heatmap, (points[i, 0], points[i, 1]), 5)\n",
        "                    selected_points.append(points[i])\n",
        "\n",
        "            print(\"selected_points num: \", len(selected_points))\n",
        "            #             print(selected_points)\n",
        "            maskk = np.zeros((512, 512))\n",
        "            rr, cc = skimage.draw.polygon(\n",
        "                np.array(selected_points)[:, 1],\n",
        "                np.array(selected_points)[:, 0])\n",
        "            maskk[rr, cc] = 1\n",
        "            intersection = np.logical_and(label_ori, maskk)\n",
        "            union = np.logical_or(label_ori, maskk)\n",
        "            iou_score = np.sum(intersection) / np.sum(union)\n",
        "            print(iou_score)\n",
        "    return label_ori, point_heatmap\n",
        "\n",
        "\n",
        "def point_gen_isic2018():\n",
        "    R = 10\n",
        "    N = 25\n",
        "    data_dir = '/raid/wjc/data/skin_lesion/isic2018/Label'\n",
        "\n",
        "    save_dir = data_dir.replace('Label', 'Point')\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    path_list = os.listdir(data_dir)\n",
        "    path_list.sort()\n",
        "    num = 0\n",
        "    for path in tqdm(path_list):\n",
        "        name = path[:-4]\n",
        "        label_path = os.path.join(data_dir, path)\n",
        "        print(label_path)\n",
        "        label_ori, point_heatmap = kpm_gen(label_path, R, N)\n",
        "\n",
        "        save_path = os.path.join(save_dir, name + '.npy')\n",
        "        np.save(save_path, point_heatmap)\n",
        "        num += 1\n",
        "\n",
        "\n",
        "def point_gen_isic2016():\n",
        "    R = 10\n",
        "    N = 25\n",
        "    for split in ['Train', 'Test', 'Validation']:\n",
        "        data_dir = '/raid/wjc/data/skin_lesion/isic2016/{}/Label'.format(split)\n",
        "\n",
        "        save_dir = data_dir.replace('Label', 'Point')\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        path_list = os.listdir(data_dir)\n",
        "        path_list.sort()\n",
        "        num = 0\n",
        "        for path in tqdm(path_list):\n",
        "            name = path[:-4]\n",
        "            label_path = os.path.join(data_dir, path)\n",
        "            print(label_path)\n",
        "            label_ori, point_heatmap = kpm_gen(label_path, R, N)\n",
        "            save_path = os.path.join(save_dir, name + '.npy')\n",
        "            np.save(save_path, point_heatmap)\n",
        "            num += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rpAsnr89UPL"
      },
      "source": [
        "## BAT utility modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mqFXmim9UPL"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=512,\n",
        "                 dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "\n",
        "    def forward(self, tgt, src):\n",
        "        \"tgt shape: Batch_size, C, H, W \"\n",
        "        \"src shape: Batch_size, 1, C    \"\n",
        "\n",
        "        B, C, h, w = tgt.shape\n",
        "        tgt = tgt.view(B, C, h*w).permute(2,0,1)  # shape: L, B, C\n",
        "\n",
        "        src = src.permute(1,0,2)  # shape: Q:1, B, C\n",
        "\n",
        "        fusion_feature = self.cross_attn(query=tgt,\n",
        "                                         key=src,\n",
        "                                         value=src)[0]\n",
        "        tgt = tgt + self.dropout1(fusion_feature)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt1 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout2(tgt1)\n",
        "        tgt = self.norm2(tgt)\n",
        "        return tgt.permute(1, 2, 0).view(B, C, h, w)\n",
        "\n",
        "class BoundaryCrossAttention(CrossAttention):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=True,\n",
        "                 dim_feedforward=512,\n",
        "                 dropout=0.0):\n",
        "        super().__init__(d_model, nhead, dim_feedforward, dropout)\n",
        "\n",
        "        #self.BAG = nn.Sequential(\n",
        "        #    nn.Conv2d(d_model, d_model, kernel_size=3, padding=1, bias=False),\n",
        "        #    nn.BatchNorm2d(d_model),\n",
        "        #    nn.ReLU(inplace=False),\n",
        "        #    nn.Conv2d(d_model, d_model, kernel_size=3, padding=1, bias=False),\n",
        "        #    nn.BatchNorm2d(d_model),\n",
        "        #    nn.ReLU(inplace=False),\n",
        "        #    nn.Conv2d(d_model, 1, kernel_size=1))\n",
        "        self.BAG_type = BAG_type\n",
        "        if self.BAG_type == '1D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous1D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate1D(d_model)\n",
        "        elif self.BAG_type == '2D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous2D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate2D(d_model)\n",
        "\n",
        "    def forward(self, tgt, src):\n",
        "        \"tgt shape: Batch_size, C, H, W \"\n",
        "        \"src shape: Batch_size, 1, C    \"\n",
        "\n",
        "        B, C, h, w = tgt.shape\n",
        "        tgt = tgt.view(B, C, h*w).permute(2,0,1)  # shape: L, B, C\n",
        "\n",
        "        src = src.permute(1,0,2)  # shape: Q:1, B, C\n",
        "\n",
        "        fusion_feature = self.cross_attn(query=tgt,\n",
        "                                         key=src,\n",
        "                                         value=src)[0]\n",
        "        tgt = tgt + self.dropout1(fusion_feature)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt1 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout2(tgt1)\n",
        "        tgt = self.norm2(tgt)\n",
        "\n",
        "        if self.BAG_type == '1D':\n",
        "            tgt = tgt.permute(1,2,0)\n",
        "            tgt, weights = self.BAG(tgt)\n",
        "            tgt = tgt.view(B, C, h, w).contiguous()\n",
        "            weights = weights.view(B, 1, h, w)\n",
        "        elif self.BAG_type == '2D':\n",
        "            tgt = tgt.permute(1,2,0).view(B, C, h, w)\n",
        "            tgt, weights = self.BAG(tgt)\n",
        "            tgt = tgt.contiguous()\n",
        "        return tgt, weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements a multi head attention module like proposed in:\n",
        "    https://arxiv.org/abs/2005.12872\n",
        "    \"\"\"\n",
        "    def __init__(self, query_dimension: int = 64, hidden_features: int = 64, number_of_heads: int = 16,\n",
        "                 dropout: float = 0.0) -> None:\n",
        "        \"\"\"\n",
        "        Constructor method\n",
        "        :param query_dimension: (int) Dimension of query tensor\n",
        "        :param hidden_features: (int) Number of hidden features in detr\n",
        "        :param number_of_heads: (int) Number of prediction heads\n",
        "        :param dropout: (float) Dropout factor to be utilized\n",
        "        \"\"\"\n",
        "        # Call super constructor\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Save parameters\n",
        "        self.hidden_features = hidden_features\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.dropout = dropout\n",
        "        # Init layer\n",
        "        self.layer_box_embedding = nn.Linear(in_features=query_dimension, out_features=hidden_features, bias=True)\n",
        "        # Init convolution layer\n",
        "        self.layer_image_encoding = nn.Conv2d(in_channels=query_dimension, out_channels=hidden_features,\n",
        "                                              kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=True)\n",
        "        # Init normalization factor\n",
        "        self.normalization_factor = torch.tensor(self.hidden_features / self.number_of_heads, dtype=torch.float).sqrt()\n",
        "\n",
        "        # Linear\n",
        "        self.linear = nn.Linear(in_features=number_of_heads, out_features=1)\n",
        "\n",
        "    def forward(self, input_box_embeddings: torch.Tensor, input_image_encoding: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        :param input_box_embeddings: (torch.Tensor) Bounding box embeddings\n",
        "        :param input_image_encoding: (torch.Tensor) Encoded image of the transformer encoder\n",
        "        :return: (torch.Tensor) Attention maps of shape (batch size, n, m, height, width)\n",
        "        \"\"\"\n",
        "        # Map box embeddings\n",
        "        output_box_embeddings = self.layer_box_embedding(input_box_embeddings)\n",
        "        # Map image features\n",
        "        output_image_encoding = self.layer_image_encoding(input_image_encoding)\n",
        "        # Reshape output box embeddings\n",
        "        output_box_embeddings = output_box_embeddings.view(output_box_embeddings.shape[0],\n",
        "                                                           output_box_embeddings.shape[1],\n",
        "                                                           self.number_of_heads,\n",
        "                                                           self.hidden_features // self.number_of_heads)\n",
        "        # Reshape output image encoding\n",
        "        output_image_encoding = output_image_encoding.view(output_image_encoding.shape[0],\n",
        "                                                           self.number_of_heads,\n",
        "                                                           self.hidden_features // self.number_of_heads,\n",
        "                                                           output_image_encoding.shape[-2],\n",
        "                                                           output_image_encoding.shape[-1])\n",
        "        # Combine tensors and normalize\n",
        "        output = torch.einsum(\"bqnc,bnchw->bqnhw\",\n",
        "                              output_box_embeddings * self.normalization_factor,\n",
        "                              output_image_encoding)\n",
        "        # Apply softmax\n",
        "        output = F.softmax(output.flatten(start_dim=2), dim=-1).view_as(output)\n",
        "\n",
        "        # Linear: to generate one map\n",
        "        b, _, _, h, w = output.shape\n",
        "        output = torch.sigmoid(self.linear(output.flatten(start_dim=3).permute(0,1,3,2))).view(b,1,h,w)\n",
        "\n",
        "        # Perform dropout if utilized\n",
        "        if self.dropout > 0.0:\n",
        "            output = F.dropout(input=output, p=self.dropout, training=self.training)\n",
        "#         print(\"MultiHead Attention\",output.shape)\n",
        "        return output.contiguous()\n",
        "\n",
        "\n",
        "class BoundaryWiseAttentionGateAtrous2D(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "\n",
        "        super(BoundaryWiseAttentionGateAtrous2D,self).__init__()\n",
        "\n",
        "        modules = []\n",
        "\n",
        "        if hidden_channels == None:\n",
        "            hidden_channels = in_channels // 2\n",
        "\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=1, dilation=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=2, dilation=2, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=4, dilation=4, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=6, dilation=6, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "\n",
        "        self.convs = nn.ModuleList(modules)\n",
        "\n",
        "        self.conv_out = nn.Conv2d(5 * hidden_channels, 1, 1, bias=False)\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, H, W \"\n",
        "        \" return: feature, weight (B,C,H,W) \"\n",
        "        res = []\n",
        "        for conv in self.convs:\n",
        "            res.append(conv(x))\n",
        "        res = torch.cat(res, dim=1)\n",
        "        weight = torch.sigmoid(self.conv_out(res))\n",
        "        x = x * weight + x\n",
        "        return x, weight\n",
        "\n",
        "class BoundaryWiseAttentionGateAtrous1D(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "\n",
        "        super(BoundaryWiseAttentionGateAtrous1D,self).__init__()\n",
        "\n",
        "        modules = []\n",
        "\n",
        "        if hidden_channels == None:\n",
        "            hidden_channels = in_channels // 2\n",
        "\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 1, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=1, dilation=1, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=2, dilation=2, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=4, dilation=4, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=6, dilation=6, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "\n",
        "        self.convs = nn.ModuleList(modules)\n",
        "\n",
        "        self.conv_out = nn.Conv1d(5 * hidden_channels, 1, 1, bias=False)\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, L \"\n",
        "        \" return: feature, weight (B,C,L) \"\n",
        "        res = []\n",
        "        for conv in self.convs:\n",
        "            res.append(conv(x))\n",
        "        res = torch.cat(res, dim=1)\n",
        "        weight = torch.sigmoid(self.conv_out(res))\n",
        "        x = x * weight + x\n",
        "        return x, weight\n",
        "\n",
        "class BoundaryWiseAttentionGate2D(nn.Sequential):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "        super(BoundaryWiseAttentionGate2D,self).__init__(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels, 1, kernel_size=1))\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, H, W \"\n",
        "        \" return: feature, weight (B,C,H,W) \"\n",
        "        weight = torch.sigmoid(super(BoundaryWiseAttentionGate2D,self).forward(x))\n",
        "        x = x * weight + x\n",
        "        return x, weight\n",
        "\n",
        "class BoundaryWiseAttentionGate1D(nn.Sequential):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "        super(BoundaryWiseAttentionGate1D,self).__init__(\n",
        "            nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv1d(in_channels, 1, kernel_size=1))\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, L \"\n",
        "        \" return: feature, weight (B,C,L) \"\n",
        "        weight = torch.sigmoid(super(BoundaryWiseAttentionGate1D,self).forward(x))\n",
        "        x = x * weight + x\n",
        "        return x, weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wyBin1-klDc"
      },
      "source": [
        "## transformer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7bRaHbhklDd"
      },
      "outputs": [],
      "source": [
        "# Based on: https://github.com/facebookresearch/detr/blob/master/models/transformer.py\n",
        "import copy\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "# from .BAT_Modules import BoundaryWiseAttentionGate2D, BoundaryWiseAttentionGate1D, BoundaryWiseAttentionGateAtrous2D, BoundaryWiseAttentionGateAtrous1D\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers,\n",
        "                                          encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        #         print(\"Trans Encoder\",memory.shape)\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers=6,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = BoundaryAwareTransformerEncoderLayer(\n",
        "            d_model, nhead, BAG_type, Atrous, dim_feedforward, dropout,\n",
        "            activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = BoundaryAwareTransformerEncoder(point_pred_layers,\n",
        "                                                       encoder_layer,\n",
        "                                                       num_encoder_layers,\n",
        "                                                       encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory, weights = self.encoder(src,\n",
        "                                       src_key_padding_mask=mask,\n",
        "                                       pos=pos_embed,\n",
        "                                       height=h,\n",
        "                                       width=w)\n",
        "\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h,\n",
        "                                                                w), weights\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask,\n",
        "                           pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers,\n",
        "                 encoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None):\n",
        "        super().__init__()\n",
        "        self.point_pred_layers = point_pred_layers\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        output = src\n",
        "        weights = []\n",
        "\n",
        "        for layer_i, layer in enumerate(self.layers):\n",
        "            if layer_i > self.num_layers - self.point_pred_layers - 1:\n",
        "                output, weight = layer(\n",
        "                    True,\n",
        "                    output,\n",
        "                    src_mask=mask,\n",
        "                    src_key_padding_mask=src_key_padding_mask,\n",
        "                    pos=pos,\n",
        "                    height=height,\n",
        "                    width=width)\n",
        "                weights.append(weight)\n",
        "            else:\n",
        "                output = layer(False,\n",
        "                               output,\n",
        "                               src_mask=mask,\n",
        "                               src_key_padding_mask=src_key_padding_mask,\n",
        "                               pos=pos,\n",
        "                               height=height,\n",
        "                               width=width)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src2,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoderLayer(TransformerEncoderLayer):\n",
        "    \"    Add Boundary-wise Attention Gate to Transformer's Encoder\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=True,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation,\n",
        "                         normalize_before)\n",
        "        if BAG_type == '1D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous1D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate1D(d_model)\n",
        "        elif BAG_type == '2D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous2D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate2D(d_model)\n",
        "        self.BAG_type = BAG_type\n",
        "\n",
        "    def forward(self,\n",
        "                use_bag,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        if self.normalize_before:\n",
        "            features = self.forward_pre(src, src_mask, src_key_padding_mask,\n",
        "                                        pos)\n",
        "            if use_bag:\n",
        "                b, c = features.shape[1:]\n",
        "                if self.BAG_type == '1D':\n",
        "                    features = features.permute(1, 2, 0)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.permute(2, 0, 1).contiguous()\n",
        "                    weights = weights.view(b, 1, height, width)\n",
        "                elif self.BAG_type == '2D':\n",
        "                    features = features.permute(1, 2,\n",
        "                                                0).view(b, c, height, width)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.flatten(2).permute(2, 0,\n",
        "                                                           1).contiguous()\n",
        "                return features, weights\n",
        "            else:\n",
        "                return features\n",
        "        features = self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "        if use_bag:\n",
        "            b, c = features.shape[1:]\n",
        "            if self.BAG_type == '1D':\n",
        "                features = features.permute(1, 2, 0)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.permute(2, 0, 1).contiguous()\n",
        "                weights = weights.view(b, 1, height, width)\n",
        "            elif self.BAG_type == '2D':\n",
        "                features = features.permute(1, 2, 0).view(b, c, height, width)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.flatten(2).permute(2, 0, 1).contiguous()\n",
        "            return features, weights\n",
        "        else:\n",
        "            return features\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 decoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None,\n",
        "                 return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           memory,\n",
        "                           tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos,\n",
        "                           query_pos=query_pos)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model,\n",
        "                                                    nhead,\n",
        "                                                    dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     tgt,\n",
        "                     memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    tgt,\n",
        "                    memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt2,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask,\n",
        "                                    memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask,\n",
        "                                 pos, query_pos)\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args.hidden_dim,\n",
        "        dropout=args.dropout,\n",
        "        nhead=args.nheads,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        num_encoder_layers=args.enc_layers,\n",
        "        num_decoder_layers=args.dec_layers,\n",
        "        normalize_before=args.pre_norm,\n",
        "        return_intermediate_dec=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"leaky relu\":\n",
        "        return F.leaky_relu\n",
        "    if activation == \"selu\":\n",
        "        return F.selu\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(\n",
        "        F\"activation should be relu, gelu, glu, leaky relu or selu, not {activation}.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLaynkwn9UPM"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsyy9B33klD8"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50, resnet34, resnet18, ResNet50_Weights, ResNet18_Weights, ResNet34_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlV1_wDH9UPM"
      },
      "outputs": [],
      "source": [
        "# For ResNet18, use layers up to layer3 so that the output has OS16.\n",
        "def ResNet18_OS16(multi_scale=False):\n",
        "    resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "    # Original resnet18 children:\n",
        "    # [conv1, bn1, relu, maxpool, layer1, layer2, layer3, layer4, avgpool, fc]\n",
        "    # Removing layer4, avgpool and fc gives a feature map of size roughly (B, 256, H/16, W/16)\n",
        "    features = nn.Sequential(*list(resnet.children())[:-3])\n",
        "    return features\n",
        "\n",
        "# For ResNet50, adjust layer4 to preserve OS16.\n",
        "def ResNet50_OS16(multi_scale=False):\n",
        "    resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "    # The children of resnet50 are:\n",
        "    # [conv1, bn1, relu, maxpool, layer1, layer2, layer3, layer4, avgpool, fc]\n",
        "    # To achieve OS16 instead of OS32, modify layer4 so that it does not downsample:\n",
        "    #  - Change the stride in the first Bottleneck of layer4 from 2 to 1.\n",
        "    resnet.layer4[0].conv2.stride = (1, 1)\n",
        "    if resnet.layer4[0].downsample:  # adjust the downsampling layer as well\n",
        "        resnet.layer4[0].downsample[0].stride = (1, 1)\n",
        "    # Additionally, increase dilation in layer4 so that its receptive field remains large.\n",
        "    for m in resnet.layer4.modules():\n",
        "        if isinstance(m, nn.Conv2d) and m.kernel_size == (3, 3):\n",
        "            m.dilation = (2, 2)\n",
        "            m.padding = (2, 2)\n",
        "    # Now remove the avgpool and fc layers.\n",
        "    features = nn.Sequential(*list(resnet.children())[:-2])\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFIKfzTH9UPN"
      },
      "source": [
        "## Atrous Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMt0ON6a9UPO"
      },
      "outputs": [],
      "source": [
        "# # camera-ready\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class ASPP(nn.Module):\n",
        "#     def __init__(self, num_classes, head = True):\n",
        "#         super(ASPP, self).__init__()\n",
        "\n",
        "#         self.conv_1x1_1 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "#         self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.conv_3x3_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "#         self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.conv_3x3_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "#         self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.conv_3x3_3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "#         self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "#         self.conv_1x1_2 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "#         self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "#         self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         if head:\n",
        "#             self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "#         self.head = head\n",
        "\n",
        "#     def forward(self, feature_map):\n",
        "#         # (feature_map has shape (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16. If self.resnet instead is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8))\n",
        "\n",
        "#         feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "#         feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "#         out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "#         out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "#         out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "#         out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "#         out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "#         out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "#         out_img = F.interpolate(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "#         out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "#         out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "#         if self.head:\n",
        "#             out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "\n",
        "#         return out\n",
        "\n",
        "# class ASPP_Bottleneck(nn.Module):\n",
        "#     def __init__(self, num_classes):\n",
        "#         super(ASPP_Bottleneck, self).__init__()\n",
        "\n",
        "#         self.conv_1x1_1 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "#         self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.conv_3x3_1 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "#         self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.conv_3x3_2 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "#         self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.conv_3x3_3 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "#         self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "#         self.conv_1x1_2 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "#         self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "#         self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "#     def forward(self, feature_map):\n",
        "#         # (feature_map has shape (batch_size, 4*512, h/16, w/16))\n",
        "\n",
        "#         feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "#         feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "#         out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "#         out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "#         out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "#         out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "#         out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "#         out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "#         out_img = F.interpolate(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "#         out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "#         out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "#         out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "\n",
        "#         return out\n",
        "\n",
        "# camera-ready\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, num_classes, head = True):\n",
        "        super(ASPP, self).__init__()\n",
        "\n",
        "        self.conv_1x1_1 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.conv_1x1_2 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        if head:\n",
        "            self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "        self.head = head\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # (feature_map has shape (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16. If self.resnet instead is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8))\n",
        "        print(f\"Shape of feature_map: {feature_map.shape}\")\n",
        "\n",
        "        feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "        feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out_1x1: {out_1x1.shape}\")\n",
        "        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out_3x3_1: {out_3x3_1.shape}\")\n",
        "        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out_3x3_2: {out_3x3_2.shape}\")\n",
        "        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out_3x3_3: {out_3x3_3.shape}\")\n",
        "\n",
        "        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "        print(f\"Shape of out_img after avg_pool: {out_img.shape}\")  \n",
        "        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "        print(f\"Shape of out_img after conv_1x1_2: {out_img.shape}\")\n",
        "        out_img = F.interpolate(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out_img after interpolate: {out_img.shape}\")\n",
        "\n",
        "        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "        print(f\"Shape of out after concatenation: {out.shape}\")\n",
        "        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out after conv_1x1_3: {out.shape}\")\n",
        "        if self.head:\n",
        "            out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "            print(f\"Shape of out after conv_1x1_4 (head): {out.shape}\")\n",
        "\n",
        "        return out\n",
        "\n",
        "class ASPP_Bottleneck(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ASPP_Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv_1x1_1 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_1 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_2 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_3 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.conv_1x1_2 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # (feature_map has shape (batch_size, 4*512, h/16, w/16))\n",
        "        print(f\"Shape of feature_map: {feature_map.shape}\")\n",
        "\n",
        "        feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "        feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out_1x1: {out_1x1.shape}\")\n",
        "        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out_3x3_1: {out_3x3_1.shape}\")\n",
        "        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out_3x3_2: {out_3x3_2.shape}\")\n",
        "        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out_3x3_3: {out_3x3_3.shape}\")\n",
        "\n",
        "        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "        print(f\"Shape of out_img after avg_pool: {out_img.shape}\")\n",
        "        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "        print(f\"Shape of out_img after conv_1x1_2: {out_img.shape}\")\n",
        "        out_img = F.interpolate(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out_img after interpolate: {out_img.shape}\")\n",
        "\n",
        "        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "        print(f\"Shape of out after concatenation: {out.shape}\")\n",
        "        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        print(f\"Shape of out after conv_1x1_3: {out.shape}\")\n",
        "        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "        print(f\"Shape of out after conv_1x1_4: {out.shape}\")\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCLcdf0B9UPN"
      },
      "source": [
        "## DeepLabV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVGsTLZxklEK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import sys\n",
        "# sys.path.insert(0, '../')\n",
        "\n",
        "# from Ours.resnet import ResNet18_OS16, ResNet34_OS16, ResNet50_OS16, ResNet101_OS16, ResNet152_OS16, ResNet18_OS8, ResNet34_OS8\n",
        "# from Ours.ASPP import ASPP, ASPP_Bottleneck\n",
        "\n",
        "# class DeepLabV3(nn.Module): | \"DeepLabV3\" is called as \"base\" in the BAT class\n",
        "# class base(nn.Module):\n",
        "#     def __init__(self, num_classes, num_layers):\n",
        "#         super(base, self).__init__()\n",
        "\n",
        "#         self.num_classes = num_classes\n",
        "#         layers = num_layers\n",
        "#         # NOTE! specify the type of ResNet here\n",
        "#         # NOTE! if you use ResNet50-152, set self.aspp = ASPP_Bottleneck(num_classes=self.num_classes) instead\n",
        "#         if layers == 18:\n",
        "#             self.resnet = ResNet18_OS16()\n",
        "#             self.aspp = ASPP(num_classes=self.num_classes)\n",
        "#         elif layers == 50:\n",
        "#             self.resnet = ResNet50_OS16()\n",
        "#             self.aspp = ASPP_Bottleneck(num_classes=self.num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # (x has shape (batch_size, 3, h, w))\n",
        "#         h = x.size()[2]\n",
        "#         w = x.size()[3]\n",
        "#         feature_map = self.resnet(x)\n",
        "\n",
        "#         # (shape: (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16.\n",
        "#         # If self.resnet is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8).\n",
        "#         # If self.resnet is ResNet50-152, it will be (batch_size, 4*512, h/16, w/16))\n",
        "#         output = self.aspp(\n",
        "#             feature_map)  # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "#         output = F.upsample(\n",
        "#             output, size=(h, w),\n",
        "#             mode=\"bilinear\")  # (shape: (batch_size, num_classes, h, w))\n",
        "#         return output\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# The \"base\" model acts like a DeepLabV3 backbone.\n",
        "# It instantiates a segmentation network using a ResNet backbone (with OS16)\n",
        "# plus an ASPP module. (Ensure that ASPP and ASPP_Bottleneck are defined/imported.)\n",
        "# -------------------------------------------------------------------\n",
        "class base(nn.Module):\n",
        "    def __init__(self, num_classes, num_layers):\n",
        "        super(base, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        layers = num_layers\n",
        "\n",
        "        if layers == 18:\n",
        "            self.resnet = ResNet18_OS16()\n",
        "            self.aspp = ASPP(num_classes=self.num_classes)  # ASPP for BasicBlock variant\n",
        "        elif layers == 50:\n",
        "            self.resnet = ResNet50_OS16()\n",
        "            self.aspp = ASPP_Bottleneck(num_classes=self.num_classes)  # ASPP_Bottleneck for Bottleneck variant\n",
        "        else:\n",
        "            raise ValueError(\"Only 18 and 50 are supported in this example.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is of shape (batch_size, 3, h, w)\n",
        "        h, w = x.size(2), x.size(3)\n",
        "        feature_map = self.resnet(x)  # Expected shape: [B, C, h/16, w/16]\n",
        "        # Process through the ASPP module:\n",
        "        output = self.aspp(feature_map)  # Expected shape: (batch_size, num_classes, h/16, w/16)\n",
        "        # Upsample the output to original image resolution.\n",
        "        output = F.interpolate(output, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZWZ7jTw9UPO"
      },
      "source": [
        "## Base Transformer (BAT class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6jR8NwS9UPO"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "\n",
        "# root_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..')\n",
        "# sys.path.insert(0, os.path.join(root_path))\n",
        "# sys.path.insert(0, os.path.join(root_path, 'lib'))\n",
        "# sys.path.insert(0, os.path.join(root_path, 'lib/Cell_DETR_master'))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from Ours.base import DeepLabV3 as base\n",
        "\n",
        "# from src.BAT_Modules import BoundaryCrossAttention, CrossAttention\n",
        "# from src.BAT_Modules import MultiHeadAttention as Attention_head\n",
        "# from src.transformer import BoundaryAwareTransformer, Transformer\n",
        "\n",
        "\n",
        "class BAT(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_classes,\n",
        "            num_layers,\n",
        "            point_pred,\n",
        "            decoder=False,\n",
        "            transformer_type_index=0,\n",
        "            hidden_features=128,  # 256\n",
        "            number_of_query_positions=1,\n",
        "            segmentation_attention_heads=8):\n",
        "\n",
        "        super(BAT, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.point_pred = point_pred\n",
        "        self.transformer_type = \"BoundaryAwareTransformer\" if transformer_type_index == 0 else \"Transformer\"\n",
        "        self.use_decoder = decoder\n",
        "\n",
        "        self.deeplab = base(num_classes, num_layers)\n",
        "\n",
        "        in_channels = 2048 if num_layers == 50 else 512\n",
        "\n",
        "        self.convolution_mapping = nn.Conv2d(in_channels=in_channels,\n",
        "                                             out_channels=hidden_features,\n",
        "                                             kernel_size=(1, 1),\n",
        "                                             stride=(1, 1),\n",
        "                                             padding=(0, 0),\n",
        "                                             bias=True)\n",
        "\n",
        "        self.query_positions = nn.Parameter(data=torch.randn(\n",
        "            number_of_query_positions, hidden_features, dtype=torch.float),\n",
        "                                            requires_grad=True)\n",
        "\n",
        "        self.row_embedding = nn.Parameter(data=torch.randn(100,\n",
        "                                                           hidden_features //\n",
        "                                                           2,\n",
        "                                                           dtype=torch.float),\n",
        "                                          requires_grad=True)\n",
        "        self.column_embedding = nn.Parameter(data=torch.randn(\n",
        "            100, hidden_features // 2, dtype=torch.float),\n",
        "                                             requires_grad=True)\n",
        "\n",
        "        self.transformer = [\n",
        "            Transformer(d_model=hidden_features),\n",
        "            BoundaryAwareTransformer(d_model=hidden_features)\n",
        "        ][point_pred]\n",
        "\n",
        "        if self.use_decoder:\n",
        "            self.BCA = BoundaryCrossAttention(hidden_features, 8)\n",
        "\n",
        "        self.trans_out_conv = nn.Conv2d(in_channels=hidden_features,\n",
        "                                        out_channels=in_channels,\n",
        "                                        kernel_size=(1, 1),\n",
        "                                        stride=(1, 1),\n",
        "                                        padding=(0, 0),\n",
        "                                        bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x.size()[2]\n",
        "        w = x.size()[3]\n",
        "        feature_map = self.deeplab.resnet(x)\n",
        "\n",
        "        features = self.convolution_mapping(feature_map)\n",
        "        height, width = features.shape[2:]\n",
        "        batch_size = features.shape[0]\n",
        "        positional_embeddings = torch.cat([\n",
        "            self.column_embedding[:height].unsqueeze(dim=0).repeat(\n",
        "                height, 1, 1),\n",
        "            self.row_embedding[:width].unsqueeze(dim=1).repeat(1, width, 1)\n",
        "        ],\n",
        "                                          dim=-1).permute(\n",
        "                                              2, 0, 1).unsqueeze(0).repeat(\n",
        "                                                  batch_size, 1, 1, 1)\n",
        "\n",
        "        if self.transformer_type == 'BoundaryAwareTransformer':\n",
        "            latent_tensor, features_encoded, point_maps = self.transformer(\n",
        "                features, None, self.query_positions, positional_embeddings)\n",
        "        else:\n",
        "            latent_tensor, features_encoded = self.transformer(\n",
        "                features, None, self.query_positions, positional_embeddings)\n",
        "            point_maps = []\n",
        "\n",
        "        latent_tensor = latent_tensor.permute(2, 0, 1)\n",
        "        # shape:(bs, 1 , 128)\n",
        "\n",
        "        if self.use_decoder:\n",
        "            features_encoded, point_dec = self.BCA(features_encoded,\n",
        "                                                   latent_tensor)\n",
        "            point_maps.append(point_dec)\n",
        "\n",
        "        trans_feature_maps = self.trans_out_conv(\n",
        "            features_encoded.contiguous())  #.contiguous()\n",
        "\n",
        "        trans_feature_maps = trans_feature_maps + feature_map\n",
        "\n",
        "        output = self.deeplab.aspp(\n",
        "            trans_feature_maps\n",
        "        )  # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "        output = F.interpolate(\n",
        "            output, size=(h, w),\n",
        "            mode=\"bilinear\")  # (shape: (batch_size, num_classes, h, w))\n",
        "\n",
        "        if self.point_pred == 1:\n",
        "            return output, point_maps\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIIdw0tLK2S4"
      },
      "source": [
        "## 2. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svv3B6UJ9UPP",
        "outputId": "53d9123e-b080-494d-ed1c-7df24ddccac9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 92.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = BAT(1, parse_config.net_layer, parse_config.point_pred,\n",
        "                    parse_config.ppl).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ag6qGuR9UPP"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20fGhStqikfl"
      },
      "outputs": [],
      "source": [
        "# Based on: https://github.com/facebookresearch/detr/blob/master/models/transformer.py\n",
        "import copy\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "# from .BAT_Modules import BoundaryWiseAttentionGate2D, BoundaryWiseAttentionGate1D, BoundaryWiseAttentionGateAtrous2D, BoundaryWiseAttentionGateAtrous1D\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers,\n",
        "                                          encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        #         print(\"Trans Encoder\",memory.shape)\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers=6,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = BoundaryAwareTransformerEncoderLayer(\n",
        "            d_model, nhead, BAG_type, Atrous, dim_feedforward, dropout,\n",
        "            activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = BoundaryAwareTransformerEncoder(point_pred_layers,\n",
        "                                                       encoder_layer,\n",
        "                                                       num_encoder_layers,\n",
        "                                                       encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory, weights = self.encoder(src,\n",
        "                                       src_key_padding_mask=mask,\n",
        "                                       pos=pos_embed,\n",
        "                                       height=h,\n",
        "                                       width=w)\n",
        "\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h,\n",
        "                                                                w), weights\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask,\n",
        "                           pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers,\n",
        "                 encoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None):\n",
        "        super().__init__()\n",
        "        self.point_pred_layers = point_pred_layers\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        output = src\n",
        "        weights = []\n",
        "\n",
        "        for layer_i, layer in enumerate(self.layers):\n",
        "            if layer_i > self.num_layers - self.point_pred_layers - 1:\n",
        "                output, weight = layer(\n",
        "                    True,\n",
        "                    output,\n",
        "                    src_mask=mask,\n",
        "                    src_key_padding_mask=src_key_padding_mask,\n",
        "                    pos=pos,\n",
        "                    height=height,\n",
        "                    width=width)\n",
        "                weights.append(weight)\n",
        "            else:\n",
        "                output = layer(False,\n",
        "                               output,\n",
        "                               src_mask=mask,\n",
        "                               src_key_padding_mask=src_key_padding_mask,\n",
        "                               pos=pos,\n",
        "                               height=height,\n",
        "                               width=width)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src2,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoderLayer(TransformerEncoderLayer):\n",
        "    \"    Add Boundary-wise Attention Gate to Transformer's Encoder\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=True,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation,\n",
        "                         normalize_before)\n",
        "        if BAG_type == '1D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous1D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate1D(d_model)\n",
        "        elif BAG_type == '2D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous2D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate2D(d_model)\n",
        "        self.BAG_type = BAG_type\n",
        "\n",
        "    def forward(self,\n",
        "                use_bag,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        if self.normalize_before:\n",
        "            features = self.forward_pre(src, src_mask, src_key_padding_mask,\n",
        "                                        pos)\n",
        "            if use_bag:\n",
        "                b, c = features.shape[1:]\n",
        "                if self.BAG_type == '1D':\n",
        "                    features = features.permute(1, 2, 0)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.permute(2, 0, 1).contiguous()\n",
        "                    weights = weights.view(b, 1, height, width)\n",
        "                elif self.BAG_type == '2D':\n",
        "                    features = features.permute(1, 2,\n",
        "                                                0).view(b, c, height, width)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.flatten(2).permute(2, 0,\n",
        "                                                           1).contiguous()\n",
        "                return features, weights\n",
        "            else:\n",
        "                return features\n",
        "        features = self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "        if use_bag:\n",
        "            b, c = features.shape[1:]\n",
        "            if self.BAG_type == '1D':\n",
        "                features = features.permute(1, 2, 0)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.permute(2, 0, 1).contiguous()\n",
        "                weights = weights.view(b, 1, height, width)\n",
        "            elif self.BAG_type == '2D':\n",
        "                features = features.permute(1, 2, 0).view(b, c, height, width)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.flatten(2).permute(2, 0, 1).contiguous()\n",
        "            return features, weights\n",
        "        else:\n",
        "            return features\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 decoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None,\n",
        "                 return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           memory,\n",
        "                           tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos,\n",
        "                           query_pos=query_pos)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model,\n",
        "                                                    nhead,\n",
        "                                                    dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     tgt,\n",
        "                     memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    tgt,\n",
        "                    memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt2,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask,\n",
        "                                    memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask,\n",
        "                                 pos, query_pos)\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args.hidden_dim,\n",
        "        dropout=args.dropout,\n",
        "        nhead=args.nheads,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        num_encoder_layers=args.enc_layers,\n",
        "        num_decoder_layers=args.dec_layers,\n",
        "        normalize_before=args.pre_norm,\n",
        "        return_intermediate_dec=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"leaky relu\":\n",
        "        return F.leaky_relu\n",
        "    if activation == \"selu\":\n",
        "        return F.selu\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(\n",
        "        F\"activation should be relu, gelu, glu, leaky relu or selu, not {activation}.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzze2VhIK2S5"
      },
      "source": [
        "### Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbTO94ePK2S5"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.0001):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.mode = mode\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        self.BEST_MODEL_PATH = \"\"\n",
        "        if self.mode == \"min\":\n",
        "            self.val_score = np.inf\n",
        "        else:\n",
        "            self.val_score = -np.inf\n",
        "\n",
        "    def __call__(self, epoch, epoch_score, model, optimizer, loss, model_path):\n",
        "        if self.mode == \"min\":\n",
        "            score = -1.0 * epoch_score\n",
        "        else:\n",
        "            score = np.copy(epoch_score)\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch, epoch_score, model, optimizer, loss, model_path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(\n",
        "                \"EarlyStopping counter: {} out of {}\".format(\n",
        "                    self.counter, self.patience\n",
        "                )\n",
        "            )\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch, epoch_score, model, optimizer, loss, model_path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch, epoch_score, model, optimizer, loss, model_path):\n",
        "        model_path = Path(model_path)\n",
        "        parent = model_path.parent\n",
        "        os.makedirs(parent, exist_ok=True)\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
        "            print(\n",
        "                \"Validation score improved ({} --> {}). Model saved at {}!\".format(\n",
        "                    self.val_score, epoch_score, model_path\n",
        "                )\n",
        "            )\n",
        "            torch.save({\"Epoch\": epoch, \"Train Loss\": loss, \"Validation Dice Score\": self.val_score, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, model_path)\n",
        "            self.BEST_MODEL_PATH = model_path\n",
        "            print(f\"Checkpoint saved on epoch - {epoch} with dice score - {epoch_score}\")\n",
        "        self.val_score = epoch_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwQu_LDS9UPS"
      },
      "outputs": [],
      "source": [
        "criterion = CRITERION\n",
        "es = EarlyStopping(patience=EARLY_STOPPING_PATIENCE, mode='max')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOlM9jTEK2S5"
      },
      "source": [
        "### Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_amog1GHK2S5"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbn3pKTaK2S5"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(train_loader, model, optimizer, loss_fn, accumulation_steps=int(EFFECTIVE_BATCH_SIZE/BATCH_SIZE), device='cuda'):\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # Lists to store batch-to-batch progress details within the epoch while training\n",
        "    batch_count_train = []\n",
        "    batch_train_loss = []\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    if accumulation_steps > 1:\n",
        "      optimizer.zero_grad()\n",
        "    tk0 = tqdm(train_loader, total=len(train_loader))\n",
        "    for b_idx, data in enumerate(tk0):\n",
        "      print(data['image'].shape) # print(data['image'].shape) -> torch.Size([8, 3, 512, 512])\n",
        "      print(data['mask'].shape) # print(data['mask'].shape) -> torch.Size([8, 1, 512, 512])\n",
        "      if (b_idx + 1) % accumulation_steps == 0:\n",
        "        batch_count_train.append(b_idx)\n",
        "\n",
        "      # moves image tensor and mask tensor to gpu\n",
        "      for key, value in data.items():\n",
        "        data[key] = value.to(\"cuda\")\n",
        "      point = (data['point'] > 0).cuda().float()\n",
        "\n",
        "      if parse_config.net_layer == 18:\n",
        "          point_c4 = nn.functional.max_pool2d(point,\n",
        "                                              kernel_size=(16, 16),\n",
        "                                              stride=(16, 16))\n",
        "          point = nn.functional.max_pool2d(point,\n",
        "                                          kernel_size=(8, 8),\n",
        "                                          stride=(8, 8))\n",
        "      else:\n",
        "          point_c5 = nn.functional.max_pool2d(point,\n",
        "                                              kernel_size=(32, 32),\n",
        "                                              stride=(32, 32))\n",
        "\n",
        "          point_c4 = nn.functional.max_pool2d(point,\n",
        "                                              kernel_size=(16, 16),\n",
        "                                              stride=(16, 16))\n",
        "\n",
        "      if accumulation_steps == 1 and b_idx == 0:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "      if parse_config.point_pred == 1:\n",
        "            output, point_maps_pre = model(data['image'])\n",
        "            output = torch.sigmoid(output)\n",
        "\n",
        "            print(\"point_pre shape:{}, point shape:{}\".format(point_pre.shape,point.shape))\n",
        "            assert (output.shape == data['mask'].float().shape)\n",
        "            loss_dc = dice_loss(output, data['mask'].float())\n",
        "            print(point_maps_pre[-1].shape, point_c4.shape)\n",
        "            assert (point_maps_pre[-1].shape == point_c4.shape)\n",
        "\n",
        "            point_loss = 0.\n",
        "            for i in range(len(point_maps_pre)):\n",
        "                point_loss += criterion(point_maps_pre[i], point_c4)\n",
        "            point_loss = point_loss / len(point_maps_pre)\n",
        "\n",
        "            loss = loss_dc + point_loss  # point_loss weight: 3\n",
        "\n",
        "            with torch.set_grad_enabled(True):\n",
        "              loss.backward()\n",
        "              # if (b_idx + 1) % accumulation_steps == 0:\n",
        "              #   if GRADIENT_CLIPPING:\n",
        "              #     clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING_THRESHOLD)\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "            iteration = iteration + 1\n",
        "\n",
        "            if (b_idx + 1) % 10 == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    b_idx * len(data), len(train_dataloader.dataset),\n",
        "                    100. * b_idx / len(train_dataloader), loss.item()))\n",
        "      print(\"Iteration numbers: \", iteration)\n",
        "\n",
        "      ###############################################################################################\n",
        "      # out  = model(data['image']) # out.shape = torch.Size([8, 1, 512, 512])\n",
        "      # loss = loss_fn(out, data['mask'].float()) # mask.shape = torch.Size([8, 1, 512, 512])\n",
        "      # with torch.set_grad_enabled(True):\n",
        "      #   loss.backward()\n",
        "      #   if (b_idx + 1) % accumulation_steps == 0:\n",
        "      #     if GRADIENT_CLIPPING:\n",
        "      #       clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING_THRESHOLD)\n",
        "      #     optimizer.step()\n",
        "      #     optimizer.zero_grad()\n",
        "      losses.update(loss.item(), train_loader.batch_size)\n",
        "      if (b_idx + 1) % accumulation_steps == 0:\n",
        "        batch_train_loss.append(loss.item())\n",
        "      tk0.set_postfix(loss=losses.avg, learning_rate=optimizer.param_groups[0]['lr'])\n",
        "    return losses.avg, batch_count_train, batch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKL6S336-unk",
        "outputId": "1f28070e-953b-410b-fd08-cd67078095b0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>focal_loss</b><br/>def focal_loss(inputs: torch.Tensor, targets: torch.Tensor, alpha: float=0.6, gamma: float=2, reduction: str=&#x27;mean&#x27;) -&gt; torch.Tensor</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-16-defc6de845ef&gt;</a>&lt;no docstring&gt;</pre></div>"
            ],
            "text/plain": [
              "<function __main__.focal_loss(inputs: torch.Tensor, targets: torch.Tensor, alpha: float = 0.6, gamma: float = 2, reduction: str = 'mean') -> torch.Tensor>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K136b3eqK2S5"
      },
      "source": [
        "### Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RY2NxC-unk"
      },
      "source": [
        "#### Mask Binarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4my02Uf-unk"
      },
      "outputs": [],
      "source": [
        "class MaskBinarization():\n",
        "    def __init__(self):\n",
        "        self.thresholds = 0.5\n",
        "    def transform(self, predicted):\n",
        "        yield predicted > self.thresholds\n",
        "\n",
        "class SimpleMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, score_thresholds):\n",
        "        super().__init__()\n",
        "        self.thresholds = score_thresholds\n",
        "    def transform(self, predicted):\n",
        "        for thr in self.thresholds:\n",
        "            yield predicted > thr\n",
        "\n",
        "class DupletMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, duplets, with_channels=True):\n",
        "        super().__init__()\n",
        "        self.thresholds = duplets\n",
        "        self.dims = (2,3) if with_channels else (1,2)\n",
        "    def transform(self, predicted):\n",
        "        for score_threshold, area_threshold in self.thresholds:\n",
        "            mask = predicted > score_threshold\n",
        "            mask[mask.sum(dim=self.dims) < area_threshold] = 0\n",
        "            yield mask\n",
        "\n",
        "class TripletMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, triplets, with_channels=True):\n",
        "        super().__init__()\n",
        "        self.thresholds = triplets\n",
        "        self.dims = (2,3) if with_channels else (1,2) # dims should be HxW, basically it should ignore batch_size and no_of_channels in the general format of BxCxHxW\n",
        "    def transform(self, predicted):\n",
        "        for top_score_threshold, area_threshold, bottom_score_threshold in self.thresholds:\n",
        "            clf_mask = (predicted > top_score_threshold).float()\n",
        "            pred_mask = (predicted > bottom_score_threshold).float()\n",
        "            pred_mask[clf_mask.sum(dim=self.dims) < area_threshold] = 0\n",
        "            yield pred_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joGnfZgw-unl"
      },
      "source": [
        "#### Metric used in validation and evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlWOcwQdK2S5"
      },
      "outputs": [],
      "source": [
        "def metric(probability, truth):\n",
        "    if probability.shape[0] == truth.shape[0]: # checking for batch size mismatches in the code for image & mask\n",
        "        batch_size = probability.shape[0]\n",
        "    with torch.no_grad():\n",
        "        probability = probability.view(batch_size, -1) # probability's size is [8, 1*512*512]\n",
        "        truth = truth.view(batch_size, -1)             # truth's size is [8, 1*512*512]\n",
        "        assert(probability.shape == truth.shape)\n",
        "\n",
        "        p = probability.float() # prob_preds already comes in binarized.\n",
        "        t = (truth > 0.5).float()\n",
        "\n",
        "        t_sum = t.sum(-1) # t_sum size is 8 # Each value in the vector represents the sum of all pixels in one mask\n",
        "        p_sum = p.sum(-1) # p_sum size is 8 # Each value in the vector represents the sum of all elements in one pred_probs\n",
        "        neg_index = torch.nonzero(t_sum == 0) # indices of masks which are negative.\n",
        "        pos_index = torch.nonzero(t_sum >= 1) # indices of masks which are positive.\n",
        "\n",
        "        dice_neg = (p_sum == 0).float() # tensor of size 8\n",
        "        \"\"\"\n",
        "        if t_sum = torch.tensor([0.0, 1000.0, 0.0, 600.0, 720.0, 420.0, 0.0, 0.0]), then dice_neg = tensor([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0])\n",
        "        \"\"\"\n",
        "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1)) # tensor of size 8\n",
        "\n",
        "        dice_neg = dice_neg[neg_index] # selects elements of dice_neg acc to the indices in neg_index, it can have more than one element.\n",
        "        dice_pos = dice_pos[pos_index] # similar to the above code line.\n",
        "        dice = torch.cat([dice_pos, dice_neg])\n",
        "\n",
        "        num_neg = len(neg_index) # no. of negative masks in a batch\n",
        "        num_pos = len(pos_index) # no. of positive masks in a batch\n",
        "\n",
        "    return dice.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3ovO3f9XHQt"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_best_thresholds(epoch, b_idx, best_metric, best_threshold, filepath=Path(save_best_thresholds_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch,\n",
        "        'batch_number': b_idx,\n",
        "        'best_metric': best_metric,\n",
        "        'best_threshold': best_threshold\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7opE5TuMRCvg"
      },
      "outputs": [],
      "source": [
        "epoch_count_thr = []\n",
        "batch_indices = []\n",
        "best_metrics_list = []\n",
        "best_thresholds_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww6ftfKmK2S5"
      },
      "outputs": [],
      "source": [
        "def evaluate(valid_loader, model, epoch, device=DEVICE, metric=metric, loss_fn=criterion):\n",
        "    losses = AverageMeter()\n",
        "    combolosses = AverageMeter()\n",
        "    metrics = defaultdict(float)\n",
        "    # Lists to store batch-to-batch progress details within the epoch while training\n",
        "    batch_count_val = []\n",
        "    batch_val_loss_values = []\n",
        "################################# top\n",
        "    dice_value = 0\n",
        "    iou_value = 0\n",
        "    dice_average = 0\n",
        "    iou_average = 0\n",
        "    numm = 0\n",
        "################################# bottom\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    tk0 = tqdm(valid_loader, total=len(valid_loader))\n",
        "    with torch.inference_mode():\n",
        "        for b_idx, data in enumerate(tk0):\n",
        "            batch_count_val.append(b_idx)\n",
        "################################################################################ top\n",
        "            data = data['image'].cuda().float()\n",
        "            label = data['label'].cuda().float()\n",
        "            point = (data['point'] > 0).cuda().float()\n",
        "            point_c5 = nn.functional.max_pool2d(point,\n",
        "                                                kernel_size=(32, 32),\n",
        "                                                stride=(32, 32))\n",
        "            point_c4 = nn.functional.max_pool2d(point,\n",
        "                                                kernel_size=(16, 16),\n",
        "                                                stride=(16, 16))\n",
        "################################################################################### bottom\n",
        "\n",
        "#################################################################################### top\n",
        "            if parse_config.arch == 'transfuse':\n",
        "                _, _, output = model(data)\n",
        "                loss_fuse = structure_loss(output, label)\n",
        "            elif parse_config.point_pred == 0:\n",
        "                output = model(data)\n",
        "            elif parse_config.cross == 1 and parse_config.point_pred == 1:\n",
        "                output, point_maps_pre_1, point_maps_pre_2 = model(data)\n",
        "                point_loss_c4 = 0.\n",
        "                for i in range(len(point_maps_pre_1) - 1):\n",
        "                    point_loss_c4 += criterion(point_maps_pre_1[i], point_c4)\n",
        "                point_loss_c4 = 1.0 / len(point_maps_pre_1) * (\n",
        "                    point_loss_c4 + criterion(point_maps_pre_1[-1], point_c4))\n",
        "                point_loss_c5 = 0.\n",
        "                for i in range(len(point_maps_pre_2) - 1):\n",
        "                    point_loss_c5 += criterion(point_maps_pre_2[i], point_c5)\n",
        "                point_loss_c5 = 1.0 / len(point_maps_pre_2) * (\n",
        "                    point_loss_c5 + criterion(point_maps_pre_2[-1], point_c5))\n",
        "                point_loss = 0.5 * (point_loss_c4 + point_loss_c5)\n",
        "            elif parse_config.point_pred == 1:\n",
        "                output, point_maps_pre = model(data)\n",
        "                point_loss = 0.\n",
        "                for i in range(len(point_maps_pre) - 1):\n",
        "                    point_loss += criterion(point_maps_pre[i], point_c4)\n",
        "                point_loss = 1.0 / len(point_maps_pre) * (\n",
        "                    point_loss + criterion(point_maps_pre[-1], point_c4))\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "\n",
        "            loss_dc = dice_loss(output, label)\n",
        "\n",
        "            if parse_config.arch == 'transfuse':\n",
        "                loss = loss_fuse\n",
        "            elif parse_config.arch == 'transunet':\n",
        "                loss = 0.5 * loss_dc + 0.5 * ce_loss(output, label)\n",
        "            elif parse_config.point_pred == 0:\n",
        "                loss = loss_dc\n",
        "            elif parse_config.cross == 1 and parse_config.point_pred == 1:\n",
        "                loss = loss_dc + point_loss\n",
        "            elif parse_config.point_pred == 1:\n",
        "                loss = loss_dc + 3 * point_loss\n",
        "\n",
        "            output = output.cpu().numpy() > 0.5\n",
        "\n",
        "            label = label.cpu().numpy()\n",
        "            assert (output.shape == label.shape)\n",
        "            dice_ave = metric(output, label)\n",
        "            # iou_ave = jc(output, label)\n",
        "            dice_value += dice_ave\n",
        "            # iou_value += iou_ave\n",
        "            numm += 1\n",
        "\n",
        "    dice_average = dice_value / numm\n",
        "    # iou_average = iou_value / numm\n",
        "    print(\"Average dice value of evaluation dataset = \", dice_average)\n",
        "    # print(\"Average iou value of evaluation dataset = \", iou_average)\n",
        "#################################################################################### bottom\n",
        "\n",
        "\n",
        "    tk0.set_description('score: {:.5f}'.format(dice_ave))\n",
        "\n",
        "    epoch_count_thr.append(epoch)\n",
        "    batch_indices.append(b_idx)\n",
        "    best_metrics_list.append(dice_ave)\n",
        "\n",
        "    # if .item() is used then .cpu() is NOT required. .item() will itself return a float value.\n",
        "    losses.update(dice_ave, valid_loader.batch_size)\n",
        "    combolosses.update(loss.item(), valid_loader.batch_size)\n",
        "    batch_val_loss_values.append(loss.item())\n",
        "    tk0.set_postfix(dice_score=losses.avg, val_loss=combolosses.avg)\n",
        "    return losses.avg, batch_count_val, batch_val_loss_values, combolosses.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE7TO5J9XHQt"
      },
      "source": [
        "### Optimizer & Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGkO2pA_9UPW"
      },
      "outputs": [],
      "source": [
        "if PRETRAINED:\n",
        "  checkpoint = torch.load(TRAINING_MODEL_PATH)\n",
        "  model.to(DEVICE)\n",
        "  model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay= L2_WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
        "elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn0XAwlyK2S6"
      },
      "outputs": [],
      "source": [
        "# if PRETRAINED:\n",
        "#   checkpoint = torch.load(TRAINING_MODEL_PATH)\n",
        "#   model.to(DEVICE)\n",
        "#   model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "#   optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay= L2_WEIGHT_DECAY)\n",
        "#   if OPTIMIZER_LOAD:\n",
        "#     optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "#   # Manually set the new learning rate for this stage of training as loading optimizer's state dict will\n",
        "#   # load parameters that was there while saving the previous checkpoint but loading the optimizer's state dict is\n",
        "#   # crucial\n",
        "#   for param_group in optimizer.param_groups:\n",
        "#     param_group['lr'] = LEARNING_RATE\n",
        "\n",
        "#   if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=SCHEDULER_PARAMS[\"factor\"], patience=SCHEDULER_PARAMS[\"patience\"], threshold=SCHEDULER_PARAMS[\"threshold\"], min_lr=SCHEDULER_PARAMS[\"min_lr\"])\n",
        "#   elif SCHEDULER == \"CosineAnnealingWarmRestarts\":\n",
        "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=SCHEDULER_PARAMS[\"T_0\"], T_mult=SCHEDULER_PARAMS[\"T_mult\"], eta_min=SCHEDULER_PARAMS[\"eta_min\"])\n",
        "#   elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=SCHEDULER_PARAMS[\"T_max\"], eta_min=SCHEDULER_PARAMS[\"eta_min\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK69cmOlRCvg"
      },
      "outputs": [],
      "source": [
        "# if not PRETRAINED:\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay= L2_WEIGHT_DECAY)\n",
        "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=SCHEDULER_PARAMS[\"factor\"], patience=SCHEDULER_PARAMS[\"patience\"], threshold=SCHEDULER_PARAMS[\"threshold\"], min_lr=SCHEDULER_PARAMS[\"min_lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqlFcFDSRCvh",
        "outputId": "d53a1211-145f-446e-e206-1bee8643b7db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New learning rate: 0.0001\n"
          ]
        }
      ],
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "    print(f\"New learning rate: {param_group['lr']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig7E3afMOi-6"
      },
      "outputs": [],
      "source": [
        "# initial_lr_scheduler = scheduler.base_lrs[0]  # Assuming a single learning rate for all parameters\n",
        "# print(\"Initial learning rate of scheduler:\", initial_lr_scheduler)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOkP2HYpvsqr"
      },
      "source": [
        "### GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypne222KBjPe"
      },
      "outputs": [],
      "source": [
        "# # memory footprint support libraries/code\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "# def printm():\n",
        "#  process = psutil.Process(os.getpid())\n",
        "#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "# printm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtHHjC45vsqr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcACWXgAXHQu"
      },
      "source": [
        "#### Saving Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzIGIAe2FmJQ"
      },
      "outputs": [],
      "source": [
        "def store_batch_training_details(epoch, batch_count_train, batch_train_loss):\n",
        "  # Directory where you want to save the CSV file\n",
        "  directory = store_batch_training_details_path\n",
        "\n",
        "  # Ensure the directory exists\n",
        "  os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "  # File path\n",
        "  file_path = os.path.join(directory, name_of_batch_training_details_csv + f\"{epoch}.csv\")\n",
        "\n",
        "  # Write to CSV file\n",
        "  with open(file_path, \"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      # Write header\n",
        "      writer.writerow([\"Batch Count Train\", \"Batch Train Loss\"])\n",
        "      # Write data rows\n",
        "      for batch_count, train_loss in zip(batch_count_train, batch_train_loss):\n",
        "          writer.writerow([batch_count, train_loss])\n",
        "\n",
        "  print(f\"CSV file for epoch-{epoch}has been created at: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5-0shprvsqr"
      },
      "outputs": [],
      "source": [
        "def store_batch_validation_details(epoch, batch_count_val, batch_val_score_values):\n",
        "  # Directory where you want to save the CSV file\n",
        "  directory = store_batch_validation_details_path\n",
        "\n",
        "  # Ensure the directory exists\n",
        "  os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "  # File path\n",
        "  file_path = os.path.join(directory, name_of_batch_validation_details_csv + f\"{epoch}.csv\")\n",
        "\n",
        "  # Write to CSV file\n",
        "  with open(file_path, \"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      # Write header\n",
        "      writer.writerow([\"Batch Count Validation\", \"Batch Validation Loss\"])\n",
        "      # Write data rows\n",
        "      for batch_count, batch_val_score in zip(batch_count_val, batch_val_score_values):\n",
        "          writer.writerow([batch_count, batch_val_score])\n",
        "\n",
        "  print(f\"CSV file for validation, epoch-{epoch}has been created at: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijkQ3VlU-unm"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_progress(epoch_count, loss_values, val_loss_values, filepath= Path(save_progress_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch_count,\n",
        "        'train_loss': loss_values,\n",
        "        'val_loss': [None] * (len(epoch_count) - len(val_loss_values)) + val_loss_values\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTogqmQgWkZC"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_dice_score(epoch_count, val_score_values, filepath= Path(save_dice_score_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch_count,\n",
        "        'val_dice_scores': [None] * (len(epoch_count) - len(val_score_values)) + val_score_values\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntFd7ioHXHQv"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_losses(bce_losses=bce_losses, dice_losses=dice_losses, focal_losses=focal_losses, filepath= Path(save_3losses_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'bce': bce_losses,\n",
        "        'dice': dice_losses,\n",
        "        'focal': focal_losses\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMFmkSOtXHQv"
      },
      "source": [
        "#### Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "0bPiRNBuK2S6",
        "outputId": "00dbcedd-4116-4cdb-a6f5-eedb38ca3fa0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1190 [01:36<?, ?it/s]\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-3bcfe9d0f7f1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtill_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mepoch_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_count_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mloss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstore_batch_training_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_count_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-53e957744552>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_loader, model, optimizer, loss_fn, accumulation_steps, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloss_dc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# print(point_maps_pre[-1].shape, point_c4.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpoint_maps_pre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpoint_c4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpoint_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "till_epoch = EPOCHS\n",
        "model.to(DEVICE)\n",
        "model.train()\n",
        "# Lists to store epoch-to-epoch progress details\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "val_score_values = []\n",
        "val_loss_values = []\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, till_epoch+1):\n",
        "    epoch_count.append(epoch)\n",
        "    loss, batch_count_train, batch_train_loss = train_one_epoch(train_dataloader, model, optimizer, criterion)\n",
        "    loss_values.append(loss)\n",
        "    store_batch_training_details(epoch, batch_count_train, batch_train_loss)\n",
        "    if not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step()\n",
        "    # Storing Training details for plotting curves and inference\n",
        "    print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}\")\n",
        "\n",
        "    \"\"\"-------------------VALIDATION---------------------\"\"\"\n",
        "    dice, batch_count_val, batch_val_score_values, val_loss = evaluate(val_dataloader, model, epoch=epoch) # Evaluates model performance by calculating the dice coefficient\n",
        "    val_score_values.append(dice)\n",
        "    val_loss_values.append(val_loss)\n",
        "    # Storing Validation details for plotting curves and inference\n",
        "    store_batch_validation_details(epoch, batch_count_val, batch_val_score_values)\n",
        "    print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}, VAL DICE: {dice}\")\n",
        "\n",
        "    save_progress(epoch_count, loss_values, val_loss_values)\n",
        "    save_dice_score(epoch_count, val_score_values)\n",
        "    save_losses()\n",
        "# \"\"\"\" If it is necessary to save model's state dict as checkpoint, do the essential changes\n",
        "#      in Early Stopping class.\"\"\"\n",
        "\n",
        "    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step(dice)\n",
        "\n",
        "    es(epoch, dice, model, optimizer, loss, model_path= model_checkpoint_path + f\"_epoch{epoch}_bst_model{IMG_SIZE}_fold{FOLD_ID}_{np.round(dice,4)}.tar\")\n",
        "    if es.early_stop:\n",
        "        print('\\n\\n -------------- EARLY STOPPING -------------- \\n\\n')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "key_patch_map = np.load(\"C:/Users/nisha/Desktop/Research Project May-June/gt_keypatch/1.2.276.0.7230010.3.1.4.8323329.300.1517875162.258081.npy\")\n",
        "sns.heatmap(key_patch_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x2550eab1f40>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(key_patch_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(512, 512)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "key_patch_map.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "key_patch_map = torch.unsqueeze(torch.Tensor(key_patch_map), axis=0)\n",
        "print(key_patch_map.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJFz9wAhzL4k"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgLZEbqwyAb4"
      },
      "source": [
        "20 mins for a single epoch of training and validation together with T4 GPU. If the no. of epoches is 50 and early stopping is 10, let's see how many hours it takes to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ptGWY9E2cn"
      },
      "source": [
        "Visualizations for the predictions!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KC5W67ovsqs"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")221d\n",
        "52\n",
        "# fig, ax = plt.subplots()awjk\n",
        "506\n",
        "132\n",
        "54\n",
        "# ax.plot(epoch_count, loss_values)\n",
        "# ax.set_title(\"Training Loss Curve [EPOCHS]\", fontsize=20)\n",
        "# ax.set_xlabel(\"epoch number\", fontsize=14)\n",
        "# ax.set_ylabel(\"loss value\", fontsize=14)\n",
        "# ax.tick_params(axis='both', labelsize=14)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8TB_fU0f1jh"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")\n",
        "# fig, ax = plt.subplots()\n",
        "\n",
        "# ax.plot(epoch_count, val_loss_values)\n",
        "# ax.set_title(\"Validation Loss Curve [EPOCHS]\", fontsize=20)\n",
        "# ax.set_xlabel(\"epoch number\", fontsize=14)\n",
        "# ax.set_ylabel(\"loss value\", fontsize=14)\n",
        "# ax.tick_params(axis='both', labelsize=14)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ2c6YzKSuFU"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(epoch_count, loss_values)\n",
        "# plt.plot(epoch_count, val_loss_values)\n",
        "# plt.title(\"Training and Validation Loss Curves\", fontsize=20)\n",
        "# plt.xlabel(\"epoch number\", fontsize=14)\n",
        "# plt.ylabel(\"loss value\", fontsize=14)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2BXTyFrPxzy"
      },
      "outputs": [],
      "source": [
        "# Load data from CSV into a DataFrame\n",
        "file_path = save_progress_path  # Replace with your CSV file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Plotting the line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['epoch'], df['train_loss'], marker='o', linestyle='-', color='b', label='Train Loss')\n",
        "plt.plot(df['epoch'], df['val_loss'], marker='o', linestyle='-', color='r', label='Validation Loss')\n",
        "\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOQs9Sy2VBHR"
      },
      "source": [
        "# Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_QmgkWSGPsu"
      },
      "outputs": [],
      "source": [
        "# checkpoint = torch.load(es.BEST_MODEL_PATH)\n",
        "# print(es.best_score)\n",
        "# model.to(DEVICE)\n",
        "# model.load_state_dict(checkpoint[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnM0qtplSwyW"
      },
      "outputs": [],
      "source": [
        "# model.to(DEVICE)\n",
        "# model.eval()\n",
        "# idx = 1995\n",
        "# mask = val_dataset[idx][\"mask\"].unsqueeze(0)\n",
        "# print(\"Mask Shape: \",mask.shape)\n",
        "# image = val_dataset[idx][\"image\"].unsqueeze(0)\n",
        "# print(\"Image Shape: \",image.shape)\n",
        "# raw_output = model(image.to(DEVICE))\n",
        "# print(\"raw output shape: \", raw_output.shape)\n",
        "# print(\"------ Raw Output ------\")\n",
        "# print(raw_output)\n",
        "# print(\"------- Pred Probs -------\")\n",
        "# pred_probs = torch.sigmoid(raw_output)\n",
        "# print(pred_probs)\n",
        "# print(\"------- How does the Mask look like? ------\")\n",
        "# print(mask)\n",
        "# print(\"-------- Predicted Segmentation mask --------\")\n",
        "# # binarizer_fn = TripletMaskBinarization(triplets=[[0.7, 600, 0.3]])\n",
        "# # mask = binarizer_fn.transform(mask).float()\n",
        "# # print(segmentation_mask)\n",
        "# segmentation_mask = (pred_probs > 0.4).float()\n",
        "# print(segmentation_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cALrPDiZFyhu"
      },
      "outputs": [],
      "source": [
        "# mask.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prdvPG3XZiuL"
      },
      "outputs": [],
      "source": [
        "# segmentation_mask.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAzD4mVaTYJg"
      },
      "outputs": [],
      "source": [
        "# plt.style.use(\"classic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak3eAHrDdY_V"
      },
      "outputs": [],
      "source": [
        "# first_channel_tensor = image[0, 0, :, :]\n",
        "# print(first_channel_tensor.shape)  # torch.Size([512, 512])\n",
        "# second_channel_tensor = image[0, 1, :, :]\n",
        "# print(second_channel_tensor.shape)  # torch.Size([512, 512])\n",
        "# third_channel_tensor = image[0, 2, :, :]\n",
        "# print(third_channel_tensor.shape)  # torch.Size([512, 512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRJzqWT1drZv"
      },
      "outputs": [],
      "source": [
        "# plt.title(\"First Channel Image\")\n",
        "# plt.imshow(first_channel_tensor.detach().cpu().numpy(), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS4Lttr2TLad"
      },
      "outputs": [],
      "source": [
        "# plt.imshow(mask.squeeze().detach().cpu().numpy(), cmap = 'gray')\n",
        "# print(mask.squeeze().shape)\n",
        "# plt.title(\"Mask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X9nSoL1TNw_"
      },
      "outputs": [],
      "source": [
        "# print(segmentation_mask.squeeze().shape)\n",
        "# plt.imshow(segmentation_mask.squeeze().detach().cpu().numpy(), cmap='gray')\n",
        "# plt.title(\"Segmentation Mask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi-kZiELUV79"
      },
      "outputs": [],
      "source": [
        "# print(type(pred_probs), type(mask))\n",
        "# print(pred_probs.shape, mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKAY7_pFh_Vt"
      },
      "outputs": [],
      "source": [
        "# print(segmentation_mask.squeeze().squeeze().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm9Qm1_FiFbS"
      },
      "outputs": [],
      "source": [
        "# print(mask.squeeze().squeeze().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kiIFEw2TPZn"
      },
      "outputs": [],
      "source": [
        "# # \"\"\"\n",
        "# # Both methods aim to capture the overlap between predicted positive regions and actual positive regions in the ground truth.\n",
        "# # The dice_metric approach leverages the full range of predicted probabilities, while the  metric function relies on a binary classification based on a chosen threshold.\n",
        "# # \"\"\"\n",
        "# # Dice = metric(pred_probs.detach().cpu(), mask).item()\n",
        "# # print(f\"Dice coefficient: {Dice}\")\n",
        "# # dice_metric_score = dice_metric(pred_probs.detach().cpu(), mask)\n",
        "# # print(f\"Dice coefficient: {dice_metric_score}\")\n",
        "\n",
        "# \"\"\"\n",
        "# Both methods aim to capture the overlap between predicted positive regions and actual positive regions in the ground truth.\n",
        "# The dice_metric approach leverages the full range of predicted probabilities, while the  metric function relies on a binary classification based on a chosen threshold.\n",
        "# \"\"\"\n",
        "# Dice = metric(segmentation_mask.detach().cpu(), mask).item()\n",
        "# print(f\"Dice coefficient: {Dice}\")\n",
        "# dice_metric_score = dice_metric(segmentation_mask.squeeze().squeeze().detach().cpu(), mask.squeeze().squeeze(), per_image=False)\n",
        "# print(f\"Dice coefficient: {dice_metric_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLi3ZH-8ALZM"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P0OtlxdALZN"
      },
      "outputs": [],
      "source": [
        "# from torchvision.transforms import Resize\n",
        "# from torchvision.utils import make_grid\n",
        "\n",
        "\n",
        "# def preprocess_image(image, target_size):\n",
        "#     transform = Resize(target_size)\n",
        "#     image = transform(image)\n",
        "#     return image\n",
        "\n",
        "# def generate_predictions(model, images):\n",
        "#     with torch.inference_mode():\n",
        "#         predictions = model(images.to(DEVICE))\n",
        "#         predictions = torch.sigmoid(predictions)\n",
        "#         return predictions\n",
        "\n",
        "\n",
        "# def visualize_predictions(images, target_masks, preds):\n",
        "#     images = images.cpu() # shape: [8, 3, 1024, 1024] or [8, 3, 512, 512]\n",
        "#     target_masks = target_masks.cpu() # shape: [8, 1, 1024, 1024] or [8, 1, 512, 512]\n",
        "#     preds = preds.cpu() # shape: [8, 1, 1024, 1024] or [8, 1, 512, 512]\n",
        "\n",
        "#     fig, axes = plt.subplots(2, 3, figsize=(20, 30))\n",
        "\n",
        "#     for i in range(2):\n",
        "#         # axes[i, 0].imshow(images[i].permute(1, 2, 0))\n",
        "#         axes[i, 0].imshow(images[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 0].set_title(\"Input Image\")\n",
        "#         # axes[i, 0].axis(\"off\")\n",
        "\n",
        "#         axes[i, 1].imshow(target_masks[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 1].set_title(\"Target Mask\")\n",
        "#         # axes[i, 1].axis(\"off\")\n",
        "\n",
        "#         axes[i, 2].imshow(preds[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 2].set_title(\"Model 512 prediction\")\n",
        "#         # axes[i, 2].axis(\"off\")\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPbHLrDvALZN"
      },
      "outputs": [],
      "source": [
        "# \"\"\"----------------------------- Inference --------------------------------\"\"\"\n",
        "\n",
        "# metrics512 = defaultdict(float)\n",
        "# for b_idx, data in enumerate(val_dataloader):\n",
        "#     batch_images = data[\"image\"] # shape: [8, 3, 1024, 1024]\n",
        "#     images_resized_512 = preprocess_image(batch_images, (512, 512))\n",
        "\n",
        "#     # Generate predictions\n",
        "#     preds = generate_predictions(model, images_resized_512)  # shape: [1, 1, 512, 512]\n",
        "#     print(\"preds - Shape: \", preds.shape, \" || device: \", preds.get_device())\n",
        "#     seg_mask_ensemble_512 = preds > 0.4\n",
        "#     # mask preprocessing\n",
        "#     batch_masks = data[\"mask\"]\n",
        "#     masks_resized_512 = preprocess_image(batch_masks, (512, 512))\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"---------------------------- Dice Scores -----------------------------------\")\n",
        "\n",
        "\n",
        "#     dice_scores_512 = dice_metric(seg_mask_ensemble_512, masks_resized_512.to(DEVICE), per_image=True)\n",
        "#     print(\"dice_scores_512 - Shape: \", dice_scores_512.shape, \" || type: \", type(dice_scores_512))\n",
        "\n",
        "#     # dice_scores_1024 = dice_metric(combined_pred, batch_images.to(DEVICE), per_image=True)\n",
        "#     # print(\"dice_scores_1024 - Shape: \", dice_scores_1024.shape, \" || type: \", type(dice_scores_1024))\n",
        "\n",
        "#     print(\"Dice scores of predictions on the 512x512 scale\\n\", dice_scores_512)\n",
        "#     print(\"Dice score of the batch 512x512 scale: \", dice_scores_512.mean())\n",
        "\n",
        "#     print(\"---------------------------- SCALE = 1024x1024 -----------------------------------\")\n",
        "#     visualize_predictions(batch_images, batch_masks, seg_mask_ensemble_512)\n",
        "#     print(\"-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-\")\n",
        "#     break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Y7HFiESzDWcY",
        "Xfcsl-YW_ck9",
        "3A4XlffPf1jV",
        "7xtZrBIJeLH_",
        "A-y8ovXEK2St",
        "od_pZDUoK2Su",
        "EqxMQpqB5JFV",
        "BNLEHf3QK2Sv",
        "6-dcysMmK2Sw",
        "Vzze2VhIK2S5",
        "kOlM9jTEK2S5",
        "K136b3eqK2S5",
        "pOkP2HYpvsqr",
        "LcACWXgAXHQu",
        "gJFz9wAhzL4k",
        "iOQs9Sy2VBHR",
        "cLi3ZH-8ALZM"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deeplearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
