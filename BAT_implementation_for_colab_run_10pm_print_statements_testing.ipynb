{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY48Ap7wK2Sn"
      },
      "source": [
        "PNEUMOTHORAX SEGMENTATION USING SIIM-ACR DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMmEp3gqUyn1",
        "outputId": "609b10c1-d955-46b2-ac7a-7c7cc853a121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y_qWc-LUIJCv"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/qubvel/segmentation_models.pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz4WJSBg9UOk",
        "outputId": "c6d3dd1f-991f-489e-d1c6-764bafe1019c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sEfIEmqfADNu"
      },
      "outputs": [],
      "source": [
        "# !pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ECBIUxVDADNv"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from monai.networks.nets.swin_unetr import SwinTransformer as SwinViT\n",
        "# from monai.utils import ensure_tuple_rep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7HFiESzDWcY"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CUKZcMbBK2Ss"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from collections import defaultdict\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score\n",
        "import torchvision.models as models # NEW MARCH-JUNE EDIT\n",
        "\n",
        "try:\n",
        "    from itertools import ifilterfalse\n",
        "except ImportError:  # py3k\n",
        "    from itertools import filterfalse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pzbBvorxADNw"
      },
      "outputs": [],
      "source": [
        "def init_seed(SEED=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Un-DgSH9ADNx"
      },
      "outputs": [],
      "source": [
        "init_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfcsl-YW_ck9"
      },
      "source": [
        "## Config - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-vx6RpZ8_bba"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE         = 512 # This is efficient and better for the completion of the project\n",
        "WHOSE_DIR        = \"Neeshanth\"\n",
        "\n",
        "if WHOSE_DIR == \"Aathesh\":\n",
        "    DIR              = \"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\mask\")\n",
        "    DATA_FRAME_PATH  = \"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\RLE_kfold.csv\"\n",
        "elif WHOSE_DIR == \"Neeshanth\":\n",
        "    DIR              = \"/content/drive/MyDrive/datasets\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"/content/drive/MyDrive/datasets/train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"/content/drive/MyDrive/datasets/mask\")\n",
        "    DATA_FRAME_PATH  = \"/content/drive/MyDrive/datasets/RLE_kfold.csv\"\n",
        "\n",
        "elif WHOSE_DIR == \"Kousik\":\n",
        "    DIR              = r\"C:\\Users\\kousi\\Downloads\\datasets\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(r\"C:\\Users\\kousi\\Downloads\\datasets\\train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(r\"C:\\Users\\kousi\\Downloads\\datasets\\mask\")\n",
        "    DATA_FRAME_PATH  = r\"C:\\Users\\kousi\\Downloads\\datasets\\RLE_kfold.csv\"\n",
        "\n",
        "elif WHOSE_DIR == \"wonder_boys\":\n",
        "    DIR              = \"/content/drive/MyDrive/Colab_Notebooks/datasets/main\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"/content/drive/MyDrive/Colab_Notebooks/datasets/main/train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"/content/drive/MyDrive/Colab_Notebooks/datasets/main/mask\")\n",
        "    DATA_FRAME_PATH  = \"/content/drive/MyDrive/Colab_Notebooks/datasets/main/RLE_kfold.csv\"\n",
        "\n",
        "KFOLD_PATH       = \"\"\n",
        "TRAIN_BATCH_SIZE = 2\n",
        "VALID_BATCH_SIZE = 2\n",
        "BATCH_SIZE       = 2\n",
        "EPOCHS           = 50\n",
        "# Path for pretrained model weights\n",
        "TRAINING_MODEL_PATH = \"\"\n",
        "USE_SAMPLER      = True\n",
        "POSTIVE_PERC     = 0.8\n",
        "DEVICE           = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PRETRAINED       = False # False means we're using ImageNet weights, so essentially, it's pretrained & never from scratch!!!!!\n",
        "LEARNING_RATE    = 0.0001\n",
        "NUM_WORKERS      = 8\n",
        "USE_CRIT         = True\n",
        "FOLD_ID          = 4\n",
        "EVAL_METRICS      = [\"metric - it calculates dice coefficient.\"]\n",
        "\n",
        "# Regularization Settings\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "L2_WEIGHT_DECAY  = 0.000005\n",
        "GRADIENT_CLIPPING = True\n",
        "GRADIENT_CLIPPING_THRESHOLD = 0.1\n",
        "\n",
        "# IF U DON'T WANT TO CHANGE Learning rate from previous experiment then set this to True.\n",
        "OPTIMIZER_LOAD = False\n",
        "# U MUST VERIFY IF LR IS GETTING SET PROPERLY FOR THE SCHEDULER IN THE \"OPTIMIZER & SCHEDULER\" SECTION OF THE NOTEBOOK.\n",
        "\n",
        "# Learning Rate Scheduler Settings\n",
        "SCHEDULER        = \"ReduceLROnPlateau\"\n",
        "if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "    SCHEDULER_PARAMS = {'factor': 0.1, 'patience': 1, 'threshold': 0.0000001, 'min_lr': 0.0000001} # patience changed from 2 to 1 on 12-04-2025\n",
        "elif SCHEDULER == \"CosineAnnealingWarmRestarts\":\n",
        "    SCHEDULER_PARAMS = {'T_0': 1, 'T_mult': 2, 'eta_min': 0.00000001}\n",
        "elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "    SCHEDULER_PARAMS = {'T_max': 8, 'eta_min': 0.0000001}\n",
        "\n",
        "# Thresholds for 1024x1024 is there along with div by 2 values and div by 4 values\n",
        "TRIPLET_THRESHOLDS = [  [0.6, 500.0, 0.35], [0.67, 500.0, 0.37], [0.75, 500.0, 0.3],\n",
        "                        [0.75, 500.0, 0.4], [0.75, 1000.0, 0.3], [0.75, 1000.0, 0.4],\n",
        "                        [0.6, 1000.0, 0.3], [0.6, 1000.0, 0.4], [0.6, 1500.0, 0.3],\n",
        "                        [0.6, 1500.0, 0.4], [0.6, 250.0, 0.35], [0.67, 250.0, 0.37],\n",
        "                        [0.75, 250.0, 0.3], [0.75, 250.0, 0.4], [0.75, 500.0, 0.3],\n",
        "                        [0.75, 500.0, 0.4], [0.6, 500.0, 0.3], [0.6, 500.0, 0.4],\n",
        "                        [0.6, 750.0, 0.3], [0.6, 750.0, 0.4], [0.6, 1000, 0.35],\n",
        "                        [0.67, 1000, 0.37], [0.75, 1000, 0.3], [0.75, 1000, 0.4],\n",
        "                        [0.75, 2000, 0.3], [0.75, 2000, 0.4], [0.6, 2000, 0.3],\n",
        "                        [0.6, 2000, 0.4], [0.6, 3000, 0.3], [0.6, 3000, 0.4]       ]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    \"-------------------------------SAVING losses & metrics-------------------------------------\"\n",
        "\"\"\"\n",
        "ACCOUNT         = \"Neeshanth\" # \"Neeshanth\" or \"wonder_boys\" or others\n",
        "PURPOSE         = \"Project-2\" # training/inference/hyperparameter-tuning or hpt OR ANYTHING MORE SPECIFIC\n",
        "EXP_NO          = \"first_run\" # hpt experiment with changed settings\n",
        "PHASE           = \"12_04_2025\" # Just date\n",
        "\n",
        "EFFECTIVE_BATCH_SIZE = 2 # accumulation_steps * BATCH_SIZE\n",
        "\n",
        "if ACCOUNT == \"Neeshanth\":\n",
        "    # Save Config.txt\n",
        "    CONFIG_FILE_LOC = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "if ACCOUNT == \"Kousik\":\n",
        "    # Save Config.txt\n",
        "    CONFIG_FILE_LOC = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch-wise comboloss during training\n",
        "    store_batch_training_details_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\"  # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch-wise comboloss during validation\n",
        "    store_batch_validation_details_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch-wise train and val losses to monitor overfitting\n",
        "    save_progress_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch-wise dice coefficient\n",
        "    save_dice_score_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save BCE, dice & focal losses separately after training is done\n",
        "    save_3losses_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "\n",
        "if ACCOUNT == \"wonder_boys\":\n",
        "    # Save Config.txt file\n",
        "    CONFIG_FILE_LOC = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "if ACCOUNT == \"pc_aathesh\":\n",
        "    # Save Config.txt file\n",
        "    CONFIG_FILE_LOC = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\CHECKPOINTS\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_dice_score.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_all_loss_vals.csv\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class - CREATE A NEW FOLDER\n",
        "    model_checkpoint_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_best_checkpoint_thresholds.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A4XlffPf1jV"
      },
      "source": [
        "## Saving Config as .txt - TYPE ESSENTIAL DETAILS TO RECOVER THIS EXPERIMENT IN THE BELOW SNIPPET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB0Igy_Tf1jV"
      },
      "source": [
        "ESSENTIALS include current_notebook_loc_in_pc, previous_notebook_loc_in_pc, key_changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3likqSVNf1jV",
        "outputId": "b3618377-a2e6-4802-caaa-2819a382efdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current IST date and time is: 13-04-2025 22:35:05\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# Define the IST timezone\n",
        "ist_timezone = pytz.timezone('Asia/Kolkata')\n",
        "\n",
        "# Get the current time in IST\n",
        "ist_time = datetime.now(ist_timezone)\n",
        "\n",
        "# Format the date and time to DD-MM-YYYY HH:MM:SS\n",
        "formatted_ist_time = ist_time.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "\n",
        "# Print the formatted IST time\n",
        "print(\"Current IST date and time is:\", formatted_ist_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bwNCwwVf1jV",
        "outputId": "2812b8c1-f16c-4b41-a0ef-25db34182186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyper-parameters have been saved to /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_first_run_12_04_2025/Config.txt\n"
          ]
        }
      ],
      "source": [
        "hyperparameters = {\n",
        "    'image_size': f\"{IMG_SIZE}x{IMG_SIZE}\",\n",
        "    'date': formatted_ist_time,\n",
        "    'account': ACCOUNT,\n",
        "    'purpose': PURPOSE,\n",
        "    'experiment_no': EXP_NO,\n",
        "    'key_changes': \"\" ,\n",
        "    'checkpoint_used_loc': TRAINING_MODEL_PATH,\n",
        "    'is_optimizer_loaded': OPTIMIZER_LOAD,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'batch_size': EFFECTIVE_BATCH_SIZE,\n",
        "    'gradient_accumulation_steps': int(EFFECTIVE_BATCH_SIZE/BATCH_SIZE),\n",
        "    'num_epochs': EPOCHS,\n",
        "    'IsSamplerUsed': USE_SAMPLER,\n",
        "    'percentage_of_positive_samples': POSTIVE_PERC,\n",
        "    'optimizer': 'Adam',\n",
        "    'loss_function': 'ComboLoss',\n",
        "    'scheduler': SCHEDULER,\n",
        "    'scheduler_params': SCHEDULER_PARAMS,\n",
        "    'L2_Regularization_weight_decay':L2_WEIGHT_DECAY,\n",
        "    'early_stopping_patience': EARLY_STOPPING_PATIENCE,\n",
        "    'is_gradient_clipping_used': GRADIENT_CLIPPING,\n",
        "    'gradient_clipping_threshold': GRADIENT_CLIPPING_THRESHOLD,\n",
        "    'model': 'Hybrid Vision Transformer',\n",
        "    'training_phase': f\"Phase - {PHASE}\",\n",
        "    'fold_ID': FOLD_ID,\n",
        "    'GPU_name': torch.cuda.get_device_name(torch.cuda.current_device()),\n",
        "    'num_workers': NUM_WORKERS,\n",
        "    'weights_given_to_loss_functions': {'bce': 3, 'dice': 1, 'focal': 4},\n",
        "    'triplet_thresholds': TRIPLET_THRESHOLDS,\n",
        "    'evaluation_metrics': EVAL_METRICS,\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a formatted string\n",
        "hyperparameters_str = json.dumps(hyperparameters, indent=4)\n",
        "\n",
        "# Specify the file name\n",
        "file_name = CONFIG_FILE_LOC\n",
        "\n",
        "os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "# Write the string to a file\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(hyperparameters_str)\n",
        "\n",
        "print(f\"Hyper-parameters have been saved to {file_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xtZrBIJeLH_"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BjVO5UiC1z-h"
      },
      "outputs": [],
      "source": [
        "bce_losses = []\n",
        "dice_losses = []\n",
        "focal_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8MgSVyz79UOs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from scipy.ndimage import distance_transform_edt as distance\n",
        "from skimage import segmentation as skimage_seg\n",
        "\n",
        "\n",
        "def dice_loss(score, target):\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * target)\n",
        "    y_sum = torch.sum(target * target)\n",
        "    z_sum = torch.sum(score * score)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def dice_loss1(score, target):\n",
        "    # non-square\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * target)\n",
        "    y_sum = torch.sum(target)\n",
        "    z_sum = torch.sum(score)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def iou_loss(score, target):\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    tp_sum = torch.sum(score * target)\n",
        "    fp_sum = torch.sum(score * (1 - target))\n",
        "    fn_sum = torch.sum((1 - score) * target)\n",
        "    loss = (tp_sum + smooth) / (tp_sum + fp_sum + fn_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def entropy_loss(p, C=2):\n",
        "    ## p N*C*W*H*D\n",
        "    y1 = -1 * torch.sum(p * torch.log(p + 1e-6), dim=1) / torch.tensor(\n",
        "        np.log(C)).cuda()\n",
        "    ent = torch.mean(y1)\n",
        "\n",
        "    return ent\n",
        "\n",
        "\n",
        "def softmax_dice_loss(input_logits, target_logits):\n",
        "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_softmax = F.softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "    n = input_logits.shape[1]\n",
        "    dice = 0\n",
        "    for i in range(0, n):\n",
        "        dice += dice_loss1(input_softmax[:, i], target_softmax[:, i])\n",
        "    mean_dice = dice / n\n",
        "\n",
        "    return mean_dice\n",
        "\n",
        "\n",
        "def entropy_loss_map(p, C=2):\n",
        "    ent = -1 * torch.sum(p * torch.log(p + 1e-6), dim=1,\n",
        "                         keepdim=True) / torch.tensor(np.log(C)).cuda()\n",
        "    return ent\n",
        "\n",
        "\n",
        "def softmax_mse_loss(input_logits, target_logits):\n",
        "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_softmax = F.softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "\n",
        "    mse_loss = (input_softmax - target_softmax)**2\n",
        "    return mse_loss\n",
        "\n",
        "\n",
        "def softmax_kl_loss(input_logits, target_logits):\n",
        "    \"\"\"Takes softmax on both sides and returns KL divergence\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_log_softmax = F.log_softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "\n",
        "    # return F.kl_div(input_log_softmax, target_softmax)\n",
        "    kl_div = F.kl_div(input_log_softmax, target_softmax, reduction='none')\n",
        "    # mean_kl_div = torch.mean(0.2*kl_div[:,0,...]+0.8*kl_div[:,1,...])\n",
        "    return kl_div\n",
        "\n",
        "\n",
        "def symmetric_mse_loss(input1, input2):\n",
        "    \"\"\"Like F.mse_loss but sends gradients to both directions\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to both input1 and input2.\n",
        "    \"\"\"\n",
        "    assert input1.size() == input2.size()\n",
        "    return torch.mean((input1 - input2)**2)\n",
        "\n",
        "\n",
        "def compute_sdf01(segmentation):\n",
        "    \"\"\"\n",
        "    compute the signed distance map of binary mask\n",
        "    input: segmentation, shape = (batch_size, class, x, y, z)\n",
        "    output: the Signed Distance Map (SDM)\n",
        "    sdm(x) = 0; x in segmentation boundary\n",
        "             -inf|x-y|; x in segmentation\n",
        "             +inf|x-y|; x out of segmentation\n",
        "    \"\"\"\n",
        "    # print(type(segmentation), segmentation.shape)\n",
        "\n",
        "    segmentation = segmentation.astype(np.uint8)\n",
        "\n",
        "    if len(segmentation.shape) == 4:  # 3D image\n",
        "        segmentation = np.expand_dims(segmentation, 1)\n",
        "    normalized_sdf = np.zeros(segmentation.shape)\n",
        "    if segmentation.shape[1] == 1:\n",
        "        dis_id = 0\n",
        "    else:\n",
        "        dis_id = 1\n",
        "    for b in range(segmentation.shape[0]):  # batch size\n",
        "        for c in range(dis_id, segmentation.shape[1]):  # class_num\n",
        "            # ignore background\n",
        "            posmask = segmentation[b][c]\n",
        "            if np.max(posmask) == 0:\n",
        "                continue\n",
        "            negmask = ~posmask\n",
        "            posdis = distance(posmask)\n",
        "            negdis = distance(negmask)\n",
        "            boundary = skimage_seg.find_boundaries(\n",
        "                posmask, mode='inner').astype(np.uint8)\n",
        "            sdf = negdis / np.max(negdis) / 2 - posdis / np.max(\n",
        "                posdis) / 2 + 0.5\n",
        "            sdf[boundary > 0] = 0.5\n",
        "            normalized_sdf[b][c] = sdf\n",
        "    return normalized_sdf\n",
        "\n",
        "\n",
        "def compute_sdf1_1(segmentation):\n",
        "    \"\"\"\n",
        "    compute the signed distance map of binary mask\n",
        "    input: segmentation, shape = (batch_size, class, x, y, z)\n",
        "    output: the Signed Distance Map (SDM)\n",
        "    sdm(x) = 0; x in segmentation boundary\n",
        "             -inf|x-y|; x in segmentation\n",
        "             +inf|x-y|; x out of segmentation\n",
        "    \"\"\"\n",
        "    # print(type(segmentation), segmentation.shape)\n",
        "\n",
        "    segmentation = segmentation.astype(np.uint8)\n",
        "    if len(segmentation.shape) == 4:  # 3D image\n",
        "        segmentation = np.expand_dims(segmentation, 1)\n",
        "    normalized_sdf = np.zeros(segmentation.shape)\n",
        "    if segmentation.shape[1] == 1:\n",
        "        dis_id = 0\n",
        "    else:\n",
        "        dis_id = 1\n",
        "    for b in range(segmentation.shape[0]):  # batch size\n",
        "        for c in range(dis_id, segmentation.shape[1]):  # class_num\n",
        "            # ignore background\n",
        "            posmask = segmentation[b][c]\n",
        "            if np.max(posmask) == 0:\n",
        "                continue\n",
        "            negmask = ~posmask\n",
        "            posdis = distance(posmask)\n",
        "            negdis = distance(negmask)\n",
        "            boundary = skimage_seg.find_boundaries(\n",
        "                posmask, mode='inner').astype(np.uint8)\n",
        "            sdf = negdis / np.max(negdis) - posdis / np.max(posdis)\n",
        "            sdf[boundary > 0] = 0\n",
        "            normalized_sdf[b][c] = sdf\n",
        "    return normalized_sdf\n",
        "\n",
        "\n",
        "def compute_fore_dist(segmentation):\n",
        "    \"\"\"\n",
        "    compute the foreground of binary mask\n",
        "    input: segmentation, shape = (batch_size, class, x, y, z)\n",
        "    output: the Signed Distance Map (SDM)\n",
        "    sdm(x) = 0; x in segmentation boundary\n",
        "             -inf|x-y|; x in segmentation\n",
        "             +inf|x-y|; x out of segmentation\n",
        "    \"\"\"\n",
        "    # print(type(segmentation), segmentation.shape)\n",
        "\n",
        "    segmentation = segmentation.astype(np.uint8)\n",
        "    if len(segmentation.shape) == 4:  # 3D image\n",
        "        segmentation = np.expand_dims(segmentation, 1)\n",
        "    normalized_sdf = np.zeros(segmentation.shape)\n",
        "    if segmentation.shape[1] == 1:\n",
        "        dis_id = 0\n",
        "    else:\n",
        "        dis_id = 1\n",
        "    for b in range(segmentation.shape[0]):  # batch size\n",
        "        for c in range(dis_id, segmentation.shape[1]):  # class_num\n",
        "            # ignore background\n",
        "            posmask = segmentation[b][c]\n",
        "            posdis = distance(posmask)\n",
        "            normalized_sdf[b][c] = posdis / np.max(posdis)\n",
        "    return normalized_sdf\n",
        "\n",
        "\n",
        "def sum_tensor(inp, axes, keepdim=False):\n",
        "    axes = np.unique(axes).astype(int)\n",
        "    if keepdim:\n",
        "        for ax in axes:\n",
        "            inp = inp.sum(int(ax), keepdim=True)\n",
        "    else:\n",
        "        for ax in sorted(axes, reverse=True):\n",
        "            inp = inp.sum(int(ax))\n",
        "    return inp\n",
        "\n",
        "\n",
        "def AAAI_sdf_loss(net_output, gt):\n",
        "    \"\"\"\n",
        "    net_output: net logits; shape=(batch_size, class, x, y, z)\n",
        "    gt: ground truth; (shape (batch_size, 1, x, y, z) OR (batch_size, x, y, z))\n",
        "    \"\"\"\n",
        "    smooth = 1e-5\n",
        "    axes = tuple(range(2, len(net_output.size())))\n",
        "    shp_x = net_output.shape\n",
        "    shp_y = gt.shape\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if len(shp_x) != len(shp_y):\n",
        "            gt = gt.view((shp_y[0], 1, *shp_y[1:]))\n",
        "\n",
        "        if all([i == j for i, j in zip(net_output.shape, gt.shape)]):\n",
        "            # if this is the case then gt is probably already a one hot encoding\n",
        "            y_onehot = gt\n",
        "        else:\n",
        "            gt = gt.long()\n",
        "            y_onehot = torch.zeros(shp_x)\n",
        "            if net_output.device.type == \"cuda\":\n",
        "                y_onehot = y_onehot.cuda(net_output.device.index)\n",
        "            y_onehot.scatter_(1, gt, 1)\n",
        "        gt_sdm_npy = compute_sdf1_1(y_onehot.cpu().numpy())\n",
        "        if net_output.device.type == \"cuda\":\n",
        "            gt_sdm = torch.from_numpy(gt_sdm_npy).float().cuda(\n",
        "                net_output.device.index)\n",
        "        else:\n",
        "            gt_sdm = torch.from_numpy(gt_sdm_npy).float()\n",
        "    intersect = sum_tensor(net_output * gt_sdm, axes, keepdim=False)\n",
        "    pd_sum = sum_tensor(net_output**2, axes, keepdim=False)\n",
        "    gt_sum = sum_tensor(gt_sdm**2, axes, keepdim=False)\n",
        "    L_product = (intersect + smooth) / (intersect + pd_sum + gt_sum)\n",
        "    # print('L_product.shape', L_product.shape) (4,2)\n",
        "    L_SDF_AAAI = -L_product.mean() + torch.norm(net_output - gt_sdm,\n",
        "                                                1) / torch.numel(net_output)\n",
        "\n",
        "    return L_SDF_AAAI\n",
        "\n",
        "\n",
        "def sdf_kl_loss(net_output, gt):\n",
        "    \"\"\"\n",
        "    net_output: net logits; shape=(batch_size, class, x, y, z)\n",
        "    gt: ground truth; (shape (batch_size, 1, x, y, z) OR (batch_size, x, y, z))\n",
        "    \"\"\"\n",
        "    smooth = 1e-5\n",
        "    axes = tuple(range(2, len(net_output.size())))\n",
        "    shp_x = net_output.shape\n",
        "    shp_y = gt.shape\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if len(shp_x) != len(shp_y):\n",
        "            gt = gt.view((shp_y[0], 1, *shp_y[1:]))\n",
        "\n",
        "        if all([i == j for i, j in zip(net_output.shape, gt.shape)]):\n",
        "            # if this is the case then gt is probably already a one hot encoding\n",
        "            y_onehot = gt\n",
        "        else:\n",
        "            gt = gt.long()\n",
        "            y_onehot = torch.zeros(shp_x)\n",
        "            if net_output.device.type == \"cuda\":\n",
        "                y_onehot = y_onehot.cuda(net_output.device.index)\n",
        "            y_onehot.scatter_(1, gt, 1)\n",
        "        # print('y_onehot.shape', y_onehot.shape)\n",
        "        gt_sdf_npy = compute_sdf(y_onehot.cpu().numpy())\n",
        "        gt_sdf = torch.from_numpy(gt_sdf_npy + smooth).float().cuda(\n",
        "            net_output.device.index)\n",
        "    # print('net_output, gt_sdf', net_output.shape, gt_sdf.shape)\n",
        "    # exit()\n",
        "    sdf_kl_loss = F.kl_div(net_output,\n",
        "                           gt_sdf[:, 1:2, ...],\n",
        "                           reduction='batchmean')\n",
        "\n",
        "    return sdf_kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-GiUgprmeQ97"
      },
      "outputs": [],
      "source": [
        "eps = 1e-6\n",
        "\n",
        "\n",
        "def soft_dice_loss(outputs, targets, per_image=False, per_channel=False):\n",
        "    \"\"\"\n",
        "        If per_image = False, then the function calculates dice loss for a single image-mask pair.\n",
        "    \"\"\"\n",
        "    batch_size, n_channels = outputs.size(0), outputs.size(1)\n",
        "\n",
        "    eps = 1e-6\n",
        "    n_parts = 1\n",
        "    if per_image:\n",
        "        n_parts = batch_size\n",
        "    if per_channel:\n",
        "        n_parts = batch_size * n_channels\n",
        "\n",
        "    dice_target = targets.contiguous().view(n_parts, -1).float()\n",
        "    dice_output = outputs.contiguous().view(n_parts, -1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1)\n",
        "    loss = (1 - (2 * intersection + eps) / (union + eps)) # returns a tensor of size [8]\n",
        "    return loss.mean() # returns a tensor of size [1].\n",
        "\n",
        "def dice_metric(preds, trues, per_image=False, per_channel=False):\n",
        "    preds = preds.float()\n",
        "    return 1 - soft_dice_loss(preds, trues, per_image, per_channel)\n",
        "\n",
        "\n",
        "def jaccard(outputs, targets, per_image=False, non_empty=False, min_pixels=5):\n",
        "    batch_size = outputs.size()[0]\n",
        "    eps = 1e-3\n",
        "    if not per_image:\n",
        "        batch_size = 1\n",
        "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
        "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
        "    target_sum = torch.sum(dice_target, dim=1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    losses = 1 - (intersection + eps) / (torch.sum(dice_output + dice_target, dim=1) - intersection + eps)\n",
        "    if non_empty:\n",
        "        assert per_image == True\n",
        "        non_empty_images = 0\n",
        "        sum_loss = 0\n",
        "        for i in range(batch_size):\n",
        "            if target_sum[i] > min_pixels:\n",
        "                sum_loss += losses[i]\n",
        "                non_empty_images += 1\n",
        "        if non_empty_images == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return sum_loss / non_empty_images\n",
        "\n",
        "    return losses.mean()\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        dice_loss = soft_dice_loss(input, target, per_image=self.per_image)\n",
        "        dice_losses.append(dice_loss.item())\n",
        "        return dice_loss\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False, non_empty=False, apply_sigmoid=False,\n",
        "                 min_pixels=5):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "        self.non_empty = non_empty\n",
        "        self.apply_sigmoid = apply_sigmoid\n",
        "        self.min_pixels = min_pixels\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if self.apply_sigmoid:\n",
        "            input = torch.sigmoid(input)\n",
        "        return jaccard(input, target, per_image=self.per_image, non_empty=self.non_empty, min_pixels=self.min_pixels)\n",
        "\n",
        "class StableBCELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StableBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        bce_loss_with_logits = nn.BCEWithLogitsLoss(reduction='mean') # mean is taken across batches\n",
        "        bce_loss = bce_loss_with_logits(logits, target)\n",
        "        bce_losses.append(bce_loss.item())\n",
        "        return bce_loss # returns a tensor of size [1]\n",
        "\n",
        "\n",
        "class ComboLoss(nn.Module):\n",
        "    def __init__(self, weights, per_image=True, channel_weights=[1, 0.5, 0.5], channel_losses=None):\n",
        "        super().__init__()\n",
        "        self.weights = weights\n",
        "        self.bce = StableBCELoss()\n",
        "        self.dice = DiceLoss(per_image=True)\n",
        "        self.jaccard = JaccardLoss(per_image=True)\n",
        "        self.lovasz = LovaszLoss(per_image=per_image)\n",
        "        self.lovasz_sigmoid = LovaszLossSigmoid(per_image=per_image)\n",
        "        self.focal = FocalLoss2d()\n",
        "        self.mapping = {'bce': self.bce,\n",
        "                        'dice': self.dice,\n",
        "                        'focal': self.focal,\n",
        "                        'jaccard': self.jaccard,\n",
        "                        'lovasz': self.lovasz,\n",
        "                        'lovasz_sigmoid': self.lovasz_sigmoid}\n",
        "        self.expect_sigmoid = {'dice', 'focal', 'jaccard', 'lovasz_sigmoid'}\n",
        "        self.per_channel = {'dice', 'jaccard', 'lovasz_sigmoid'}\n",
        "        self.values = {}\n",
        "        self.channel_weights = channel_weights\n",
        "        self.channel_losses = channel_losses\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        loss = 0\n",
        "        weights = self.weights\n",
        "        sigmoid_input = torch.sigmoid(outputs)\n",
        "        for k, v in weights.items():\n",
        "            if not v:\n",
        "                continue\n",
        "            val = 0\n",
        "            if k in self.per_channel:\n",
        "                channels = targets.size(1)\n",
        "                for c in range(channels):\n",
        "                    if not self.channel_losses or k in self.channel_losses[c]:\n",
        "                        val += self.channel_weights[c] * self.mapping[k](sigmoid_input[:, c, ...] if k in self.expect_sigmoid else outputs[:, c, ...],\n",
        "                                               targets[:, c, ...])\n",
        "\n",
        "            else:\n",
        "                val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n",
        "\n",
        "            self.values[k] = val\n",
        "            loss += self.weights[k] * val\n",
        "        return loss.clamp(min=1e-5)\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    p = len(gt_sorted)\n",
        "    gts = gt_sorted.sum()\n",
        "    intersection = gts.float() - gt_sorted.float().cumsum(0)\n",
        "    union = gts.float() + (1 - gt_sorted).float().cumsum(0)\n",
        "    jaccard = 1. - intersection / union\n",
        "    if p > 1:  # cover 1-pixel case\n",
        "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
        "    return jaccard\n",
        "\n",
        "\n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
        "                    for log, lab in zip(logits, labels))\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "    if len(labels) == 0:\n",
        "        # only void pixels, the gradients should be 0\n",
        "        return logits.sum() * 0.\n",
        "    signs = 2. * labels.float() - 1.\n",
        "    errors = (1. - logits * Variable(signs))\n",
        "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
        "    perm = perm.data\n",
        "    gt_sorted = labels[perm]\n",
        "    grad = lovasz_grad(gt_sorted)\n",
        "    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = scores.view(-1)\n",
        "    labels = labels.view(-1)\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = (labels != ignore)\n",
        "    vscores = scores[valid]\n",
        "    vlabels = labels[valid]\n",
        "    return vscores, vlabels\n",
        "\n",
        "\n",
        "def lovasz_sigmoid(probas, labels, per_image=False, ignore=None):\n",
        "    \"\"\"\n",
        "    Multi-class Lovasz-Softmax loss\n",
        "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n",
        "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
        "      only_present: average only on classes present in ground truth\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class labels\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_sigmoid_flat(*flatten_binary_scores(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
        "                          for prob, lab in zip(probas, labels))\n",
        "    else:\n",
        "        loss = lovasz_sigmoid_flat(*flatten_binary_scores(probas, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def lovasz_sigmoid_flat(probas, labels):\n",
        "    \"\"\"\n",
        "    Multi-class Lovasz-Softmax loss\n",
        "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
        "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
        "      only_present: average only on classes present in ground truth\n",
        "    \"\"\"\n",
        "    fg = labels.float()\n",
        "    errors = (Variable(fg) - probas).abs()\n",
        "    errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
        "    perm = perm.data\n",
        "    fg_sorted = fg[perm]\n",
        "    loss = torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))\n",
        "    return loss\n",
        "\n",
        "def symmetric_lovasz(outputs, targets, ):\n",
        "    return (lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1 - targets)) / 2\n",
        "\n",
        "def mean(l, ignore_nan=False, empty=0):\n",
        "    \"\"\"\n",
        "    nanmean compatible with generators.\n",
        "    \"\"\"\n",
        "    l = iter(l)\n",
        "    if ignore_nan:\n",
        "        l = ifilterfalse(np.isnan, l)\n",
        "    try:\n",
        "        n = 1\n",
        "        acc = next(l)\n",
        "    except StopIteration:\n",
        "        if empty == 'raise':\n",
        "            raise ValueError('Empty mean')\n",
        "        return empty\n",
        "    for n, v in enumerate(l, 2):\n",
        "        acc += v\n",
        "    if n == 1:\n",
        "        return acc\n",
        "    return acc / n\n",
        "\n",
        "\n",
        "class LovaszLoss(nn.Module):\n",
        "    def __init__(self, ignore_index=255, per_image=True):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        return symmetric_lovasz(outputs, targets)\n",
        "\n",
        "class LovaszLossSigmoid(nn.Module):\n",
        "    def __init__(self, ignore_index=255, per_image=True):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        return lovasz_sigmoid(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n",
        "\n",
        "class FocalLoss2d(nn.Module):\n",
        "    def __init__(self, alpha=0.65, gamma=2,n_parts=BATCH_SIZE):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.n_parts = n_parts\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        n_parts = self.n_parts\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        eps = 1e-6\n",
        "        # non_ignored = targets.view(n_parts, -1) != self.ignore_index\n",
        "        targets = targets.view(n_parts, -1).float()\n",
        "        outputs = outputs.view(n_parts, -1)\n",
        "        # clamp just makes sure the values of the tensor is within the given range.\n",
        "        outputs = torch.clamp(outputs, eps, 1. - eps)\n",
        "        targets = torch.clamp(targets, eps, 1. - eps)\n",
        "        \"\"\" pt = predicted probability for the true class\n",
        "        when targets = 1, pt = outputs which means pt now has the predicted probability of positive class.\n",
        "        when tagets = 0, pt = 1 - outputs which means pt has the predicted probability of negative class.\n",
        "        \"\"\"\n",
        "        pt = (1 - targets) * (1 - outputs) + targets * outputs\n",
        "        alpha_t = self.alpha * targets + (1 - self.alpha)*(1 - targets)\n",
        "        pt_proc = -(alpha_t*((1. - pt) ** self.gamma * torch.log(pt))) # torch.log is natural log\n",
        "        focal_loss = pt_proc.mean(dim=1).mean()\n",
        "        focal_losses.append(focal_loss.item())\n",
        "        return focal_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-y8ovXEK2St"
      },
      "source": [
        "## Config - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQG4nEiq9UOu",
        "outputId": "8a06869a-2c7d-4da9-ddbd-89a39e04b2e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(arch='BAT', gpu='1', net_layer=50, seg_loss=0, pre=0, trans=1, point_pred=1, ppl=6, cross=0)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--arch', type=str, default='BAT')\n",
        "parser.add_argument('--gpu', type=str, default='1')\n",
        "parser.add_argument('--net_layer', type=int, default=50)\n",
        "# parser.add_argument('--dataset', type=str, default='isic2016')\n",
        "# parser.add_argument('--exp_name', type=str, default='')\n",
        "# parser.add_argument('--fold', type=str, default='0')\n",
        "# parser.add_argument('--lr_seg', type=float, default=1e-4)  #0.0003\n",
        "# parser.add_argument('--n_epochs', type=int, default=200)  #100\n",
        "# parser.add_argument('--bt_size', type=int, default=8)  #36\n",
        "parser.add_argument('--seg_loss', type=int, default=0, choices=[0, 1])\n",
        "# parser.add_argument('--aug', type=int, default=1)\n",
        "# parser.add_argument('--patience', type=int, default=10)  #50\n",
        "\n",
        "# pre-train\n",
        "parser.add_argument('--pre', type=int, default=0)\n",
        "\n",
        "# transformer\n",
        "parser.add_argument('--trans', type=int, default=1)\n",
        "\n",
        "# point constrain\n",
        "parser.add_argument('--point_pred', type=int, default=1)\n",
        "parser.add_argument('--ppl', type=int, default=6)\n",
        "\n",
        "# cross-scale framework\n",
        "parser.add_argument('--cross', type=int, default=0)\n",
        "\n",
        "parse_config , unknown = parser.parse_known_args()\n",
        "print(parse_config )\n",
        "\n",
        "# if parse_config.arch == 'BAT':\n",
        "#     parse_config.exp_name += '_{}_{}_{}_e{}'.format(parse_config.trans,\n",
        "#                                                     parse_config.point_pred,\n",
        "#                                                     parse_config.cross,\n",
        "#                                                     parse_config.ppl)\n",
        "# exp_name = parse_config.dataset + '/' + parse_config.exp_name + '_loss_' + str(\n",
        "#     parse_config.seg_loss) + '_aug_' + str(parse_config.aug) + '/fold_' + str(\n",
        "#         parse_config.fold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hz72Jtwr9UOu"
      },
      "outputs": [],
      "source": [
        "def ce_loss(pred, gt):\n",
        "    pred = torch.clamp(pred, 1e-6, 1 - 1e-6)\n",
        "    return (-gt * torch.log(pred) - (1 - gt) * torch.log(1 - pred)).mean()\n",
        "\n",
        "\n",
        "def structure_loss(pred, mask):\n",
        "    \"\"\"            TransFuse train loss        \"\"\"\n",
        "    \"\"\"            Without sigmoid             \"\"\"\n",
        "    weit = 1 + 5 * torch.abs(\n",
        "        F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
        "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduction='none')\n",
        "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
        "\n",
        "    pred = torch.sigmoid(pred)\n",
        "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
        "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
        "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
        "    return (wbce + wiou).mean()\n",
        "\n",
        "\n",
        "def focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.6,  #0.8\n",
        "    gamma: float = 2,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    p = inputs\n",
        "    ce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"mean\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t)**gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "CRITERION = [focal_loss, ce_loss][parse_config.seg_loss]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SW4ViKfK2St"
      },
      "source": [
        "The [ComboLoss](https://github.com/sneddy/pneumothorax-segmentation/blob/master/unet_pipeline/Losses.py#L104) function used in CRITERION below also comes from the winning solution by Anuar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "g3__XL68K2St"
      },
      "outputs": [],
      "source": [
        "# CRITERION        = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})\n",
        "\n",
        "# # Use During Inference Stage to store images of predicted segmentation masks\n",
        "# # PREDICTION_PATH  = \"/content/drive/MyDrive/Colab_Notebooks/datasets/archive_png_siim_acr/Predicted_masks/tests\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od_pZDUoK2Su"
      },
      "source": [
        "\n",
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JuSwxSqK2Su"
      },
      "source": [
        "General utility functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3LcOlxiwK2Sv"
      },
      "outputs": [],
      "source": [
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    fig,ax = plt.subplots(figsize=(10,6))\n",
        "    ax.imshow(img.permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LkS16buBK2Sv"
      },
      "outputs": [],
      "source": [
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    images = {k:v.numpy() for k,v in images.items() if isinstance(v, torch.Tensor)} #convert tensor to numpy\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    image, mask = images['image'], images['mask']\n",
        "    plt.imshow(image.transpose(1,2,0), vmin=0, vmax=1)\n",
        "    if mask.max()>0:\n",
        "        plt.imshow(mask.squeeze(0), alpha=0.25)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5KOb07Zlb3GN"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"Saving checkpoint...\")\n",
        "  torch.save(state, filename)\n",
        "  print(\"Checkpoint saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cIcBkgjOcPWQ",
        "outputId": "dbaa976e-04b2-4138-f375-495801898584"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef load_checkpoint(checkpoint):\\n  print(\"Loading checkpoint...\")\\n  model.load_state_dict(checkpoint[\\'state_dict\\'])\\n  optimizer.load_state_dict(checkpoint[\\'optimizer\\'])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\"\"\"\n",
        "def load_checkpoint(checkpoint):\n",
        "  print(\"Loading checkpoint...\")\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NrindR018hyO"
      },
      "outputs": [],
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item() # Calculates where two tensors are equal\n",
        "  acc = (correct / len(y_pred) ) * 100\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqxMQpqB5JFV"
      },
      "source": [
        "# ---------------------- DL Workflow -----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLEHf3QK2Sv"
      },
      "source": [
        "## 1. Data -> Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wbgxfpGK2Sv"
      },
      "source": [
        "### Create five-fold splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDyy5r93K2Sv",
        "outputId": "b437fa65-67e3-4442-ed14-df4aee59ab0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8570, 2142)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# single fold training for now, rerun notebook to train for multi-fold\n",
        "DF       = pd.read_csv(DATA_FRAME_PATH)\n",
        "TRAIN_DF = DF.query(f'kfold!={FOLD_ID}').reset_index(drop=True)\n",
        "VAL_DF   = DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)\n",
        "len(TRAIN_DF), len(VAL_DF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njKNWpqFK2Sw"
      },
      "source": [
        "### Dataset and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9ywRGbXTK2Sw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Key-Patch Map Generation\n",
        "# =============================================================================\n",
        "\n",
        "def resize_and_clip(img, target_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Resize image or mask to target_size using nearest neighbor interpolation\n",
        "    and clip its pixel values to [0, 255].\n",
        "    \"\"\"\n",
        "    resized = cv2.resize(img, target_size, interpolation=cv2.INTER_NEAREST)\n",
        "    resized = np.clip(resized, 0, 255)\n",
        "    return resized\n",
        "\n",
        "def create_circular_mask(h, w, center, radius):\n",
        "    \"\"\"\n",
        "    Create a boolean mask with a circle of given center and radius.\n",
        "    \"\"\"\n",
        "    Y, X = np.ogrid[:h, :w]\n",
        "    dist_from_center = np.sqrt((X - center[0])**2 + (Y - center[1])**2)\n",
        "    mask = dist_from_center <= radius\n",
        "    return mask\n",
        "\n",
        "def draw_msra_gaussian(heatmap, center, sigma):\n",
        "    \"\"\"\n",
        "    Draw a gaussian blob onto the heatmap centered at `center` with standard deviation `sigma`.\n",
        "    \"\"\"\n",
        "    tmp_size = sigma * 3\n",
        "    mu_x = int(center[0] + 0.5)\n",
        "    mu_y = int(center[1] + 0.5)\n",
        "    h, w = heatmap.shape[0], heatmap.shape[1]\n",
        "    ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
        "    br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
        "    if ul[0] >= w or ul[1] >= h or br[0] < 0 or br[1] < 0:\n",
        "        return heatmap\n",
        "    size = 2 * tmp_size + 1\n",
        "    x = np.arange(0, size, 1, np.float32)\n",
        "    y = x[:, np.newaxis]\n",
        "    x0 = y0 = size // 2\n",
        "    g = np.exp(-((x - x0)**2 + (y - y0)**2) / (2 * sigma**2))\n",
        "    g_x = max(0, -ul[0]), min(br[0], w) - ul[0]\n",
        "    g_y = max(0, -ul[1]), min(br[1], h) - ul[1]\n",
        "    img_x = max(0, ul[0]), min(br[0], w)\n",
        "    img_y = max(0, ul[1]), min(br[1], h)\n",
        "    heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]] = np.maximum(\n",
        "        heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]],\n",
        "        g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
        "    )\n",
        "    return heatmap\n",
        "\n",
        "def compute_key_patch_map(mask_orig, R=8, N=5):\n",
        "    \"\"\"\n",
        "    Compute the key-patch (ground truth point) map for a given mask.\n",
        "\n",
        "    This function:\n",
        "      1. Resizes the input mask (assumed to be 1024x1024) to 512x512.\n",
        "      2. Thresholds it to obtain a binary mask.\n",
        "      3. Downsamples the binary mask further for contour extraction.\n",
        "      4. Finds contours and, for each contour, selects keypoints based on neighborhood overlap statistics.\n",
        "      5. Draws a gaussian on a heatmap for each selected keypoint.\n",
        "\n",
        "    Returns the 512x512 generated heatmap.\n",
        "    \"\"\"\n",
        "    # Resize the original mask to 512x512\n",
        "    mask_resized = resize_and_clip(mask_orig, (512, 512))\n",
        "\n",
        "    # Threshold mask to binary (pixels > 127 become foreground)\n",
        "    _, mask_bin = cv2.threshold(mask_resized, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Downsample for contour extraction: using factor 2 (512 -> 256)\n",
        "    mask_small = cv2.resize(mask_bin, (mask_bin.shape[1] // 2, mask_bin.shape[0] // 2), interpolation=cv2.INTER_NEAREST)\n",
        "    mask_small = np.uint8(mask_small)\n",
        "\n",
        "    # Extract contours\n",
        "    contours, _ = cv2.findContours(mask_small, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "    # Initialize heatmap of size 512x512\n",
        "    point_heatmap = np.zeros((512, 512), dtype=np.float32)\n",
        "\n",
        "    for contour in contours:\n",
        "        points = contour[:, 0]  # Each point in contour is (x, y)\n",
        "        # Scale points to the 512x512 coordinate system\n",
        "        points = points * 2\n",
        "        points_number = points.shape[0]\n",
        "        if points_number < 20:\n",
        "            continue\n",
        "\n",
        "        # Adjust parameters for pneumothorax segmentation:\n",
        "        if points_number < 150:\n",
        "            radius = R\n",
        "            neighbor_points_n_oneside = N\n",
        "        elif points_number < 300:\n",
        "            radius = R + 4\n",
        "            neighbor_points_n_oneside = N + 5\n",
        "        else:\n",
        "            radius = R + 7\n",
        "            neighbor_points_n_oneside = N + 10\n",
        "\n",
        "        # Calculate the overlap statistic for each point in the contour.\n",
        "        stds = []\n",
        "        for i in range(points_number):\n",
        "            pt = points[i]\n",
        "            mask_circ = create_circular_mask(512, 512, (pt[0], pt[1]), radius)\n",
        "            overlap = np.sum(mask_circ * (mask_resized > 0)) / (np.pi * radius * radius)\n",
        "            stds.append(overlap)\n",
        "        stds = np.array(stds)\n",
        "\n",
        "        # Select keypoints by comparing each point's overlap with its neighbors.\n",
        "        for i in range(points_number):\n",
        "            neighbor_idx = np.concatenate([\n",
        "                np.arange(-neighbor_points_n_oneside, 0),\n",
        "                np.arange(1, neighbor_points_n_oneside + 1)\n",
        "            ]) + i\n",
        "            # Handle wrap-around indexing\n",
        "            neighbor_idx[neighbor_idx < 0] += points_number\n",
        "            neighbor_idx[neighbor_idx >= points_number] -= points_number\n",
        "\n",
        "            if stds[i] < np.min(stds[neighbor_idx]) or stds[i] > np.max(stds[neighbor_idx]):\n",
        "                point_heatmap = draw_msra_gaussian(point_heatmap, (points[i, 0], points[i, 1]), sigma=R)\n",
        "\n",
        "    return point_heatmap\n",
        "\n",
        "# =============================================================================\n",
        "# Modified Dataset Class\n",
        "# =============================================================================\n",
        "\n",
        "class Dataset():\n",
        "    def __init__(self, rle_df, image_base_dir, masks_base_dir, augmentation=None, mask_augmentation=None):\n",
        "        self.df                 = rle_df\n",
        "        self.image_base_dir     = image_base_dir\n",
        "        self.masks_base_dir     = masks_base_dir\n",
        "        self.image_ids          = rle_df.ImageId.values\n",
        "        self.augmentation       = augmentation\n",
        "        self.mask_augmentation  = mask_augmentation\n",
        "\n",
        "    def __image_ids__(self):\n",
        "        print(self.image_ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image_id  = self.image_ids[i]\n",
        "        img_path  = os.path.join(self.image_base_dir, Path(image_id + '.png'))\n",
        "        mask_path = os.path.join(self.masks_base_dir, Path(image_id + '.png'))\n",
        "\n",
        "        # Load image and mask using OpenCV.\n",
        "        # Image is loaded in color (RGB) for compatibility with models and augmentations.\n",
        "        image     = cv2.imread(img_path, 1)\n",
        "        # Mask is loaded in grayscale.\n",
        "        mask      = cv2.imread(mask_path, 0)\n",
        "\n",
        "        # Normalize image (scale to [0, 1]) and convert mask to binary.\n",
        "        image = (image / 255.0).astype(np.float32)\n",
        "        mask = (mask > 0).astype(np.float32)\n",
        "\n",
        "        # Generate the ground truth key-patch map (\"point\") on the fly.\n",
        "        # The compute function internally resizes the original mask (from 1024x1024 to 512x512) and generates the heatmap.\n",
        "        point = compute_key_patch_map(mask * 255)\n",
        "        # Note: We multiply mask by 255 because when read as grayscale it might be in {0,1} (after normalization).\n",
        "        # If your input mask is still in [0,255] scale before thresholding, adjust accordingly.\n",
        "\n",
        "        # Apply augmentations (if provided) to the image and mask.\n",
        "        if self.augmentation and self.mask_augmentation:\n",
        "            img_sample = {\"image\": image}\n",
        "            img_sample = self.augmentation(**img_sample)\n",
        "            image = img_sample['image']\n",
        "            mask_sample = {\"image\": mask}\n",
        "            mask_sample = self.mask_augmentation(**mask_sample)\n",
        "            mask = mask_sample[\"image\"]\n",
        "            # Note: We are not applying augmentation to the computed point map.\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'mask' : mask,\n",
        "            'point': torch.unsqueeze(torch.Tensor(point), axis=0)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxv5It8ZK2Sw",
        "outputId": "6b436fb5-e04f-4bf3-929e-f2c46645c4fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Test transforms\n",
        "TEST_TFMS = albu.Compose([\n",
        "    albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1),\n",
        "    albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
        "    ToTensorV2(),\n",
        "],\n",
        "    is_check_shapes=True,\n",
        "    p=1.0,\n",
        ")\n",
        "\n",
        "# New Train Transforms - Aggressive Augmentations to avoid overfitting.\n",
        "TFMS =  albu.Compose(\n",
        "    [\n",
        "        albu.OneOf([\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.OneOf([\n",
        "            albu.ElasticTransform(alpha=120, sigma=6.0, p=0.5),\n",
        "            albu.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), p=0.5),\n",
        "            albu.OpticalDistortion(distort_limit=(-2, 2), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, p=0.5),\n",
        "        albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "        albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1.0),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    p=1.0\n",
        ")\n",
        "\n",
        "MASK_TFMS =  albu.Compose(\n",
        "    [\n",
        "        albu.OneOf([\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.OneOf([\n",
        "            albu.ElasticTransform(alpha=120, sigma=6.0, p=0.5),\n",
        "            albu.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), p=0.5),\n",
        "            albu.OpticalDistortion(distort_limit=(-2, 2), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, p=0.5),\n",
        "        albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    p=1.0\n",
        ")\n",
        "\n",
        "mask_transform = albu.Compose([\n",
        "    albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "    ToTensorV2()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4DKpUR1QK2Sw"
      },
      "outputs": [],
      "source": [
        "# train dataset\n",
        "train_dataset = Dataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TFMS, MASK_TFMS)\n",
        "val_dataset   = Dataset(VAL_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TEST_TFMS, mask_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zno_0Q4eVHv",
        "outputId": "823c41fd-30a4-4113-aaf3-2f4cd4597406"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8570"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "train_dataset.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8SW1GpbQK2Sw"
      },
      "outputs": [],
      "source": [
        "# # plot one with mask\n",
        "# visualize(**train_dataset[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-dcysMmK2Sw"
      },
      "source": [
        "### Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Qx49nrt4K2Sw"
      },
      "outputs": [],
      "source": [
        "class PneumoSampler(Sampler):\n",
        "    def __init__(self, train_df, positive_perc=0.8):\n",
        "        assert positive_perc > 0, 'percentage of positive pneumothorax images must be greater then zero'\n",
        "        self.train_df = train_df\n",
        "        self.positive_perc = positive_perc\n",
        "        self.positive_idxs = self.train_df.query('has_mask==1').index.values\n",
        "        self.negative_idxs = self.train_df.query('has_mask!=1').index.values\n",
        "        self.n_positive = len(self.positive_idxs)\n",
        "        self.n_negative = int(self.n_positive * (1 - self.positive_perc) / self.positive_perc)\n",
        "\n",
        "    def __iter__(self):\n",
        "        negative_sample = np.random.choice(self.negative_idxs, size=self.n_negative)\n",
        "        shuffled = np.random.permutation(np.hstack((negative_sample, self.positive_idxs)))\n",
        "        return iter(shuffled.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_positive + self.n_negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kzJerUgtK2Sx"
      },
      "outputs": [],
      "source": [
        "SAMPLER = PneumoSampler(TRAIN_DF, positive_perc=POSTIVE_PERC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asd-jMF9K2Sx"
      },
      "source": [
        "### DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Saav3bv6K2Sx",
        "outputId": "f5d88160-5601-4734-9f57-a12b06b32ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE,\n",
        "                              shuffle=True if not USE_SAMPLER else False,\n",
        "                              num_workers=NUM_WORKERS,\n",
        "                              sampler=SAMPLER if USE_SAMPLER else None)\n",
        "val_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9nGVmHlj6z5",
        "outputId": "4d90b5fc-d9fd-4588-d09b-b63396b7cf1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7dbb5b365550>, <torch.utils.data.dataloader.DataLoader object at 0x7dbb5b366550>)\n",
            "Length of train dataloader: 1190 batches of 2\n",
            "length of validation dataloader: 1071 batches of 2\n"
          ]
        }
      ],
      "source": [
        "print(f\"Dataloaders: {train_dataloader , val_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"length of validation dataloader: {len(val_dataloader)} batches of {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jmBiDQJwFTN"
      },
      "source": [
        "for batch_index, data in enumerate(train_dataloader):\n",
        "    for z in range(3):\n",
        "        print(\"Image: \", data['image'].shape)\n",
        "        print(\"Mask: \", data['mask'].shape)\n",
        "        print(\"data['mask'].unsqueeze(1)\", data['mask'].unsqueeze(1).shape)\n",
        "    break\n",
        "\n",
        "Output\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPWAK3BK9UPJ"
      },
      "source": [
        "## NonBlockND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "EJdpZrHW9UPJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class _NonLocalBlockND(nn.Module):\n",
        "    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n",
        "        \"\"\"\n",
        "        :param in_channels:\n",
        "        :param inter_channels:\n",
        "        :param dimension:\n",
        "        :param sub_sample:\n",
        "        :param bn_layer:\n",
        "        \"\"\"\n",
        "\n",
        "        super(_NonLocalBlockND, self).__init__()\n",
        "\n",
        "        assert dimension in [1, 2, 3]\n",
        "\n",
        "        self.dimension = dimension\n",
        "        self.sub_sample = sub_sample\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.inter_channels = inter_channels\n",
        "\n",
        "        if self.inter_channels is None:\n",
        "            self.inter_channels = in_channels // 2\n",
        "            if self.inter_channels == 0:\n",
        "                self.inter_channels = 1\n",
        "\n",
        "        if dimension == 3:\n",
        "            conv_nd = nn.Conv3d\n",
        "            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "            bn = nn.BatchNorm3d\n",
        "        elif dimension == 2:\n",
        "            conv_nd = nn.Conv2d\n",
        "            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "            bn = nn.BatchNorm2d\n",
        "        else:\n",
        "            conv_nd = nn.Conv1d\n",
        "            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n",
        "            bn = nn.BatchNorm1d\n",
        "\n",
        "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                         kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        if bn_layer:\n",
        "            self.W = nn.Sequential(\n",
        "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
        "                        kernel_size=1, stride=1, padding=0),\n",
        "                bn(self.in_channels)\n",
        "            )\n",
        "            nn.init.constant_(self.W[1].weight, 0)\n",
        "            nn.init.constant_(self.W[1].bias, 0)\n",
        "        else:\n",
        "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
        "                             kernel_size=1, stride=1, padding=0)\n",
        "            nn.init.constant_(self.W.weight, 0)\n",
        "            nn.init.constant_(self.W.bias, 0)\n",
        "\n",
        "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                             kernel_size=1, stride=1, padding=0)\n",
        "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                           kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        if sub_sample:\n",
        "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
        "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
        "\n",
        "    def forward(self, x, y, return_nl_map=False):\n",
        "        \"\"\"\n",
        "        :param x: (b, c, h, w)\n",
        "        :param y: (b, c, 1)\n",
        "        :param return_nl_map: if True return z, nl_map, else only return z.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        h, w = x.shape[2:]\n",
        "\n",
        "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
        "        #g_x = g_x.permute(0, 2, 1)\n",
        "\n",
        "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
        "        theta_x = theta_x.permute(0, 2, 1)\n",
        "\n",
        "        #phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
        "        phi_x = self.phi(y.unsqueeze(-1)).view(batch_size, self.inter_channels, -1)\n",
        "        f = torch.matmul(theta_x, phi_x)\n",
        "        #f_div_C = F.softmax(f, dim=-1)\n",
        "        f_div_C = torch.sigmoid(f)\n",
        "        f_div_C = f_div_C.permute(0,2,1).contiguous()\n",
        "\n",
        "        #y = torch.matmul(f_div_C, g_x)\n",
        "        #y = y.permute(0, 2, 1).contiguous()\n",
        "        y = g_x * f_div_C\n",
        "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
        "        W_y = self.W(y)\n",
        "        z = W_y + x\n",
        "\n",
        "        if return_nl_map:\n",
        "            return z, f_div_C.view(batch_size, 1, h, w)\n",
        "        return z\n",
        "\n",
        "class NONLocalBlock2D(_NonLocalBlockND):\n",
        "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
        "        super(NONLocalBlock2D, self).__init__(in_channels,\n",
        "                                              inter_channels=inter_channels,\n",
        "                                              dimension=2, sub_sample=sub_sample,\n",
        "                                              bn_layer=bn_layer,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld_aLxHH9UPK"
      },
      "source": [
        "## BAG - Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAar3gnt9UPK"
      },
      "source": [
        "### process_point.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LKkD4dhS9UPK"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import skimage.draw\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def create_circular_mask(h, w, center, radius):\n",
        "    Y, X = np.ogrid[:h, :w]\n",
        "    dist_from_center = np.sqrt((X - center[0])**2 + (Y - center[1])**2)\n",
        "    mask = dist_from_center <= radius\n",
        "    return mask\n",
        "\n",
        "\n",
        "def NMS(heatmap, kernel=13):\n",
        "    hmax = F.max_pool2d(heatmap, kernel, stride=1, padding=(kernel - 1) // 2)\n",
        "    keep = (hmax == heatmap).float()\n",
        "    return heatmap * keep, hmax, keep\n",
        "\n",
        "\n",
        "def draw_msra_gaussian(heatmap, center, sigma):\n",
        "    tmp_size = sigma * 3\n",
        "    mu_x = int(center[0] + 0.5)\n",
        "    mu_y = int(center[1] + 0.5)\n",
        "    w, h = heatmap.shape[0], heatmap.shape[1]\n",
        "    ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
        "    br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
        "    if ul[0] >= h or ul[1] >= w or br[0] < 0 or br[1] < 0:\n",
        "        return heatmap\n",
        "    size = 2 * tmp_size + 1\n",
        "    x = np.arange(0, size, 1, np.float32)\n",
        "    y = x[:, np.newaxis]\n",
        "    x0 = y0 = size // 2\n",
        "    g = np.exp(-((x - x0)**2 + (y - y0)**2) / (2 * sigma**2))\n",
        "    g_x = max(0, -ul[0]), min(br[0], h) - ul[0]\n",
        "    g_y = max(0, -ul[1]), min(br[1], w) - ul[1]\n",
        "    img_x = max(0, ul[0]), min(br[0], h)\n",
        "    img_y = max(0, ul[1]), min(br[1], w)\n",
        "    heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]] = np.maximum(\n",
        "        heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]], g[g_y[0]:g_y[1],\n",
        "                                                         g_x[0]:g_x[1]])\n",
        "    return heatmap\n",
        "\n",
        "\n",
        "def kpm_gen(label_path, R, N):\n",
        "    label = np.load(label_path)\n",
        "    #     label = label[0]\n",
        "    label_ori = label.copy()\n",
        "    label = label[::4, ::4]\n",
        "    label = np.uint8(label * 255)\n",
        "    contours, hierarchy = cv2.findContours(label, cv2.RETR_LIST,\n",
        "                                           cv2.CHAIN_APPROX_NONE)\n",
        "    contour_len = len(contours)\n",
        "\n",
        "    label = np.repeat(label[..., np.newaxis], 3, axis=-1)\n",
        "    draw_label = cv2.drawContours(label.copy(), contours, -1, (0, 0, 255), 1)\n",
        "\n",
        "    point_file = []\n",
        "    if contour_len == 0:\n",
        "        point_heatmap = np.zeros((512, 512))\n",
        "    else:\n",
        "        point_heatmap = np.zeros((512, 512))\n",
        "        for contour in contours:\n",
        "            stds = []\n",
        "            points = contour[:, 0]  # (N,2)\n",
        "            points = points * 4\n",
        "            points_number = contour.shape[0]\n",
        "            if points_number < 30:\n",
        "                continue\n",
        "\n",
        "            if points_number < 100:\n",
        "                radius = 6\n",
        "                neighbor_points_n_oneside = 3\n",
        "            elif points_number < 200:\n",
        "                radius = 10\n",
        "                neighbor_points_n_oneside = 15\n",
        "            elif points_number < 300:\n",
        "                radius = 10\n",
        "                neighbor_points_n_oneside = 20\n",
        "            elif points_number < 350:\n",
        "                radius = 15\n",
        "                neighbor_points_n_oneside = 30\n",
        "            else:\n",
        "                radius = 10\n",
        "                neighbor_points_n_oneside = 40\n",
        "\n",
        "            for i in range(points_number):\n",
        "                current_point = points[i]\n",
        "                mask = create_circular_mask(512, 512, points[i], radius)\n",
        "                overlap_area = np.sum(\n",
        "                    mask * label_ori) / (np.pi * radius * radius)\n",
        "                stds.append(overlap_area)\n",
        "            print(\"stds len: \", len(stds))\n",
        "\n",
        "            # show\n",
        "            selected_points = []\n",
        "            stds = np.array(stds)\n",
        "            neighbor_points = []\n",
        "            for i in range(len(points)):\n",
        "                current_point = points[i]\n",
        "                neighbor_points_index = np.concatenate([\n",
        "                    np.arange(-neighbor_points_n_oneside, 0),\n",
        "                    np.arange(1, neighbor_points_n_oneside + 1)\n",
        "                ]) + i\n",
        "                neighbor_points_index[np.where(\n",
        "                    neighbor_points_index < 0)[0]] += len(points)\n",
        "                neighbor_points_index[np.where(\n",
        "                    neighbor_points_index > len(points) - 1)[0]] -= len(points)\n",
        "                if stds[i] < np.min(\n",
        "                        stds[neighbor_points_index]) or stds[i] > np.max(\n",
        "                            stds[neighbor_points_index]):\n",
        "                    #                     print(points[i])\n",
        "                    point_heatmap = draw_msra_gaussian(\n",
        "                        point_heatmap, (points[i, 0], points[i, 1]), 5)\n",
        "                    selected_points.append(points[i])\n",
        "\n",
        "            print(\"selected_points num: \", len(selected_points))\n",
        "            #             print(selected_points)\n",
        "            maskk = np.zeros((512, 512))\n",
        "            rr, cc = skimage.draw.polygon(\n",
        "                np.array(selected_points)[:, 1],\n",
        "                np.array(selected_points)[:, 0])\n",
        "            maskk[rr, cc] = 1\n",
        "            intersection = np.logical_and(label_ori, maskk)\n",
        "            union = np.logical_or(label_ori, maskk)\n",
        "            iou_score = np.sum(intersection) / np.sum(union)\n",
        "            print(iou_score)\n",
        "    return label_ori, point_heatmap\n",
        "\n",
        "\n",
        "def point_gen_isic2018():\n",
        "    R = 10\n",
        "    N = 25\n",
        "    data_dir = '/raid/wjc/data/skin_lesion/isic2018/Label'\n",
        "\n",
        "    save_dir = data_dir.replace('Label', 'Point')\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    path_list = os.listdir(data_dir)\n",
        "    path_list.sort()\n",
        "    num = 0\n",
        "    for path in tqdm(path_list):\n",
        "        name = path[:-4]\n",
        "        label_path = os.path.join(data_dir, path)\n",
        "        print(label_path)\n",
        "        label_ori, point_heatmap = kpm_gen(label_path, R, N)\n",
        "\n",
        "        save_path = os.path.join(save_dir, name + '.npy')\n",
        "        np.save(save_path, point_heatmap)\n",
        "        num += 1\n",
        "\n",
        "\n",
        "def point_gen_isic2016():\n",
        "    R = 10\n",
        "    N = 25\n",
        "    for split in ['Train', 'Test', 'Validation']:\n",
        "        data_dir = '/raid/wjc/data/skin_lesion/isic2016/{}/Label'.format(split)\n",
        "\n",
        "        save_dir = data_dir.replace('Label', 'Point')\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        path_list = os.listdir(data_dir)\n",
        "        path_list.sort()\n",
        "        num = 0\n",
        "        for path in tqdm(path_list):\n",
        "            name = path[:-4]\n",
        "            label_path = os.path.join(data_dir, path)\n",
        "            print(label_path)\n",
        "            label_ori, point_heatmap = kpm_gen(label_path, R, N)\n",
        "            save_path = os.path.join(save_dir, name + '.npy')\n",
        "            np.save(save_path, point_heatmap)\n",
        "            num += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rpAsnr89UPL"
      },
      "source": [
        "## BAT utility modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_mqFXmim9UPL"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=512,\n",
        "                 dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "\n",
        "    def forward(self, tgt, src):\n",
        "        \"tgt shape: Batch_size, C, H, W \"\n",
        "        \"src shape: Batch_size, 1, C    \"\n",
        "\n",
        "        B, C, h, w = tgt.shape\n",
        "        tgt = tgt.view(B, C, h*w).permute(2,0,1)  # shape: L, B, C\n",
        "\n",
        "        src = src.permute(1,0,2)  # shape: Q:1, B, C\n",
        "\n",
        "        fusion_feature = self.cross_attn(query=tgt,\n",
        "                                         key=src,\n",
        "                                         value=src)[0]\n",
        "        tgt = tgt + self.dropout1(fusion_feature)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt1 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout2(tgt1)\n",
        "        tgt = self.norm2(tgt)\n",
        "        return tgt.permute(1, 2, 0).view(B, C, h, w)\n",
        "\n",
        "class BoundaryCrossAttention(CrossAttention):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=True,\n",
        "                 dim_feedforward=512,\n",
        "                 dropout=0.0):\n",
        "        super().__init__(d_model, nhead, dim_feedforward, dropout)\n",
        "\n",
        "        #self.BAG = nn.Sequential(\n",
        "        #    nn.Conv2d(d_model, d_model, kernel_size=3, padding=1, bias=False),\n",
        "        #    nn.BatchNorm2d(d_model),\n",
        "        #    nn.ReLU(inplace=False),\n",
        "        #    nn.Conv2d(d_model, d_model, kernel_size=3, padding=1, bias=False),\n",
        "        #    nn.BatchNorm2d(d_model),\n",
        "        #    nn.ReLU(inplace=False),\n",
        "        #    nn.Conv2d(d_model, 1, kernel_size=1))\n",
        "        self.BAG_type = BAG_type\n",
        "        if self.BAG_type == '1D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous1D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate1D(d_model)\n",
        "        elif self.BAG_type == '2D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous2D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate2D(d_model)\n",
        "\n",
        "    def forward(self, tgt, src):\n",
        "        \"tgt shape: Batch_size, C, H, W \"\n",
        "        \"src shape: Batch_size, 1, C    \"\n",
        "\n",
        "        B, C, h, w = tgt.shape\n",
        "        tgt = tgt.view(B, C, h*w).permute(2,0,1)  # shape: L, B, C\n",
        "\n",
        "        src = src.permute(1,0,2)  # shape: Q:1, B, C\n",
        "\n",
        "        fusion_feature = self.cross_attn(query=tgt,\n",
        "                                         key=src,\n",
        "                                         value=src)[0]\n",
        "        tgt = tgt + self.dropout1(fusion_feature)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt1 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout2(tgt1)\n",
        "        tgt = self.norm2(tgt)\n",
        "\n",
        "        if self.BAG_type == '1D':\n",
        "            tgt = tgt.permute(1,2,0)\n",
        "            tgt, weights = self.BAG(tgt)\n",
        "            tgt = tgt.view(B, C, h, w).contiguous()\n",
        "            weights = weights.view(B, 1, h, w)\n",
        "        elif self.BAG_type == '2D':\n",
        "            tgt = tgt.permute(1,2,0).view(B, C, h, w)\n",
        "            tgt, weights = self.BAG(tgt)\n",
        "            tgt = tgt.contiguous()\n",
        "        return tgt, weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements a multi head attention module like proposed in:\n",
        "    https://arxiv.org/abs/2005.12872\n",
        "    \"\"\"\n",
        "    def __init__(self, query_dimension: int = 64, hidden_features: int = 64, number_of_heads: int = 16,\n",
        "                 dropout: float = 0.0) -> None:\n",
        "        \"\"\"\n",
        "        Constructor method\n",
        "        :param query_dimension: (int) Dimension of query tensor\n",
        "        :param hidden_features: (int) Number of hidden features in detr\n",
        "        :param number_of_heads: (int) Number of prediction heads\n",
        "        :param dropout: (float) Dropout factor to be utilized\n",
        "        \"\"\"\n",
        "        # Call super constructor\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Save parameters\n",
        "        self.hidden_features = hidden_features\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.dropout = dropout\n",
        "        # Init layer\n",
        "        self.layer_box_embedding = nn.Linear(in_features=query_dimension, out_features=hidden_features, bias=True)\n",
        "        # Init convolution layer\n",
        "        self.layer_image_encoding = nn.Conv2d(in_channels=query_dimension, out_channels=hidden_features,\n",
        "                                              kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=True)\n",
        "        # Init normalization factor\n",
        "        self.normalization_factor = torch.tensor(self.hidden_features / self.number_of_heads, dtype=torch.float).sqrt()\n",
        "\n",
        "        # Linear\n",
        "        self.linear = nn.Linear(in_features=number_of_heads, out_features=1)\n",
        "\n",
        "    def forward(self, input_box_embeddings: torch.Tensor, input_image_encoding: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        :param input_box_embeddings: (torch.Tensor) Bounding box embeddings\n",
        "        :param input_image_encoding: (torch.Tensor) Encoded image of the transformer encoder\n",
        "        :return: (torch.Tensor) Attention maps of shape (batch size, n, m, height, width)\n",
        "        \"\"\"\n",
        "        # Map box embeddings\n",
        "        output_box_embeddings = self.layer_box_embedding(input_box_embeddings)\n",
        "        # Map image features\n",
        "        output_image_encoding = self.layer_image_encoding(input_image_encoding)\n",
        "        # Reshape output box embeddings\n",
        "        output_box_embeddings = output_box_embeddings.view(output_box_embeddings.shape[0],\n",
        "                                                           output_box_embeddings.shape[1],\n",
        "                                                           self.number_of_heads,\n",
        "                                                           self.hidden_features // self.number_of_heads)\n",
        "        # Reshape output image encoding\n",
        "        output_image_encoding = output_image_encoding.view(output_image_encoding.shape[0],\n",
        "                                                           self.number_of_heads,\n",
        "                                                           self.hidden_features // self.number_of_heads,\n",
        "                                                           output_image_encoding.shape[-2],\n",
        "                                                           output_image_encoding.shape[-1])\n",
        "        # Combine tensors and normalize\n",
        "        output = torch.einsum(\"bqnc,bnchw->bqnhw\",\n",
        "                              output_box_embeddings * self.normalization_factor,\n",
        "                              output_image_encoding)\n",
        "        # Apply softmax\n",
        "        output = F.softmax(output.flatten(start_dim=2), dim=-1).view_as(output)\n",
        "\n",
        "        # Linear: to generate one map\n",
        "        b, _, _, h, w = output.shape\n",
        "        output = torch.sigmoid(self.linear(output.flatten(start_dim=3).permute(0,1,3,2))).view(b,1,h,w)\n",
        "\n",
        "        # Perform dropout if utilized\n",
        "        if self.dropout > 0.0:\n",
        "            output = F.dropout(input=output, p=self.dropout, training=self.training)\n",
        "#         print(\"MultiHead Attention\",output.shape)\n",
        "        return output.contiguous()\n",
        "\n",
        "\n",
        "class BoundaryWiseAttentionGateAtrous2D(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "\n",
        "        super(BoundaryWiseAttentionGateAtrous2D,self).__init__()\n",
        "\n",
        "        modules = []\n",
        "\n",
        "        if hidden_channels == None:\n",
        "            hidden_channels = in_channels // 2\n",
        "\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=1, dilation=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=2, dilation=2, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=4, dilation=4, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=6, dilation=6, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "\n",
        "        self.convs = nn.ModuleList(modules)\n",
        "\n",
        "        self.conv_out = nn.Conv2d(5 * hidden_channels, 1, 1, bias=False)\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, H, W \"\n",
        "        \" return: feature, weight (B,C,H,W) \"\n",
        "        res = []\n",
        "        for conv in self.convs:\n",
        "            res.append(conv(x))\n",
        "        res = torch.cat(res, dim=1)\n",
        "        weight = torch.sigmoid(self.conv_out(res))\n",
        "        x = x * weight + x\n",
        "        return x, weight\n",
        "\n",
        "class BoundaryWiseAttentionGateAtrous1D(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "\n",
        "        super(BoundaryWiseAttentionGateAtrous1D,self).__init__()\n",
        "\n",
        "        modules = []\n",
        "\n",
        "        if hidden_channels == None:\n",
        "            hidden_channels = in_channels // 2\n",
        "\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 1, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=1, dilation=1, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=2, dilation=2, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=4, dilation=4, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=6, dilation=6, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "\n",
        "        self.convs = nn.ModuleList(modules)\n",
        "\n",
        "        self.conv_out = nn.Conv1d(5 * hidden_channels, 1, 1, bias=False)\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, L \"\n",
        "        \" return: feature, weight (B,C,L) \"\n",
        "        res = []\n",
        "        for conv in self.convs:\n",
        "            res.append(conv(x))\n",
        "        res = torch.cat(res, dim=1)\n",
        "        weight = torch.sigmoid(self.conv_out(res))\n",
        "        x = x * weight + x\n",
        "        return x, weight\n",
        "\n",
        "class BoundaryWiseAttentionGate2D(nn.Sequential):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "        super(BoundaryWiseAttentionGate2D,self).__init__(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels, 1, kernel_size=1))\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, H, W \"\n",
        "        \" return: feature, weight (B,C,H,W) \"\n",
        "        weight = torch.sigmoid(super(BoundaryWiseAttentionGate2D,self).forward(x))\n",
        "        x = x * weight + x\n",
        "        return x, weight\n",
        "\n",
        "class BoundaryWiseAttentionGate1D(nn.Sequential):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "        super(BoundaryWiseAttentionGate1D,self).__init__(\n",
        "            nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv1d(in_channels, 1, kernel_size=1))\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, L \"\n",
        "        \" return: feature, weight (B,C,L) \"\n",
        "        weight = torch.sigmoid(super(BoundaryWiseAttentionGate1D,self).forward(x))\n",
        "        x = x * weight + x\n",
        "        return x, weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wyBin1-klDc"
      },
      "source": [
        "## transformer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "p7bRaHbhklDd"
      },
      "outputs": [],
      "source": [
        "# Based on: https://github.com/facebookresearch/detr/blob/master/models/transformer.py\n",
        "import copy\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "# from .BAT_Modules import BoundaryWiseAttentionGate2D, BoundaryWiseAttentionGate1D, BoundaryWiseAttentionGateAtrous2D, BoundaryWiseAttentionGateAtrous1D\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers,\n",
        "                                          encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        #         print(\"Trans Encoder\",memory.shape)\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers=6,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = BoundaryAwareTransformerEncoderLayer(\n",
        "            d_model, nhead, BAG_type, Atrous, dim_feedforward, dropout,\n",
        "            activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = BoundaryAwareTransformerEncoder(point_pred_layers,\n",
        "                                                       encoder_layer,\n",
        "                                                       num_encoder_layers,\n",
        "                                                       encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory, weights = self.encoder(src,\n",
        "                                       src_key_padding_mask=mask,\n",
        "                                       pos=pos_embed,\n",
        "                                       height=h,\n",
        "                                       width=w)\n",
        "\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h,\n",
        "                                                                w), weights\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask,\n",
        "                           pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers,\n",
        "                 encoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None):\n",
        "        super().__init__()\n",
        "        self.point_pred_layers = point_pred_layers\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        output = src\n",
        "        weights = []\n",
        "\n",
        "        for layer_i, layer in enumerate(self.layers):\n",
        "            if layer_i > self.num_layers - self.point_pred_layers - 1:\n",
        "                output, weight = layer(\n",
        "                    True,\n",
        "                    output,\n",
        "                    src_mask=mask,\n",
        "                    src_key_padding_mask=src_key_padding_mask,\n",
        "                    pos=pos,\n",
        "                    height=height,\n",
        "                    width=width)\n",
        "                weights.append(weight)\n",
        "            else:\n",
        "                output = layer(False,\n",
        "                               output,\n",
        "                               src_mask=mask,\n",
        "                               src_key_padding_mask=src_key_padding_mask,\n",
        "                               pos=pos,\n",
        "                               height=height,\n",
        "                               width=width)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src2,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoderLayer(TransformerEncoderLayer):\n",
        "    \"    Add Boundary-wise Attention Gate to Transformer's Encoder\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=True,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation,\n",
        "                         normalize_before)\n",
        "        if BAG_type == '1D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous1D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate1D(d_model)\n",
        "        elif BAG_type == '2D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous2D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate2D(d_model)\n",
        "        self.BAG_type = BAG_type\n",
        "\n",
        "    def forward(self,\n",
        "                use_bag,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        if self.normalize_before:\n",
        "            features = self.forward_pre(src, src_mask, src_key_padding_mask,\n",
        "                                        pos)\n",
        "            if use_bag:\n",
        "                b, c = features.shape[1:]\n",
        "                if self.BAG_type == '1D':\n",
        "                    features = features.permute(1, 2, 0)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.permute(2, 0, 1).contiguous()\n",
        "                    weights = weights.view(b, 1, height, width)\n",
        "                elif self.BAG_type == '2D':\n",
        "                    features = features.permute(1, 2,\n",
        "                                                0).view(b, c, height, width)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.flatten(2).permute(2, 0,\n",
        "                                                           1).contiguous()\n",
        "                return features, weights\n",
        "            else:\n",
        "                return features\n",
        "        features = self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "        if use_bag:\n",
        "            b, c = features.shape[1:]\n",
        "            if self.BAG_type == '1D':\n",
        "                features = features.permute(1, 2, 0)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.permute(2, 0, 1).contiguous()\n",
        "                weights = weights.view(b, 1, height, width)\n",
        "            elif self.BAG_type == '2D':\n",
        "                features = features.permute(1, 2, 0).view(b, c, height, width)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.flatten(2).permute(2, 0, 1).contiguous()\n",
        "            return features, weights\n",
        "        else:\n",
        "            return features\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 decoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None,\n",
        "                 return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           memory,\n",
        "                           tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos,\n",
        "                           query_pos=query_pos)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model,\n",
        "                                                    nhead,\n",
        "                                                    dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     tgt,\n",
        "                     memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    tgt,\n",
        "                    memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt2,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask,\n",
        "                                    memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask,\n",
        "                                 pos, query_pos)\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args.hidden_dim,\n",
        "        dropout=args.dropout,\n",
        "        nhead=args.nheads,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        num_encoder_layers=args.enc_layers,\n",
        "        num_decoder_layers=args.dec_layers,\n",
        "        normalize_before=args.pre_norm,\n",
        "        return_intermediate_dec=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"leaky relu\":\n",
        "        return F.leaky_relu\n",
        "    if activation == \"selu\":\n",
        "        return F.selu\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(\n",
        "        F\"activation should be relu, gelu, glu, leaky relu or selu, not {activation}.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLaynkwn9UPM"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "vsyy9B33klD8"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50, resnet34, resnet18, ResNet50_Weights, ResNet18_Weights, ResNet34_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "rlV1_wDH9UPM"
      },
      "outputs": [],
      "source": [
        "# For ResNet18, use layers up to layer3 so that the output has OS16.\n",
        "def ResNet18_OS16(multi_scale=False):\n",
        "    resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "    # Original resnet18 children:\n",
        "    # [conv1, bn1, relu, maxpool, layer1, layer2, layer3, layer4, avgpool, fc]\n",
        "    # Removing layer4, avgpool and fc gives a feature map of size roughly (B, 256, H/16, W/16)\n",
        "    features = nn.Sequential(*list(resnet.children())[:-3])\n",
        "    return features\n",
        "\n",
        "# For ResNet50, adjust layer4 to preserve OS16.\n",
        "def ResNet50_OS16(multi_scale=False):\n",
        "    resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "    # The children of resnet50 are:\n",
        "    # [conv1, bn1, relu, maxpool, layer1, layer2, layer3, layer4, avgpool, fc]\n",
        "    # To achieve OS16 instead of OS32, modify layer4 so that it does not downsample:\n",
        "    #  - Change the stride in the first Bottleneck of layer4 from 2 to 1.\n",
        "    resnet.layer4[0].conv2.stride = (1, 1)\n",
        "    if resnet.layer4[0].downsample:  # adjust the downsampling layer as well\n",
        "        resnet.layer4[0].downsample[0].stride = (1, 1)\n",
        "    # Additionally, increase dilation in layer4 so that its receptive field remains large.\n",
        "    for m in resnet.layer4.modules():\n",
        "        if isinstance(m, nn.Conv2d) and m.kernel_size == (3, 3):\n",
        "            m.dilation = (2, 2)\n",
        "            m.padding = (2, 2)\n",
        "    # Now remove the avgpool and fc layers.\n",
        "    features = nn.Sequential(*list(resnet.children())[:-2])\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFIKfzTH9UPN"
      },
      "source": [
        "## Atrous Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GMt0ON6a9UPO"
      },
      "outputs": [],
      "source": [
        "# camera-ready\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, num_classes, head = True):\n",
        "        super(ASPP, self).__init__()\n",
        "\n",
        "        self.conv_1x1_1 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.conv_1x1_2 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        if head:\n",
        "            self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "        self.head = head\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # (feature_map has shape (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16. If self.resnet instead is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8))\n",
        "\n",
        "        feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "        feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "        out_img = F.interpolate(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        if self.head:\n",
        "            out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "\n",
        "        return out\n",
        "\n",
        "class ASPP_Bottleneck(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ASPP_Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv_1x1_1 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_1 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_2 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_3 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.conv_1x1_2 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # (feature_map has shape (batch_size, 4*512, h/16, w/16))\n",
        "\n",
        "        feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "        feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "        out_img = F.interpolate(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCLcdf0B9UPN"
      },
      "source": [
        "## DeepLabV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xVGsTLZxklEK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import sys\n",
        "# sys.path.insert(0, '../')\n",
        "\n",
        "# from Ours.resnet import ResNet18_OS16, ResNet34_OS16, ResNet50_OS16, ResNet101_OS16, ResNet152_OS16, ResNet18_OS8, ResNet34_OS8\n",
        "# from Ours.ASPP import ASPP, ASPP_Bottleneck\n",
        "\n",
        "# class DeepLabV3(nn.Module): | \"DeepLabV3\" is called as \"base\" in the BAT class\n",
        "# class base(nn.Module):\n",
        "#     def __init__(self, num_classes, num_layers):\n",
        "#         super(base, self).__init__()\n",
        "\n",
        "#         self.num_classes = num_classes\n",
        "#         layers = num_layers\n",
        "#         # NOTE! specify the type of ResNet here\n",
        "#         # NOTE! if you use ResNet50-152, set self.aspp = ASPP_Bottleneck(num_classes=self.num_classes) instead\n",
        "#         if layers == 18:\n",
        "#             self.resnet = ResNet18_OS16()\n",
        "#             self.aspp = ASPP(num_classes=self.num_classes)\n",
        "#         elif layers == 50:\n",
        "#             self.resnet = ResNet50_OS16()\n",
        "#             self.aspp = ASPP_Bottleneck(num_classes=self.num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # (x has shape (batch_size, 3, h, w))\n",
        "#         h = x.size()[2]\n",
        "#         w = x.size()[3]\n",
        "#         feature_map = self.resnet(x)\n",
        "\n",
        "#         # (shape: (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16.\n",
        "#         # If self.resnet is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8).\n",
        "#         # If self.resnet is ResNet50-152, it will be (batch_size, 4*512, h/16, w/16))\n",
        "#         output = self.aspp(\n",
        "#             feature_map)  # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "#         output = F.upsample(\n",
        "#             output, size=(h, w),\n",
        "#             mode=\"bilinear\")  # (shape: (batch_size, num_classes, h, w))\n",
        "#         return output\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# The \"base\" model acts like a DeepLabV3 backbone.\n",
        "# It instantiates a segmentation network using a ResNet backbone (with OS16)\n",
        "# plus an ASPP module. (Ensure that ASPP and ASPP_Bottleneck are defined/imported.)\n",
        "# -------------------------------------------------------------------\n",
        "class base(nn.Module):\n",
        "    def __init__(self, num_classes, num_layers):\n",
        "        super(base, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        layers = num_layers\n",
        "\n",
        "        if layers == 18:\n",
        "            self.resnet = ResNet18_OS16()\n",
        "            self.aspp = ASPP(num_classes=self.num_classes)  # ASPP for BasicBlock variant\n",
        "        elif layers == 50:\n",
        "            self.resnet = ResNet50_OS16()\n",
        "            self.aspp = ASPP_Bottleneck(num_classes=self.num_classes)  # ASPP_Bottleneck for Bottleneck variant\n",
        "        else:\n",
        "            raise ValueError(\"Only 18 and 50 are supported in this example.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is of shape (batch_size, 3, h, w)\n",
        "        h, w = x.size(2), x.size(3)\n",
        "        feature_map = self.resnet(x)  # Expected shape: [B, C, h/16, w/16]\n",
        "        # Process through the ASPP module:\n",
        "        output = self.aspp(feature_map)  # Expected shape: (batch_size, num_classes, h/16, w/16)\n",
        "        # Upsample the output to original image resolution.\n",
        "        output = F.interpolate(output, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZWZ7jTw9UPO"
      },
      "source": [
        "## Base Transformer (BAT class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "N6jR8NwS9UPO"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "\n",
        "# root_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..')\n",
        "# sys.path.insert(0, os.path.join(root_path))\n",
        "# sys.path.insert(0, os.path.join(root_path, 'lib'))\n",
        "# sys.path.insert(0, os.path.join(root_path, 'lib/Cell_DETR_master'))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from Ours.base import DeepLabV3 as base\n",
        "\n",
        "# from src.BAT_Modules import BoundaryCrossAttention, CrossAttention\n",
        "# from src.BAT_Modules import MultiHeadAttention as Attention_head\n",
        "# from src.transformer import BoundaryAwareTransformer, Transformer\n",
        "\n",
        "\n",
        "class BAT(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_classes,\n",
        "            num_layers,\n",
        "            point_pred,\n",
        "            decoder=False,\n",
        "            transformer_type_index=0,\n",
        "            hidden_features=128,  # 256\n",
        "            number_of_query_positions=1,\n",
        "            segmentation_attention_heads=8):\n",
        "\n",
        "        super(BAT, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.point_pred = point_pred\n",
        "        self.transformer_type = \"BoundaryAwareTransformer\" if transformer_type_index == 0 else \"Transformer\"\n",
        "        self.use_decoder = decoder\n",
        "\n",
        "        self.deeplab = base(num_classes, num_layers)\n",
        "\n",
        "        in_channels = 2048 if num_layers == 50 else 512\n",
        "\n",
        "        self.convolution_mapping = nn.Conv2d(in_channels=in_channels,\n",
        "                                             out_channels=hidden_features,\n",
        "                                             kernel_size=(1, 1),\n",
        "                                             stride=(1, 1),\n",
        "                                             padding=(0, 0),\n",
        "                                             bias=True)\n",
        "\n",
        "        self.query_positions = nn.Parameter(data=torch.randn(\n",
        "            number_of_query_positions, hidden_features, dtype=torch.float),\n",
        "                                            requires_grad=True)\n",
        "\n",
        "        self.row_embedding = nn.Parameter(data=torch.randn(100,\n",
        "                                                           hidden_features //\n",
        "                                                           2,\n",
        "                                                           dtype=torch.float),\n",
        "                                          requires_grad=True)\n",
        "        self.column_embedding = nn.Parameter(data=torch.randn(\n",
        "            100, hidden_features // 2, dtype=torch.float),\n",
        "                                             requires_grad=True)\n",
        "\n",
        "        self.transformer = [\n",
        "            Transformer(d_model=hidden_features),\n",
        "            BoundaryAwareTransformer(d_model=hidden_features)\n",
        "        ][point_pred]\n",
        "\n",
        "        if self.use_decoder:\n",
        "            self.BCA = BoundaryCrossAttention(hidden_features, 8)\n",
        "\n",
        "        self.trans_out_conv = nn.Conv2d(in_channels=hidden_features,\n",
        "                                        out_channels=in_channels,\n",
        "                                        kernel_size=(1, 1),\n",
        "                                        stride=(1, 1),\n",
        "                                        padding=(0, 0),\n",
        "                                        bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x.size()[2]\n",
        "        w = x.size()[3]\n",
        "        feature_map = self.deeplab.resnet(x)\n",
        "\n",
        "        features = self.convolution_mapping(feature_map)\n",
        "        height, width = features.shape[2:]\n",
        "        batch_size = features.shape[0]\n",
        "        positional_embeddings = torch.cat([\n",
        "            self.column_embedding[:height].unsqueeze(dim=0).repeat(\n",
        "                height, 1, 1),\n",
        "            self.row_embedding[:width].unsqueeze(dim=1).repeat(1, width, 1)\n",
        "        ],\n",
        "                                          dim=-1).permute(\n",
        "                                              2, 0, 1).unsqueeze(0).repeat(\n",
        "                                                  batch_size, 1, 1, 1)\n",
        "\n",
        "        if self.transformer_type == 'BoundaryAwareTransformer':\n",
        "            latent_tensor, features_encoded, point_maps = self.transformer(\n",
        "                features, None, self.query_positions, positional_embeddings)\n",
        "        else:\n",
        "            latent_tensor, features_encoded = self.transformer(\n",
        "                features, None, self.query_positions, positional_embeddings)\n",
        "            point_maps = []\n",
        "\n",
        "        latent_tensor = latent_tensor.permute(2, 0, 1)\n",
        "        # shape:(bs, 1 , 128)\n",
        "\n",
        "        if self.use_decoder:\n",
        "            features_encoded, point_dec = self.BCA(features_encoded,\n",
        "                                                   latent_tensor)\n",
        "            point_maps.append(point_dec)\n",
        "\n",
        "        trans_feature_maps = self.trans_out_conv(\n",
        "            features_encoded.contiguous())  #.contiguous()\n",
        "\n",
        "        trans_feature_maps = trans_feature_maps + feature_map\n",
        "\n",
        "        output = self.deeplab.aspp(\n",
        "            trans_feature_maps\n",
        "        )  # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "        output = F.interpolate(\n",
        "            output, size=(h, w),\n",
        "            mode=\"bilinear\")  # (shape: (batch_size, num_classes, h, w))\n",
        "\n",
        "        if self.point_pred == 1:\n",
        "            return output, point_maps\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIIdw0tLK2S4"
      },
      "source": [
        "## 2. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svv3B6UJ9UPP",
        "outputId": "18a41a0f-9fcd-468b-e28b-719fdffb2de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:02<00:00, 43.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = BAT(1, parse_config.net_layer, parse_config.point_pred,\n",
        "                    parse_config.ppl).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ag6qGuR9UPP"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "20fGhStqikfl"
      },
      "outputs": [],
      "source": [
        "# Based on: https://github.com/facebookresearch/detr/blob/master/models/transformer.py\n",
        "import copy\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "# from .BAT_Modules import BoundaryWiseAttentionGate2D, BoundaryWiseAttentionGate1D, BoundaryWiseAttentionGateAtrous2D, BoundaryWiseAttentionGateAtrous1D\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers,\n",
        "                                          encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        #         print(\"Trans Encoder\",memory.shape)\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers=6,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = BoundaryAwareTransformerEncoderLayer(\n",
        "            d_model, nhead, BAG_type, Atrous, dim_feedforward, dropout,\n",
        "            activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = BoundaryAwareTransformerEncoder(point_pred_layers,\n",
        "                                                       encoder_layer,\n",
        "                                                       num_encoder_layers,\n",
        "                                                       encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory, weights = self.encoder(src,\n",
        "                                       src_key_padding_mask=mask,\n",
        "                                       pos=pos_embed,\n",
        "                                       height=h,\n",
        "                                       width=w)\n",
        "\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h,\n",
        "                                                                w), weights\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask,\n",
        "                           pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers,\n",
        "                 encoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None):\n",
        "        super().__init__()\n",
        "        self.point_pred_layers = point_pred_layers\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        output = src\n",
        "        weights = []\n",
        "\n",
        "        for layer_i, layer in enumerate(self.layers):\n",
        "            if layer_i > self.num_layers - self.point_pred_layers - 1:\n",
        "                output, weight = layer(\n",
        "                    True,\n",
        "                    output,\n",
        "                    src_mask=mask,\n",
        "                    src_key_padding_mask=src_key_padding_mask,\n",
        "                    pos=pos,\n",
        "                    height=height,\n",
        "                    width=width)\n",
        "                weights.append(weight)\n",
        "            else:\n",
        "                output = layer(False,\n",
        "                               output,\n",
        "                               src_mask=mask,\n",
        "                               src_key_padding_mask=src_key_padding_mask,\n",
        "                               pos=pos,\n",
        "                               height=height,\n",
        "                               width=width)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src2,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoderLayer(TransformerEncoderLayer):\n",
        "    \"    Add Boundary-wise Attention Gate to Transformer's Encoder\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=True,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation,\n",
        "                         normalize_before)\n",
        "        if BAG_type == '1D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous1D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate1D(d_model)\n",
        "        elif BAG_type == '2D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous2D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate2D(d_model)\n",
        "        self.BAG_type = BAG_type\n",
        "\n",
        "    def forward(self,\n",
        "                use_bag,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        if self.normalize_before:\n",
        "            features = self.forward_pre(src, src_mask, src_key_padding_mask,\n",
        "                                        pos)\n",
        "            if use_bag:\n",
        "                b, c = features.shape[1:]\n",
        "                if self.BAG_type == '1D':\n",
        "                    features = features.permute(1, 2, 0)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.permute(2, 0, 1).contiguous()\n",
        "                    weights = weights.view(b, 1, height, width)\n",
        "                elif self.BAG_type == '2D':\n",
        "                    features = features.permute(1, 2,\n",
        "                                                0).view(b, c, height, width)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.flatten(2).permute(2, 0,\n",
        "                                                           1).contiguous()\n",
        "                return features, weights\n",
        "            else:\n",
        "                return features\n",
        "        features = self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "        if use_bag:\n",
        "            b, c = features.shape[1:]\n",
        "            if self.BAG_type == '1D':\n",
        "                features = features.permute(1, 2, 0)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.permute(2, 0, 1).contiguous()\n",
        "                weights = weights.view(b, 1, height, width)\n",
        "            elif self.BAG_type == '2D':\n",
        "                features = features.permute(1, 2, 0).view(b, c, height, width)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.flatten(2).permute(2, 0, 1).contiguous()\n",
        "            return features, weights\n",
        "        else:\n",
        "            return features\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 decoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None,\n",
        "                 return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           memory,\n",
        "                           tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos,\n",
        "                           query_pos=query_pos)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model,\n",
        "                                                    nhead,\n",
        "                                                    dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     tgt,\n",
        "                     memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    tgt,\n",
        "                    memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt2,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask,\n",
        "                                    memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask,\n",
        "                                 pos, query_pos)\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args.hidden_dim,\n",
        "        dropout=args.dropout,\n",
        "        nhead=args.nheads,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        num_encoder_layers=args.enc_layers,\n",
        "        num_decoder_layers=args.dec_layers,\n",
        "        normalize_before=args.pre_norm,\n",
        "        return_intermediate_dec=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"leaky relu\":\n",
        "        return F.leaky_relu\n",
        "    if activation == \"selu\":\n",
        "        return F.selu\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(\n",
        "        F\"activation should be relu, gelu, glu, leaky relu or selu, not {activation}.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzze2VhIK2S5"
      },
      "source": [
        "### Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TbTO94ePK2S5"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.0001):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.mode = mode\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        self.BEST_MODEL_PATH = \"\"\n",
        "        if self.mode == \"min\":\n",
        "            self.val_score = np.inf\n",
        "        else:\n",
        "            self.val_score = -np.inf\n",
        "\n",
        "    def __call__(self, epoch, epoch_score, model, optimizer, loss, model_path):\n",
        "        if self.mode == \"min\":\n",
        "            score = -1.0 * epoch_score\n",
        "        else:\n",
        "            score = np.copy(epoch_score)\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch, epoch_score, model, optimizer, loss, model_path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(\n",
        "                \"EarlyStopping counter: {} out of {}\".format(\n",
        "                    self.counter, self.patience\n",
        "                )\n",
        "            )\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch, epoch_score, model, optimizer, loss, model_path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch, epoch_score, model, optimizer, loss, model_path):\n",
        "        model_path = Path(model_path)\n",
        "        parent = model_path.parent\n",
        "        os.makedirs(parent, exist_ok=True)\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
        "            print(\n",
        "                \"Validation score improved ({} --> {}). Model saved at {}!\".format(\n",
        "                    self.val_score, epoch_score, model_path\n",
        "                )\n",
        "            )\n",
        "            torch.save({\"Epoch\": epoch, \"Train Loss\": loss, \"Validation Dice Score\": self.val_score, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, model_path)\n",
        "            self.BEST_MODEL_PATH = model_path\n",
        "            print(f\"Checkpoint saved on epoch - {epoch} with dice score - {epoch_score}\")\n",
        "        self.val_score = epoch_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "JwQu_LDS9UPS"
      },
      "outputs": [],
      "source": [
        "criterion = CRITERION\n",
        "es = EarlyStopping(patience=EARLY_STOPPING_PATIENCE, mode='max')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOlM9jTEK2S5"
      },
      "source": [
        "### Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_amog1GHK2S5"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "xbn3pKTaK2S5"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(train_loader, model, optimizer, loss_fn, epoch, accumulation_steps=int(EFFECTIVE_BATCH_SIZE/BATCH_SIZE), device='cuda'):\n",
        "    losses = AverageMeter()\n",
        "    iteration = 0\n",
        "    # Lists to store batch-to-batch progress details within the epoch while training\n",
        "    batch_count_train = []\n",
        "    batch_train_loss = []\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    if accumulation_steps > 1:\n",
        "      optimizer.zero_grad()\n",
        "    tk0 = tqdm(train_loader, total=len(train_loader))\n",
        "    for b_idx, data in enumerate(tk0):\n",
        "      print(data['image'].shape) # print(data['image'].shape) -> torch.Size([8, 3, 512, 512])\n",
        "      print(data['mask'].shape) # print(data['mask'].shape) -> torch.Size([8, 1, 512, 512])\n",
        "      if (b_idx + 1) % accumulation_steps == 0:\n",
        "        batch_count_train.append(b_idx)\n",
        "\n",
        "      # moves image tensor and mask tensor to gpu\n",
        "      for key, value in data.items():\n",
        "        data[key] = value.to(\"cuda\")\n",
        "      point = (data['point'] > 0).cuda().float()\n",
        "\n",
        "      if parse_config.net_layer == 18:\n",
        "          point_c4 = nn.functional.max_pool2d(point,\n",
        "                                              kernel_size=(16, 16),\n",
        "                                              stride=(16, 16))\n",
        "          point = nn.functional.max_pool2d(point,\n",
        "                                          kernel_size=(8, 8),\n",
        "                                          stride=(8, 8))\n",
        "      else:\n",
        "          point_c5 = nn.functional.max_pool2d(point,\n",
        "                                              kernel_size=(32, 32),\n",
        "                                              stride=(32, 32))\n",
        "\n",
        "          point_c4 = nn.functional.max_pool2d(point,\n",
        "                                              kernel_size=(16, 16),\n",
        "                                              stride=(16, 16))\n",
        "\n",
        "      if accumulation_steps == 1 and b_idx == 0:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "      if parse_config.point_pred == 1:\n",
        "            output, point_maps_pre = model(data['image'])\n",
        "            output = torch.sigmoid(output)\n",
        "\n",
        "            print(\"point_maps_pre[-1] shape:{}, point_c4 shape:{}\".format(point_maps_pre[-1].shape,point_c4.shape))\n",
        "            assert (output.shape == data['mask'].float().shape)\n",
        "            loss_dc = dice_loss(output, data['mask'].float())\n",
        "            print(point_maps_pre[-1].shape, point_c4.shape)\n",
        "            assert (point_maps_pre[-1].shape == point_c4.shape)\n",
        "\n",
        "            point_loss = 0.\n",
        "            for i in range(len(point_maps_pre)):\n",
        "                point_loss += criterion(point_maps_pre[i], point_c4)\n",
        "            point_loss = point_loss / len(point_maps_pre)\n",
        "\n",
        "            loss = loss_dc + point_loss  # point_loss weight: 3\n",
        "\n",
        "            with torch.set_grad_enabled(True):\n",
        "              loss.backward()\n",
        "              # if (b_idx + 1) % accumulation_steps == 0:\n",
        "              #   if GRADIENT_CLIPPING:\n",
        "              #     clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING_THRESHOLD)\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "            iteration = iteration + 1\n",
        "\n",
        "            if (b_idx + 1) % 10 == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    b_idx * len(data['image']), b_idx, len(train_dataloader.dataset),\n",
        "                    100. * b_idx / len(train_dataloader), loss.item()))\n",
        "      print(\"Iteration numbers: \", iteration)\n",
        "\n",
        "      ###############################################################################################\n",
        "      # out  = model(data['image']) # out.shape = torch.Size([8, 1, 512, 512])\n",
        "      # loss = loss_fn(out, data['mask'].float()) # mask.shape = torch.Size([8, 1, 512, 512])\n",
        "      # with torch.set_grad_enabled(True):\n",
        "      #   loss.backward()\n",
        "      #   if (b_idx + 1) % accumulation_steps == 0:\n",
        "      #     if GRADIENT_CLIPPING:\n",
        "      #       clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING_THRESHOLD)\n",
        "      #     optimizer.step()\n",
        "      #     optimizer.zero_grad()\n",
        "      losses.update(loss.item(), train_loader.batch_size)\n",
        "      if (b_idx + 1) % accumulation_steps == 0:\n",
        "        batch_train_loss.append(loss.item())\n",
        "      tk0.set_postfix(loss=losses.avg, learning_rate=optimizer.param_groups[0]['lr'])\n",
        "    return losses.avg, batch_count_train, batch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "FKL6S336-unk",
        "outputId": "e08ca118-a997-4aba-ae74-969b8604134b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.focal_loss(inputs: torch.Tensor, targets: torch.Tensor, alpha: float = 0.6, gamma: float = 2, reduction: str = 'mean') -> torch.Tensor>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>focal_loss</b><br/>def focal_loss(inputs: torch.Tensor, targets: torch.Tensor, alpha: float=0.6, gamma: float=2, reduction: str=&#x27;mean&#x27;) -&gt; torch.Tensor</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-16-defc6de845ef&gt;</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K136b3eqK2S5"
      },
      "source": [
        "### Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RY2NxC-unk"
      },
      "source": [
        "#### Mask Binarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "V4my02Uf-unk"
      },
      "outputs": [],
      "source": [
        "class MaskBinarization():\n",
        "    def __init__(self):\n",
        "        self.thresholds = 0.5\n",
        "    def transform(self, predicted):\n",
        "        yield predicted > self.thresholds\n",
        "\n",
        "class SimpleMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, score_thresholds):\n",
        "        super().__init__()\n",
        "        self.thresholds = score_thresholds\n",
        "    def transform(self, predicted):\n",
        "        for thr in self.thresholds:\n",
        "            yield predicted > thr\n",
        "\n",
        "class DupletMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, duplets, with_channels=True):\n",
        "        super().__init__()\n",
        "        self.thresholds = duplets\n",
        "        self.dims = (2,3) if with_channels else (1,2)\n",
        "    def transform(self, predicted):\n",
        "        for score_threshold, area_threshold in self.thresholds:\n",
        "            mask = predicted > score_threshold\n",
        "            mask[mask.sum(dim=self.dims) < area_threshold] = 0\n",
        "            yield mask\n",
        "\n",
        "class TripletMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, triplets, with_channels=True):\n",
        "        super().__init__()\n",
        "        self.thresholds = triplets\n",
        "        self.dims = (2,3) if with_channels else (1,2) # dims should be HxW, basically it should ignore batch_size and no_of_channels in the general format of BxCxHxW\n",
        "    def transform(self, predicted):\n",
        "        for top_score_threshold, area_threshold, bottom_score_threshold in self.thresholds:\n",
        "            clf_mask = (predicted > top_score_threshold).float()\n",
        "            pred_mask = (predicted > bottom_score_threshold).float()\n",
        "            pred_mask[clf_mask.sum(dim=self.dims) < area_threshold] = 0\n",
        "            yield pred_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joGnfZgw-unl"
      },
      "source": [
        "#### Metric used in validation and evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "zlWOcwQdK2S5"
      },
      "outputs": [],
      "source": [
        "def metric(probability, truth):\n",
        "    if probability.shape[0] == truth.shape[0]: # checking for batch size mismatches in the code for image & mask\n",
        "        batch_size = probability.shape[0]\n",
        "    with torch.no_grad():\n",
        "        probability = probability.view(batch_size, -1) # probability's size is [8, 1*512*512]\n",
        "        truth = truth.view(batch_size, -1)             # truth's size is [8, 1*512*512]\n",
        "        assert(probability.shape == truth.shape)\n",
        "\n",
        "        p = probability.float() # prob_preds already comes in binarized.\n",
        "        t = (truth > 0.5).float()\n",
        "\n",
        "        t_sum = t.sum(-1) # t_sum size is 8 # Each value in the vector represents the sum of all pixels in one mask\n",
        "        p_sum = p.sum(-1) # p_sum size is 8 # Each value in the vector represents the sum of all elements in one pred_probs\n",
        "        neg_index = torch.nonzero(t_sum == 0) # indices of masks which are negative.\n",
        "        pos_index = torch.nonzero(t_sum >= 1) # indices of masks which are positive.\n",
        "\n",
        "        dice_neg = (p_sum == 0).float() # tensor of size 8\n",
        "        \"\"\"\n",
        "        if t_sum = torch.tensor([0.0, 1000.0, 0.0, 600.0, 720.0, 420.0, 0.0, 0.0]), then dice_neg = tensor([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0])\n",
        "        \"\"\"\n",
        "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1)) # tensor of size 8\n",
        "\n",
        "        dice_neg = dice_neg[neg_index] # selects elements of dice_neg acc to the indices in neg_index, it can have more than one element.\n",
        "        dice_pos = dice_pos[pos_index] # similar to the above code line.\n",
        "        dice = torch.cat([dice_pos, dice_neg])\n",
        "\n",
        "        num_neg = len(neg_index) # no. of negative masks in a batch\n",
        "        num_pos = len(pos_index) # no. of positive masks in a batch\n",
        "\n",
        "    return dice.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "u3ovO3f9XHQt"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_best_thresholds(epoch, b_idx, best_metric, best_threshold, filepath=Path(save_best_thresholds_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch,\n",
        "        'batch_number': b_idx,\n",
        "        'best_metric': best_metric,\n",
        "        'best_threshold': best_threshold\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "7opE5TuMRCvg"
      },
      "outputs": [],
      "source": [
        "epoch_count_thr = []\n",
        "batch_indices = []\n",
        "best_metrics_list = []\n",
        "best_thresholds_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ww6ftfKmK2S5"
      },
      "outputs": [],
      "source": [
        "def evaluate(valid_loader, model, epoch, device=DEVICE, metric=metric, loss_fn=criterion):\n",
        "    losses = AverageMeter()\n",
        "    combolosses = AverageMeter()\n",
        "    metrics = defaultdict(float)\n",
        "    # Lists to store batch-to-batch progress details within the epoch while training\n",
        "    batch_count_val = []\n",
        "    batch_val_loss_values = []\n",
        "################################# top\n",
        "    dice_value = 0\n",
        "    iou_value = 0\n",
        "    dice_average = 0\n",
        "    iou_average = 0\n",
        "    numm = 0\n",
        "################################# bottom\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    tk0 = tqdm(valid_loader, total=len(valid_loader))\n",
        "    with torch.inference_mode():\n",
        "        for b_idx, data in enumerate(tk0):\n",
        "            batch_count_val.append(b_idx)\n",
        "################################################################################ top\n",
        "            data = data['image'].cuda().float()\n",
        "            label = data['label'].cuda().float()\n",
        "            point = (data['point'] > 0).cuda().float()\n",
        "            point_c5 = nn.functional.max_pool2d(point,\n",
        "                                                kernel_size=(32, 32),\n",
        "                                                stride=(32, 32))\n",
        "            point_c4 = nn.functional.max_pool2d(point,\n",
        "                                                kernel_size=(16, 16),\n",
        "                                                stride=(16, 16))\n",
        "################################################################################### bottom\n",
        "\n",
        "#################################################################################### top\n",
        "            if parse_config.arch == 'transfuse':\n",
        "                _, _, output = model(data)\n",
        "                loss_fuse = structure_loss(output, label)\n",
        "            elif parse_config.point_pred == 0:\n",
        "                output = model(data)\n",
        "            elif parse_config.cross == 1 and parse_config.point_pred == 1:\n",
        "                output, point_maps_pre_1, point_maps_pre_2 = model(data)\n",
        "                point_loss_c4 = 0.\n",
        "                for i in range(len(point_maps_pre_1) - 1):\n",
        "                    point_loss_c4 += criterion(point_maps_pre_1[i], point_c4)\n",
        "                point_loss_c4 = 1.0 / len(point_maps_pre_1) * (\n",
        "                    point_loss_c4 + criterion(point_maps_pre_1[-1], point_c4))\n",
        "                point_loss_c5 = 0.\n",
        "                for i in range(len(point_maps_pre_2) - 1):\n",
        "                    point_loss_c5 += criterion(point_maps_pre_2[i], point_c5)\n",
        "                point_loss_c5 = 1.0 / len(point_maps_pre_2) * (\n",
        "                    point_loss_c5 + criterion(point_maps_pre_2[-1], point_c5))\n",
        "                point_loss = 0.5 * (point_loss_c4 + point_loss_c5)\n",
        "            elif parse_config.point_pred == 1:\n",
        "                output, point_maps_pre = model(data)\n",
        "                point_loss = 0.\n",
        "                for i in range(len(point_maps_pre) - 1):\n",
        "                    point_loss += criterion(point_maps_pre[i], point_c4)\n",
        "                point_loss = 1.0 / len(point_maps_pre) * (\n",
        "                    point_loss + criterion(point_maps_pre[-1], point_c4))\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "\n",
        "            loss_dc = dice_loss(output, label)\n",
        "\n",
        "            if parse_config.arch == 'transfuse':\n",
        "                loss = loss_fuse\n",
        "            elif parse_config.arch == 'transunet':\n",
        "                loss = 0.5 * loss_dc + 0.5 * ce_loss(output, label)\n",
        "            elif parse_config.point_pred == 0:\n",
        "                loss = loss_dc\n",
        "            elif parse_config.cross == 1 and parse_config.point_pred == 1:\n",
        "                loss = loss_dc + point_loss\n",
        "            elif parse_config.point_pred == 1:\n",
        "                loss = loss_dc + 3 * point_loss\n",
        "\n",
        "            output = output.cpu().numpy() > 0.5\n",
        "\n",
        "            label = label.cpu().numpy()\n",
        "            assert (output.shape == label.shape)\n",
        "            dice_ave = metric(output, label)\n",
        "            # iou_ave = jc(output, label)\n",
        "            dice_value += dice_ave\n",
        "            # iou_value += iou_ave\n",
        "            numm += 1\n",
        "\n",
        "    dice_average = dice_value / numm\n",
        "    # iou_average = iou_value / numm\n",
        "    print(\"Average dice value of evaluation dataset = \", dice_average)\n",
        "    # print(\"Average iou value of evaluation dataset = \", iou_average)\n",
        "#################################################################################### bottom\n",
        "\n",
        "\n",
        "    tk0.set_description('score: {:.5f}'.format(dice_ave))\n",
        "\n",
        "    epoch_count_thr.append(epoch)\n",
        "    batch_indices.append(b_idx)\n",
        "    best_metrics_list.append(dice_ave)\n",
        "\n",
        "    # if .item() is used then .cpu() is NOT required. .item() will itself return a float value.\n",
        "    losses.update(dice_ave, valid_loader.batch_size)\n",
        "    combolosses.update(loss.item(), valid_loader.batch_size)\n",
        "    batch_val_loss_values.append(loss.item())\n",
        "    tk0.set_postfix(dice_score=losses.avg, val_loss=combolosses.avg)\n",
        "    return losses.avg, batch_count_val, batch_val_loss_values, combolosses.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE7TO5J9XHQt"
      },
      "source": [
        "### Optimizer & Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "XGkO2pA_9UPW"
      },
      "outputs": [],
      "source": [
        "if PRETRAINED:\n",
        "  checkpoint = torch.load(TRAINING_MODEL_PATH)\n",
        "  model.to(DEVICE)\n",
        "  model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay= L2_WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
        "elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Fn0XAwlyK2S6"
      },
      "outputs": [],
      "source": [
        "# if PRETRAINED:\n",
        "#   checkpoint = torch.load(TRAINING_MODEL_PATH)\n",
        "#   model.to(DEVICE)\n",
        "#   model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "#   optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay= L2_WEIGHT_DECAY)\n",
        "#   if OPTIMIZER_LOAD:\n",
        "#     optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "#   # Manually set the new learning rate for this stage of training as loading optimizer's state dict will\n",
        "#   # load parameters that was there while saving the previous checkpoint but loading the optimizer's state dict is\n",
        "#   # crucial\n",
        "#   for param_group in optimizer.param_groups:\n",
        "#     param_group['lr'] = LEARNING_RATE\n",
        "\n",
        "#   if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=SCHEDULER_PARAMS[\"factor\"], patience=SCHEDULER_PARAMS[\"patience\"], threshold=SCHEDULER_PARAMS[\"threshold\"], min_lr=SCHEDULER_PARAMS[\"min_lr\"])\n",
        "#   elif SCHEDULER == \"CosineAnnealingWarmRestarts\":\n",
        "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=SCHEDULER_PARAMS[\"T_0\"], T_mult=SCHEDULER_PARAMS[\"T_mult\"], eta_min=SCHEDULER_PARAMS[\"eta_min\"])\n",
        "#   elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=SCHEDULER_PARAMS[\"T_max\"], eta_min=SCHEDULER_PARAMS[\"eta_min\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "gK69cmOlRCvg"
      },
      "outputs": [],
      "source": [
        "# if not PRETRAINED:\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay= L2_WEIGHT_DECAY)\n",
        "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=SCHEDULER_PARAMS[\"factor\"], patience=SCHEDULER_PARAMS[\"patience\"], threshold=SCHEDULER_PARAMS[\"threshold\"], min_lr=SCHEDULER_PARAMS[\"min_lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqlFcFDSRCvh",
        "outputId": "5dcd9fa1-7473-40f7-b7e3-cbcc7f2117a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New learning rate: 0.0001\n"
          ]
        }
      ],
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "    print(f\"New learning rate: {param_group['lr']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ig7E3afMOi-6"
      },
      "outputs": [],
      "source": [
        "# initial_lr_scheduler = scheduler.base_lrs[0]  # Assuming a single learning rate for all parameters\n",
        "# print(\"Initial learning rate of scheduler:\", initial_lr_scheduler)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOkP2HYpvsqr"
      },
      "source": [
        "### GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ypne222KBjPe"
      },
      "outputs": [],
      "source": [
        "# # memory footprint support libraries/code\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "# def printm():\n",
        "#  process = psutil.Process(os.getpid())\n",
        "#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "# printm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtHHjC45vsqr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcACWXgAXHQu"
      },
      "source": [
        "#### Saving Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "AzIGIAe2FmJQ"
      },
      "outputs": [],
      "source": [
        "def store_batch_training_details(epoch, batch_count_train, batch_train_loss):\n",
        "  # Directory where you want to save the CSV file\n",
        "  directory = store_batch_training_details_path\n",
        "\n",
        "  # Ensure the directory exists\n",
        "  os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "  # File path\n",
        "  file_path = os.path.join(directory, name_of_batch_training_details_csv + f\"{epoch}.csv\")\n",
        "\n",
        "  # Write to CSV file\n",
        "  with open(file_path, \"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      # Write header\n",
        "      writer.writerow([\"Batch Count Train\", \"Batch Train Loss\"])\n",
        "      # Write data rows\n",
        "      for batch_count, train_loss in zip(batch_count_train, batch_train_loss):\n",
        "          writer.writerow([batch_count, train_loss])\n",
        "\n",
        "  print(f\"CSV file for epoch-{epoch}has been created at: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "f5-0shprvsqr"
      },
      "outputs": [],
      "source": [
        "def store_batch_validation_details(epoch, batch_count_val, batch_val_score_values):\n",
        "  # Directory where you want to save the CSV file\n",
        "  directory = store_batch_validation_details_path\n",
        "\n",
        "  # Ensure the directory exists\n",
        "  os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "  # File path\n",
        "  file_path = os.path.join(directory, name_of_batch_validation_details_csv + f\"{epoch}.csv\")\n",
        "\n",
        "  # Write to CSV file\n",
        "  with open(file_path, \"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      # Write header\n",
        "      writer.writerow([\"Batch Count Validation\", \"Batch Validation Loss\"])\n",
        "      # Write data rows\n",
        "      for batch_count, batch_val_score in zip(batch_count_val, batch_val_score_values):\n",
        "          writer.writerow([batch_count, batch_val_score])\n",
        "\n",
        "  print(f\"CSV file for validation, epoch-{epoch}has been created at: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ijkQ3VlU-unm"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_progress(epoch_count, loss_values, val_loss_values, filepath= Path(save_progress_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch_count,\n",
        "        'train_loss': loss_values,\n",
        "        'val_loss': [None] * (len(epoch_count) - len(val_loss_values)) + val_loss_values\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "PTogqmQgWkZC"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_dice_score(epoch_count, val_score_values, filepath= Path(save_dice_score_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch_count,\n",
        "        'val_dice_scores': [None] * (len(epoch_count) - len(val_score_values)) + val_score_values\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ntFd7ioHXHQv"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_losses(bce_losses=bce_losses, dice_losses=dice_losses, focal_losses=focal_losses, filepath= Path(save_3losses_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'bce': bce_losses,\n",
        "        'dice': dice_losses,\n",
        "        'focal': focal_losses\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMFmkSOtXHQv"
      },
      "source": [
        "#### Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bPiRNBuK2S6",
        "outputId": "56b0e020-a570-47fd-99f0-1f1694167a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1190 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/1190 [00:06<2:13:01,  6.71s/it, learning_rate=0.0001, loss=1.01]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  1\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 2/1190 [00:07<1:04:52,  3.28s/it, learning_rate=0.0001, loss=1.01]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  2\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 3/1190 [00:08<40:07,  2.03s/it, learning_rate=0.0001, loss=1.03]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  3\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 4/1190 [00:09<31:47,  1.61s/it, learning_rate=0.0001, loss=0.999]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 5/1190 [00:09<24:32,  1.24s/it, learning_rate=0.0001, loss=0.998]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  5\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 6/1190 [00:10<19:24,  1.02it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 7/1190 [00:10<17:09,  1.15it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  7\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 8/1190 [00:11<15:38,  1.26it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  8\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 9/1190 [00:13<23:22,  1.19s/it, learning_rate=0.0001, loss=0.997]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  9\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/1190 [00:14<21:17,  1.08s/it, learning_rate=0.0001, loss=0.997]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 18 [9/8570 (1%)]\tLoss: 1.004840\n",
            "Iteration numbers:  10\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 11/1190 [00:15<20:08,  1.02s/it, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  11\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 12/1190 [00:16<20:49,  1.06s/it, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 13/1190 [00:16<17:34,  1.12it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  13\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 14/1190 [00:17<15:37,  1.25it/s, learning_rate=0.0001, loss=1.01]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  14\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 15/1190 [00:17<13:40,  1.43it/s, learning_rate=0.0001, loss=1.01]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 16/1190 [00:18<12:06,  1.62it/s, learning_rate=0.0001, loss=1.01]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 17/1190 [00:18<10:56,  1.79it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 18/1190 [00:19<10:10,  1.92it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  18\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 19/1190 [00:19<09:46,  2.00it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  19\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 20/1190 [00:20<11:51,  1.64it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 38 [19/8570 (2%)]\tLoss: 0.995256\n",
            "Iteration numbers:  20\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 21/1190 [00:20<10:47,  1.81it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 22/1190 [00:21<10:52,  1.79it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 23/1190 [00:22<12:01,  1.62it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  23\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 24/1190 [00:22<10:52,  1.79it/s, learning_rate=0.0001, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 25/1190 [00:23<10:07,  1.92it/s, learning_rate=0.0001, loss=0.997]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 26/1190 [00:23<09:31,  2.04it/s, learning_rate=0.0001, loss=0.993]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 27/1190 [00:24<09:53,  1.96it/s, learning_rate=0.0001, loss=0.99]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  27\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 28/1190 [00:24<09:47,  1.98it/s, learning_rate=0.0001, loss=0.988]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 29/1190 [00:25<09:40,  2.00it/s, learning_rate=0.0001, loss=0.985]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 30/1190 [00:25<09:14,  2.09it/s, learning_rate=0.0001, loss=0.98]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 58 [29/8570 (2%)]\tLoss: 0.857609\n",
            "Iteration numbers:  30\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 31/1190 [00:25<08:56,  2.16it/s, learning_rate=0.0001, loss=0.977]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 32/1190 [00:26<09:06,  2.12it/s, learning_rate=0.0001, loss=0.973]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  32\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 33/1190 [00:28<15:37,  1.23it/s, learning_rate=0.0001, loss=0.975]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  33\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 34/1190 [00:28<15:39,  1.23it/s, learning_rate=0.0001, loss=0.977]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  34\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 35/1190 [00:29<14:38,  1.31it/s, learning_rate=0.0001, loss=0.976]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 36/1190 [00:29<12:43,  1.51it/s, learning_rate=0.0001, loss=0.977]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 37/1190 [00:30<11:20,  1.69it/s, learning_rate=0.0001, loss=0.979]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 38/1190 [00:30<10:23,  1.85it/s, learning_rate=0.0001, loss=0.98]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 39/1190 [00:31<10:10,  1.88it/s, learning_rate=0.0001, loss=0.978]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 40/1190 [00:31<09:34,  2.00it/s, learning_rate=0.0001, loss=0.979]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 78 [39/8570 (3%)]\tLoss: 1.034781\n",
            "Iteration numbers:  40\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 41/1190 [00:32<09:29,  2.02it/s, learning_rate=0.0001, loss=0.98]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 42/1190 [00:32<10:04,  1.90it/s, learning_rate=0.0001, loss=0.98]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  42\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 43/1190 [00:33<10:44,  1.78it/s, learning_rate=0.0001, loss=0.98]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 44/1190 [00:34<12:09,  1.57it/s, learning_rate=0.0001, loss=0.979]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  44\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 45/1190 [00:34<11:20,  1.68it/s, learning_rate=0.0001, loss=0.979]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 46/1190 [00:35<11:48,  1.61it/s, learning_rate=0.0001, loss=0.98]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 47/1190 [00:35<10:48,  1.76it/s, learning_rate=0.0001, loss=0.977]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 48/1190 [00:36<10:16,  1.85it/s, learning_rate=0.0001, loss=0.976]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  48\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 49/1190 [00:36<10:18,  1.84it/s, learning_rate=0.0001, loss=0.969]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 50/1190 [00:37<10:16,  1.85it/s, learning_rate=0.0001, loss=0.969]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 98 [49/8570 (4%)]\tLoss: 0.949274\n",
            "Iteration numbers:  50\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 51/1190 [00:38<10:54,  1.74it/s, learning_rate=0.0001, loss=0.967]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 52/1190 [00:38<10:44,  1.77it/s, learning_rate=0.0001, loss=0.966]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  52\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 53/1190 [00:39<10:14,  1.85it/s, learning_rate=0.0001, loss=0.965]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 54/1190 [00:39<09:35,  1.97it/s, learning_rate=0.0001, loss=0.964]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 55/1190 [00:40<10:52,  1.74it/s, learning_rate=0.0001, loss=0.964]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 56/1190 [00:40<11:19,  1.67it/s, learning_rate=0.0001, loss=0.965]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  56\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 57/1190 [00:41<12:21,  1.53it/s, learning_rate=0.0001, loss=0.966]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  57\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 58/1190 [00:42<12:37,  1.49it/s, learning_rate=0.0001, loss=0.966]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 59/1190 [00:43<12:54,  1.46it/s, learning_rate=0.0001, loss=0.967]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  59\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 60/1190 [00:43<13:02,  1.44it/s, learning_rate=0.0001, loss=0.968]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 118 [59/8570 (5%)]\tLoss: 1.024971\n",
            "Iteration numbers:  60\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 61/1190 [00:44<12:12,  1.54it/s, learning_rate=0.0001, loss=0.967]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  61\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 62/1190 [00:45<11:58,  1.57it/s, learning_rate=0.0001, loss=0.967]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  62\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 63/1190 [00:45<11:23,  1.65it/s, learning_rate=0.0001, loss=0.966]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 64/1190 [00:46<10:40,  1.76it/s, learning_rate=0.0001, loss=0.966]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 65/1190 [00:46<10:33,  1.78it/s, learning_rate=0.0001, loss=0.963]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 66/1190 [00:47<09:48,  1.91it/s, learning_rate=0.0001, loss=0.962]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 67/1190 [00:47<09:55,  1.88it/s, learning_rate=0.0001, loss=0.961]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 68/1190 [00:47<09:25,  1.98it/s, learning_rate=0.0001, loss=0.961]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 69/1190 [00:48<09:28,  1.97it/s, learning_rate=0.0001, loss=0.96]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  69\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 70/1190 [00:48<09:06,  2.05it/s, learning_rate=0.0001, loss=0.96]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 138 [69/8570 (6%)]\tLoss: 0.990004\n",
            "Iteration numbers:  70\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 71/1190 [00:49<08:52,  2.10it/s, learning_rate=0.0001, loss=0.958]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  71\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 72/1190 [00:49<08:42,  2.14it/s, learning_rate=0.0001, loss=0.958]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 73/1190 [00:50<09:11,  2.03it/s, learning_rate=0.0001, loss=0.955]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 74/1190 [00:51<10:25,  1.78it/s, learning_rate=0.0001, loss=0.954]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration numbers:  74\n",
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 75/1190 [00:51<09:57,  1.87it/s, learning_rate=0.0001, loss=0.949]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 76/1190 [00:52<10:04,  1.84it/s, learning_rate=0.0001, loss=0.948]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 77/1190 [00:52<10:48,  1.72it/s, learning_rate=0.0001, loss=0.945]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 78/1190 [00:53<10:51,  1.71it/s, learning_rate=0.0001, loss=0.942]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n",
            "Iteration numbers:  79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 79/1190 [00:53<10:02,  1.84it/s, learning_rate=0.0001, loss=0.939]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 1, 512, 512])\n",
            "point_maps_pre[-1] shape:torch.Size([2, 1, 32, 32]), point_c4 shape:torch.Size([2, 1, 32, 32])\n",
            "torch.Size([2, 1, 32, 32]) torch.Size([2, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "till_epoch = EPOCHS\n",
        "model.to(DEVICE)\n",
        "model.train()\n",
        "# Lists to store epoch-to-epoch progress details\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "val_score_values = []\n",
        "val_loss_values = []\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, till_epoch+1):\n",
        "    epoch_count.append(epoch)\n",
        "    loss, batch_count_train, batch_train_loss = train_one_epoch(train_dataloader, model, optimizer, criterion, epoch=epoch)\n",
        "    loss_values.append(loss)\n",
        "    store_batch_training_details(epoch, batch_count_train, batch_train_loss)\n",
        "    if not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step()\n",
        "    # Storing Training details for plotting curves and inference\n",
        "    print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}\")\n",
        "\n",
        "    \"\"\"-------------------VALIDATION---------------------\"\"\"\n",
        "    dice, batch_count_val, batch_val_score_values, val_loss = evaluate(val_dataloader, model, epoch=epoch) # Evaluates model performance by calculating the dice coefficient\n",
        "    val_score_values.append(dice)\n",
        "    val_loss_values.append(val_loss)\n",
        "    # Storing Validation details for plotting curves and inference\n",
        "    store_batch_validation_details(epoch, batch_count_val, batch_val_score_values)\n",
        "    print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}, VAL DICE: {dice}\")\n",
        "\n",
        "    save_progress(epoch_count, loss_values, val_loss_values)\n",
        "    save_dice_score(epoch_count, val_score_values)\n",
        "    save_losses()\n",
        "# \"\"\"\" If it is necessary to save model's state dict as checkpoint, do the essential changes\n",
        "#      in Early Stopping class.\"\"\"\n",
        "\n",
        "    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step(dice)\n",
        "\n",
        "    es(epoch, dice, model, optimizer, loss, model_path= model_checkpoint_path + f\"_epoch{epoch}_bst_model{IMG_SIZE}_fold{FOLD_ID}_{np.round(dice,4)}.tar\")\n",
        "    if es.early_stop:\n",
        "        print('\\n\\n -------------- EARLY STOPPING -------------- \\n\\n')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO0V40DhG6TM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "key_patch_map = np.load(\"C:/Users/nisha/Desktop/Research Project May-June/gt_keypatch/1.2.276.0.7230010.3.1.4.8323329.300.1517875162.258081.npy\")\n",
        "sns.heatmap(key_patch_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRoO5PvbG6TM"
      },
      "outputs": [],
      "source": [
        "plt.imshow(key_patch_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSUJdNHlG6TM"
      },
      "outputs": [],
      "source": [
        "key_patch_map.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-LO3JIgG6TN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "key_patch_map = torch.unsqueeze(torch.Tensor(key_patch_map), axis=0)\n",
        "print(key_patch_map.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJFz9wAhzL4k"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgLZEbqwyAb4"
      },
      "source": [
        "20 mins for a single epoch of training and validation together with T4 GPU. If the no. of epoches is 50 and early stopping is 10, let's see how many hours it takes to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ptGWY9E2cn"
      },
      "source": [
        "Visualizations for the predictions!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KC5W67ovsqs"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")221d\n",
        "52\n",
        "# fig, ax = plt.subplots()awjk\n",
        "506\n",
        "132\n",
        "54\n",
        "# ax.plot(epoch_count, loss_values)\n",
        "# ax.set_title(\"Training Loss Curve [EPOCHS]\", fontsize=20)\n",
        "# ax.set_xlabel(\"epoch number\", fontsize=14)\n",
        "# ax.set_ylabel(\"loss value\", fontsize=14)\n",
        "# ax.tick_params(axis='both', labelsize=14)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8TB_fU0f1jh"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")\n",
        "# fig, ax = plt.subplots()\n",
        "\n",
        "# ax.plot(epoch_count, val_loss_values)\n",
        "# ax.set_title(\"Validation Loss Curve [EPOCHS]\", fontsize=20)\n",
        "# ax.set_xlabel(\"epoch number\", fontsize=14)\n",
        "# ax.set_ylabel(\"loss value\", fontsize=14)\n",
        "# ax.tick_params(axis='both', labelsize=14)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ2c6YzKSuFU"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(epoch_count, loss_values)\n",
        "# plt.plot(epoch_count, val_loss_values)\n",
        "# plt.title(\"Training and Validation Loss Curves\", fontsize=20)\n",
        "# plt.xlabel(\"epoch number\", fontsize=14)\n",
        "# plt.ylabel(\"loss value\", fontsize=14)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2BXTyFrPxzy"
      },
      "outputs": [],
      "source": [
        "# Load data from CSV into a DataFrame\n",
        "file_path = save_progress_path  # Replace with your CSV file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Plotting the line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['epoch'], df['train_loss'], marker='o', linestyle='-', color='b', label='Train Loss')\n",
        "plt.plot(df['epoch'], df['val_loss'], marker='o', linestyle='-', color='r', label='Validation Loss')\n",
        "\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOQs9Sy2VBHR"
      },
      "source": [
        "# Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_QmgkWSGPsu"
      },
      "outputs": [],
      "source": [
        "# checkpoint = torch.load(es.BEST_MODEL_PATH)\n",
        "# print(es.best_score)\n",
        "# model.to(DEVICE)\n",
        "# model.load_state_dict(checkpoint[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnM0qtplSwyW"
      },
      "outputs": [],
      "source": [
        "# model.to(DEVICE)\n",
        "# model.eval()\n",
        "# idx = 1995\n",
        "# mask = val_dataset[idx][\"mask\"].unsqueeze(0)\n",
        "# print(\"Mask Shape: \",mask.shape)\n",
        "# image = val_dataset[idx][\"image\"].unsqueeze(0)\n",
        "# print(\"Image Shape: \",image.shape)\n",
        "# raw_output = model(image.to(DEVICE))\n",
        "# print(\"raw output shape: \", raw_output.shape)\n",
        "# print(\"------ Raw Output ------\")\n",
        "# print(raw_output)\n",
        "# print(\"------- Pred Probs -------\")\n",
        "# pred_probs = torch.sigmoid(raw_output)\n",
        "# print(pred_probs)\n",
        "# print(\"------- How does the Mask look like? ------\")\n",
        "# print(mask)\n",
        "# print(\"-------- Predicted Segmentation mask --------\")\n",
        "# # binarizer_fn = TripletMaskBinarization(triplets=[[0.7, 600, 0.3]])\n",
        "# # mask = binarizer_fn.transform(mask).float()\n",
        "# # print(segmentation_mask)\n",
        "# segmentation_mask = (pred_probs > 0.4).float()\n",
        "# print(segmentation_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cALrPDiZFyhu"
      },
      "outputs": [],
      "source": [
        "# mask.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prdvPG3XZiuL"
      },
      "outputs": [],
      "source": [
        "# segmentation_mask.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAzD4mVaTYJg"
      },
      "outputs": [],
      "source": [
        "# plt.style.use(\"classic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak3eAHrDdY_V"
      },
      "outputs": [],
      "source": [
        "# first_channel_tensor = image[0, 0, :, :]\n",
        "# print(first_channel_tensor.shape)  # torch.Size([512, 512])\n",
        "# second_channel_tensor = image[0, 1, :, :]\n",
        "# print(second_channel_tensor.shape)  # torch.Size([512, 512])\n",
        "# third_channel_tensor = image[0, 2, :, :]\n",
        "# print(third_channel_tensor.shape)  # torch.Size([512, 512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRJzqWT1drZv"
      },
      "outputs": [],
      "source": [
        "# plt.title(\"First Channel Image\")\n",
        "# plt.imshow(first_channel_tensor.detach().cpu().numpy(), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS4Lttr2TLad"
      },
      "outputs": [],
      "source": [
        "# plt.imshow(mask.squeeze().detach().cpu().numpy(), cmap = 'gray')\n",
        "# print(mask.squeeze().shape)\n",
        "# plt.title(\"Mask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X9nSoL1TNw_"
      },
      "outputs": [],
      "source": [
        "# print(segmentation_mask.squeeze().shape)\n",
        "# plt.imshow(segmentation_mask.squeeze().detach().cpu().numpy(), cmap='gray')\n",
        "# plt.title(\"Segmentation Mask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi-kZiELUV79"
      },
      "outputs": [],
      "source": [
        "# print(type(pred_probs), type(mask))\n",
        "# print(pred_probs.shape, mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKAY7_pFh_Vt"
      },
      "outputs": [],
      "source": [
        "# print(segmentation_mask.squeeze().squeeze().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm9Qm1_FiFbS"
      },
      "outputs": [],
      "source": [
        "# print(mask.squeeze().squeeze().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kiIFEw2TPZn"
      },
      "outputs": [],
      "source": [
        "# # \"\"\"\n",
        "# # Both methods aim to capture the overlap between predicted positive regions and actual positive regions in the ground truth.\n",
        "# # The dice_metric approach leverages the full range of predicted probabilities, while the  metric function relies on a binary classification based on a chosen threshold.\n",
        "# # \"\"\"\n",
        "# # Dice = metric(pred_probs.detach().cpu(), mask).item()\n",
        "# # print(f\"Dice coefficient: {Dice}\")\n",
        "# # dice_metric_score = dice_metric(pred_probs.detach().cpu(), mask)\n",
        "# # print(f\"Dice coefficient: {dice_metric_score}\")\n",
        "\n",
        "# \"\"\"\n",
        "# Both methods aim to capture the overlap between predicted positive regions and actual positive regions in the ground truth.\n",
        "# The dice_metric approach leverages the full range of predicted probabilities, while the  metric function relies on a binary classification based on a chosen threshold.\n",
        "# \"\"\"\n",
        "# Dice = metric(segmentation_mask.detach().cpu(), mask).item()\n",
        "# print(f\"Dice coefficient: {Dice}\")\n",
        "# dice_metric_score = dice_metric(segmentation_mask.squeeze().squeeze().detach().cpu(), mask.squeeze().squeeze(), per_image=False)\n",
        "# print(f\"Dice coefficient: {dice_metric_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLi3ZH-8ALZM"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P0OtlxdALZN"
      },
      "outputs": [],
      "source": [
        "# from torchvision.transforms import Resize\n",
        "# from torchvision.utils import make_grid\n",
        "\n",
        "\n",
        "# def preprocess_image(image, target_size):\n",
        "#     transform = Resize(target_size)\n",
        "#     image = transform(image)\n",
        "#     return image\n",
        "\n",
        "# def generate_predictions(model, images):\n",
        "#     with torch.inference_mode():\n",
        "#         predictions = model(images.to(DEVICE))\n",
        "#         predictions = torch.sigmoid(predictions)\n",
        "#         return predictions\n",
        "\n",
        "\n",
        "# def visualize_predictions(images, target_masks, preds):\n",
        "#     images = images.cpu() # shape: [8, 3, 1024, 1024] or [8, 3, 512, 512]\n",
        "#     target_masks = target_masks.cpu() # shape: [8, 1, 1024, 1024] or [8, 1, 512, 512]\n",
        "#     preds = preds.cpu() # shape: [8, 1, 1024, 1024] or [8, 1, 512, 512]\n",
        "\n",
        "#     fig, axes = plt.subplots(2, 3, figsize=(20, 30))\n",
        "\n",
        "#     for i in range(2):\n",
        "#         # axes[i, 0].imshow(images[i].permute(1, 2, 0))\n",
        "#         axes[i, 0].imshow(images[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 0].set_title(\"Input Image\")\n",
        "#         # axes[i, 0].axis(\"off\")\n",
        "\n",
        "#         axes[i, 1].imshow(target_masks[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 1].set_title(\"Target Mask\")\n",
        "#         # axes[i, 1].axis(\"off\")\n",
        "\n",
        "#         axes[i, 2].imshow(preds[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 2].set_title(\"Model 512 prediction\")\n",
        "#         # axes[i, 2].axis(\"off\")\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPbHLrDvALZN"
      },
      "outputs": [],
      "source": [
        "# \"\"\"----------------------------- Inference --------------------------------\"\"\"\n",
        "\n",
        "# metrics512 = defaultdict(float)\n",
        "# for b_idx, data in enumerate(val_dataloader):\n",
        "#     batch_images = data[\"image\"] # shape: [8, 3, 1024, 1024]\n",
        "#     images_resized_512 = preprocess_image(batch_images, (512, 512))\n",
        "\n",
        "#     # Generate predictions\n",
        "#     preds = generate_predictions(model, images_resized_512)  # shape: [1, 1, 512, 512]\n",
        "#     print(\"preds - Shape: \", preds.shape, \" || device: \", preds.get_device())\n",
        "#     seg_mask_ensemble_512 = preds > 0.4\n",
        "#     # mask preprocessing\n",
        "#     batch_masks = data[\"mask\"]\n",
        "#     masks_resized_512 = preprocess_image(batch_masks, (512, 512))\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"---------------------------- Dice Scores -----------------------------------\")\n",
        "\n",
        "\n",
        "#     dice_scores_512 = dice_metric(seg_mask_ensemble_512, masks_resized_512.to(DEVICE), per_image=True)\n",
        "#     print(\"dice_scores_512 - Shape: \", dice_scores_512.shape, \" || type: \", type(dice_scores_512))\n",
        "\n",
        "#     # dice_scores_1024 = dice_metric(combined_pred, batch_images.to(DEVICE), per_image=True)\n",
        "#     # print(\"dice_scores_1024 - Shape: \", dice_scores_1024.shape, \" || type: \", type(dice_scores_1024))\n",
        "\n",
        "#     print(\"Dice scores of predictions on the 512x512 scale\\n\", dice_scores_512)\n",
        "#     print(\"Dice score of the batch 512x512 scale: \", dice_scores_512.mean())\n",
        "\n",
        "#     print(\"---------------------------- SCALE = 1024x1024 -----------------------------------\")\n",
        "#     visualize_predictions(batch_images, batch_masks, seg_mask_ensemble_512)\n",
        "#     print(\"-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-\")\n",
        "#     break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Y7HFiESzDWcY",
        "Xfcsl-YW_ck9",
        "3A4XlffPf1jV",
        "7xtZrBIJeLH_",
        "A-y8ovXEK2St",
        "od_pZDUoK2Su",
        "EqxMQpqB5JFV",
        "BNLEHf3QK2Sv",
        "6-dcysMmK2Sw",
        "Vzze2VhIK2S5",
        "kOlM9jTEK2S5",
        "K136b3eqK2S5",
        "pOkP2HYpvsqr",
        "LcACWXgAXHQu",
        "gJFz9wAhzL4k",
        "iOQs9Sy2VBHR",
        "cLi3ZH-8ALZM"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deeplearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}