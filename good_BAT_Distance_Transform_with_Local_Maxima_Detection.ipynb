{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY48Ap7wK2Sn"
      },
      "source": [
        "PNEUMOTHORAX SEGMENTATION USING SIIM-ACR DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMmEp3gqUyn1",
        "outputId": "baabba41-dae8-430e-8cf9-1f23b8ddbc03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y_qWc-LUIJCv"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/qubvel/segmentation_models.pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz4WJSBg9UOk",
        "outputId": "718c663c-df96-4b09-b7c7-6af639a89a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sEfIEmqfADNu"
      },
      "outputs": [],
      "source": [
        "# !pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ECBIUxVDADNv"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from monai.networks.nets.swin_unetr import SwinTransformer as SwinViT\n",
        "# from monai.utils import ensure_tuple_rep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7HFiESzDWcY"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CUKZcMbBK2Ss"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from collections import defaultdict\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score\n",
        "import torchvision.models as models # NEW MARCH-JUNE EDIT\n",
        "\n",
        "try:\n",
        "    from itertools import ifilterfalse\n",
        "except ImportError:  # py3k\n",
        "    from itertools import filterfalse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pzbBvorxADNw"
      },
      "outputs": [],
      "source": [
        "def init_seed(SEED=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Un-DgSH9ADNx"
      },
      "outputs": [],
      "source": [
        "init_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfcsl-YW_ck9"
      },
      "source": [
        "## Config - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "-vx6RpZ8_bba"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE         = 512 # This is efficient and better for the completion of the project\n",
        "WHOSE_DIR        = \"Neeshanth\"\n",
        "\n",
        "if WHOSE_DIR == \"Aathesh\":\n",
        "    DIR              = \"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\mask\")\n",
        "    DATA_FRAME_PATH  = \"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Colab_Notebooks\\\\datasets\\\\main\\\\RLE_kfold.csv\"\n",
        "elif WHOSE_DIR == \"Neeshanth\":\n",
        "    DIR              = \"/content/drive/MyDrive/datasets\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"/content/drive/MyDrive/datasets/train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"/content/drive/MyDrive/datasets/mask\")\n",
        "    DATA_FRAME_PATH  = \"/content/drive/MyDrive/datasets/RLE_kfold.csv\"\n",
        "\n",
        "elif WHOSE_DIR == \"Kousik\":\n",
        "    DIR              = r\"C:\\Users\\kousi\\Downloads\\datasets\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(r\"C:\\Users\\kousi\\Downloads\\datasets\\train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(r\"C:\\Users\\kousi\\Downloads\\datasets\\mask\")\n",
        "    DATA_FRAME_PATH  = r\"C:\\Users\\kousi\\Downloads\\datasets\\RLE_kfold.csv\"\n",
        "\n",
        "elif WHOSE_DIR == \"wonder_boys\":\n",
        "    DIR              = \"/content/drive/MyDrive/Colab_Notebooks/datasets/main\"\n",
        "    DATA_DIR         = Path(DIR)\n",
        "    TRAIN_IMG_DIR    = Path(\"/content/drive/MyDrive/Colab_Notebooks/datasets/main/train_png\")\n",
        "    TRAIN_LBL_DIR    = Path(\"/content/drive/MyDrive/Colab_Notebooks/datasets/main/mask\")\n",
        "    DATA_FRAME_PATH  = \"/content/drive/MyDrive/Colab_Notebooks/datasets/main/RLE_kfold.csv\"\n",
        "\n",
        "KFOLD_PATH       = \"\"\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 8\n",
        "BATCH_SIZE       = 8\n",
        "EPOCHS           = 50\n",
        "# Path for pretrained model weights\n",
        "TRAINING_MODEL_PATH = \"\"\n",
        "USE_SAMPLER      = True\n",
        "POSTIVE_PERC     = 0.8\n",
        "DEVICE           = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PRETRAINED       = False # False means we're using ImageNet weights, so essentially, it's pretrained & never from scratch!!!!!\n",
        "LEARNING_RATE    = 0.001\n",
        "NUM_WORKERS      = 8\n",
        "USE_CRIT         = True\n",
        "FOLD_ID          = 4\n",
        "EVAL_METRICS      = [\"metric - it calculates dice coefficient.\"]\n",
        "\n",
        "# Regularization Settings\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "L2_WEIGHT_DECAY  = 0.000005\n",
        "GRADIENT_CLIPPING = True\n",
        "GRADIENT_CLIPPING_THRESHOLD = 0.1\n",
        "\n",
        "# IF U DON'T WANT TO CHANGE Learning rate from previous experiment then set this to True.\n",
        "OPTIMIZER_LOAD = False\n",
        "# U MUST VERIFY IF LR IS GETTING SET PROPERLY FOR THE SCHEDULER IN THE \"OPTIMIZER & SCHEDULER\" SECTION OF THE NOTEBOOK.\n",
        "\n",
        "# Learning Rate Scheduler Settings\n",
        "SCHEDULER        = \"ReduceLROnPlateau\"\n",
        "if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "    SCHEDULER_PARAMS = {'factor': 0.1, 'patience': 1, 'threshold': 0.0000001, 'min_lr': 0.0000001} # patience changed from 2 to 1 on 12-04-2025\n",
        "elif SCHEDULER == \"CosineAnnealingWarmRestarts\":\n",
        "    SCHEDULER_PARAMS = {'T_0': 1, 'T_mult': 2, 'eta_min': 0.00000001}\n",
        "elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "    SCHEDULER_PARAMS = {'T_max': 8, 'eta_min': 0.0000001}\n",
        "\n",
        "# Thresholds for 1024x1024 is there along with div by 2 values and div by 4 values\n",
        "TRIPLET_THRESHOLDS = [  [0.6, 500.0, 0.35], [0.67, 500.0, 0.37], [0.75, 500.0, 0.3],\n",
        "                        [0.75, 500.0, 0.4], [0.75, 1000.0, 0.3], [0.75, 1000.0, 0.4],\n",
        "                        [0.6, 1000.0, 0.3], [0.6, 1000.0, 0.4], [0.6, 1500.0, 0.3],\n",
        "                        [0.6, 1500.0, 0.4], [0.6, 250.0, 0.35], [0.67, 250.0, 0.37],\n",
        "                        [0.75, 250.0, 0.3], [0.75, 250.0, 0.4], [0.75, 500.0, 0.3],\n",
        "                        [0.75, 500.0, 0.4], [0.6, 500.0, 0.3], [0.6, 500.0, 0.4],\n",
        "                        [0.6, 750.0, 0.3], [0.6, 750.0, 0.4], [0.6, 1000, 0.35],\n",
        "                        [0.67, 1000, 0.37], [0.75, 1000, 0.3], [0.75, 1000, 0.4],\n",
        "                        [0.75, 2000, 0.3], [0.75, 2000, 0.4], [0.6, 2000, 0.3],\n",
        "                        [0.6, 2000, 0.4], [0.6, 3000, 0.3], [0.6, 3000, 0.4]       ]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    \"-------------------------------SAVING losses & metrics-------------------------------------\"\n",
        "\"\"\"\n",
        "ACCOUNT         = \"Neeshanth\" # \"Neeshanth\" or \"wonder_boys\" or others\n",
        "PURPOSE         = \"Project-2\" # training/inference/hyperparameter-tuning or hpt OR ANYTHING MORE SPECIFIC\n",
        "EXP_NO          = \"Novel_Algo_1\" # hpt experiment with changed settings\n",
        "PHASE           = \"14_04_2025\" # Just date\n",
        "\n",
        "EFFECTIVE_BATCH_SIZE = 8 # accumulation_steps * BATCH_SIZE\n",
        "\n",
        "if ACCOUNT == \"Neeshanth\":\n",
        "    # Save Config.txt\n",
        "    CONFIG_FILE_LOC = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "if ACCOUNT == \"Kousik\":\n",
        "    # Save Config.txt\n",
        "    CONFIG_FILE_LOC = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch-wise comboloss during training\n",
        "    store_batch_training_details_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\"  # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch-wise comboloss during validation\n",
        "    store_batch_validation_details_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch-wise train and val losses to monitor overfitting\n",
        "    save_progress_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch-wise dice coefficient\n",
        "    save_dice_score_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save BCE, dice & focal losses separately after training is done\n",
        "    save_3losses_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = rf\"E:\\Project Metrics\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "\n",
        "if ACCOUNT == \"wonder_boys\":\n",
        "    # Save Config.txt file\n",
        "    CONFIG_FILE_LOC = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class\n",
        "    model_checkpoint_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_train_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_dice_scores.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_all_loss_values.csv\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"/content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/{PURPOSE}_EXP_{EXP_NO}_{PHASE}/{EXP_NO}_best_checkpoint_thresholds.csv\"\n",
        "\n",
        "if ACCOUNT == \"pc_aathesh\":\n",
        "    # Save Config.txt file\n",
        "    CONFIG_FILE_LOC = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}/Config.txt\"\n",
        "\n",
        "    # Save batch wise comboloss during training\n",
        "    store_batch_training_details_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\CHECKPOINTS\"\n",
        "    name_of_batch_training_details_csv = f\"{PURPOSE}_TRAIN_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_epoch_\" # epoch no. will be added to the end\n",
        "\n",
        "    # Save batch wise comboloss during validation\n",
        "    store_batch_validation_details_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_vali_m{IMG_SIZE}_b{EFFECTIVE_BATCH_SIZE}\"\n",
        "    name_of_batch_validation_details_csv = f\"{PURPOSE}_{IMG_SIZE}_b_{EFFECTIVE_BATCH_SIZE}_Validation_Metrics\"\n",
        "\n",
        "    # Save epoch wise train and val losses to monitor overfitting\n",
        "    save_progress_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_progress.csv\"\n",
        "\n",
        "    # Save epoch wise dice coefficient\n",
        "    save_dice_score_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_dice_score.csv\"\n",
        "\n",
        "    # Save bce, dice & focal losses separately after training is done\n",
        "    save_3losses_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_all_loss_vals.csv\"\n",
        "\n",
        "    # Save model checkpoint using early stopping class - CREATE A NEW FOLDER\n",
        "    model_checkpoint_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\m_{IMG_SIZE}_CHECKPOINTS\"\n",
        "\n",
        "    # Save best thresholds & their dice scores - done according to Triplet Scheme of Binarization\n",
        "    save_best_thresholds_path = f\"C:\\\\Users\\\\aathe\\\\Google Drive - Wonder Boys\\\\Saved Models\\\\{PURPOSE}_EXP_{EXP_NO}_{PHASE}\\\\{PURPOSE}_best_checkpoint_thresholds.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A4XlffPf1jV"
      },
      "source": [
        "## Saving Config as .txt - TYPE ESSENTIAL DETAILS TO RECOVER THIS EXPERIMENT IN THE BELOW SNIPPET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB0Igy_Tf1jV"
      },
      "source": [
        "ESSENTIALS include current_notebook_loc_in_pc, previous_notebook_loc_in_pc, key_changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3likqSVNf1jV",
        "outputId": "3c662a98-1ffe-49e7-bdbf-9af8121f63af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current IST date and time is: 14-04-2025 22:01:58\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# Define the IST timezone\n",
        "ist_timezone = pytz.timezone('Asia/Kolkata')\n",
        "\n",
        "# Get the current time in IST\n",
        "ist_time = datetime.now(ist_timezone)\n",
        "\n",
        "# Format the date and time to DD-MM-YYYY HH:MM:SS\n",
        "formatted_ist_time = ist_time.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "\n",
        "# Print the formatted IST time\n",
        "print(\"Current IST date and time is:\", formatted_ist_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bwNCwwVf1jV",
        "outputId": "6ef114fc-b5ae-4ca6-ff2a-76ef19ae8ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyper-parameters have been saved to /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Config.txt\n"
          ]
        }
      ],
      "source": [
        "hyperparameters = {\n",
        "    'image_size': f\"{IMG_SIZE}x{IMG_SIZE}\",\n",
        "    'date': formatted_ist_time,\n",
        "    'account': ACCOUNT,\n",
        "    'purpose': PURPOSE,\n",
        "    'experiment_no': EXP_NO,\n",
        "    'key_changes': \"\" ,\n",
        "    'checkpoint_used_loc': TRAINING_MODEL_PATH,\n",
        "    'is_optimizer_loaded': OPTIMIZER_LOAD,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'batch_size': EFFECTIVE_BATCH_SIZE,\n",
        "    'gradient_accumulation_steps': int(EFFECTIVE_BATCH_SIZE/BATCH_SIZE),\n",
        "    'num_epochs': EPOCHS,\n",
        "    'IsSamplerUsed': USE_SAMPLER,\n",
        "    'percentage_of_positive_samples': POSTIVE_PERC,\n",
        "    'optimizer': 'Adam',\n",
        "    'loss_function': 'ComboLoss',\n",
        "    'scheduler': SCHEDULER,\n",
        "    'scheduler_params': SCHEDULER_PARAMS,\n",
        "    'L2_Regularization_weight_decay':L2_WEIGHT_DECAY,\n",
        "    'early_stopping_patience': EARLY_STOPPING_PATIENCE,\n",
        "    'is_gradient_clipping_used': GRADIENT_CLIPPING,\n",
        "    'gradient_clipping_threshold': GRADIENT_CLIPPING_THRESHOLD,\n",
        "    'model': 'Hybrid Vision Transformer',\n",
        "    'training_phase': f\"Phase - {PHASE}\",\n",
        "    'fold_ID': FOLD_ID,\n",
        "    'GPU_name': torch.cuda.get_device_name(torch.cuda.current_device()),\n",
        "    'num_workers': NUM_WORKERS,\n",
        "    'weights_given_to_loss_functions': {'bce': 3, 'dice': 1, 'focal': 4},\n",
        "    'triplet_thresholds': TRIPLET_THRESHOLDS,\n",
        "    'evaluation_metrics': EVAL_METRICS,\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a formatted string\n",
        "hyperparameters_str = json.dumps(hyperparameters, indent=4)\n",
        "\n",
        "# Specify the file name\n",
        "file_name = CONFIG_FILE_LOC\n",
        "\n",
        "os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "# Write the string to a file\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(hyperparameters_str)\n",
        "\n",
        "print(f\"Hyper-parameters have been saved to {file_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xtZrBIJeLH_"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "BjVO5UiC1z-h"
      },
      "outputs": [],
      "source": [
        "bce_losses = []\n",
        "dice_losses = []\n",
        "focal_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "8MgSVyz79UOs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from scipy.ndimage import distance_transform_edt as distance\n",
        "from skimage import segmentation as skimage_seg\n",
        "\n",
        "\n",
        "def dice_loss(score, target):\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * target)\n",
        "    y_sum = torch.sum(target * target)\n",
        "    z_sum = torch.sum(score * score)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def dice_loss1(score, target):\n",
        "    # non-square\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * target)\n",
        "    y_sum = torch.sum(target)\n",
        "    z_sum = torch.sum(score)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def iou_loss(score, target):\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    tp_sum = torch.sum(score * target)\n",
        "    fp_sum = torch.sum(score * (1 - target))\n",
        "    fn_sum = torch.sum((1 - score) * target)\n",
        "    loss = (tp_sum + smooth) / (tp_sum + fp_sum + fn_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def entropy_loss(p, C=2):\n",
        "    ## p N*C*W*H*D\n",
        "    y1 = -1 * torch.sum(p * torch.log(p + 1e-6), dim=1) / torch.tensor(\n",
        "        np.log(C)).cuda()\n",
        "    ent = torch.mean(y1)\n",
        "\n",
        "    return ent\n",
        "\n",
        "\n",
        "def softmax_dice_loss(input_logits, target_logits):\n",
        "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_softmax = F.softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "    n = input_logits.shape[1]\n",
        "    dice = 0\n",
        "    for i in range(0, n):\n",
        "        dice += dice_loss1(input_softmax[:, i], target_softmax[:, i])\n",
        "    mean_dice = dice / n\n",
        "\n",
        "    return mean_dice\n",
        "\n",
        "\n",
        "def entropy_loss_map(p, C=2):\n",
        "    ent = -1 * torch.sum(p * torch.log(p + 1e-6), dim=1,\n",
        "                         keepdim=True) / torch.tensor(np.log(C)).cuda()\n",
        "    return ent\n",
        "\n",
        "\n",
        "def softmax_mse_loss(input_logits, target_logits):\n",
        "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_softmax = F.softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "\n",
        "    mse_loss = (input_softmax - target_softmax)**2\n",
        "    return mse_loss\n",
        "\n",
        "\n",
        "def softmax_kl_loss(input_logits, target_logits):\n",
        "    \"\"\"Takes softmax on both sides and returns KL divergence\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_log_softmax = F.log_softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "\n",
        "    # return F.kl_div(input_log_softmax, target_softmax)\n",
        "    kl_div = F.kl_div(input_log_softmax, target_softmax, reduction='none')\n",
        "    # mean_kl_div = torch.mean(0.2*kl_div[:,0,...]+0.8*kl_div[:,1,...])\n",
        "    return kl_div\n",
        "\n",
        "\n",
        "def symmetric_mse_loss(input1, input2):\n",
        "    \"\"\"Like F.mse_loss but sends gradients to both directions\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to both input1 and input2.\n",
        "    \"\"\"\n",
        "    assert input1.size() == input2.size()\n",
        "    return torch.mean((input1 - input2)**2)\n",
        "\n",
        "\n",
        "def compute_sdf01(segmentation):\n",
        "    \"\"\"\n",
        "    compute the signed distance map of binary mask\n",
        "    input: segmentation, shape = (batch_size, class, x, y, z)\n",
        "    output: the Signed Distance Map (SDM)\n",
        "    sdm(x) = 0; x in segmentation boundary\n",
        "             -inf|x-y|; x in segmentation\n",
        "             +inf|x-y|; x out of segmentation\n",
        "    \"\"\"\n",
        "    # print(type(segmentation), segmentation.shape)\n",
        "\n",
        "    segmentation = segmentation.astype(np.uint8)\n",
        "\n",
        "    if len(segmentation.shape) == 4:  # 3D image\n",
        "        segmentation = np.expand_dims(segmentation, 1)\n",
        "    normalized_sdf = np.zeros(segmentation.shape)\n",
        "    if segmentation.shape[1] == 1:\n",
        "        dis_id = 0\n",
        "    else:\n",
        "        dis_id = 1\n",
        "    for b in range(segmentation.shape[0]):  # batch size\n",
        "        for c in range(dis_id, segmentation.shape[1]):  # class_num\n",
        "            # ignore background\n",
        "            posmask = segmentation[b][c]\n",
        "            if np.max(posmask) == 0:\n",
        "                continue\n",
        "            negmask = ~posmask\n",
        "            posdis = distance(posmask)\n",
        "            negdis = distance(negmask)\n",
        "            boundary = skimage_seg.find_boundaries(\n",
        "                posmask, mode='inner').astype(np.uint8)\n",
        "            sdf = negdis / np.max(negdis) / 2 - posdis / np.max(\n",
        "                posdis) / 2 + 0.5\n",
        "            sdf[boundary > 0] = 0.5\n",
        "            normalized_sdf[b][c] = sdf\n",
        "    return normalized_sdf\n",
        "\n",
        "\n",
        "def compute_sdf1_1(segmentation):\n",
        "    \"\"\"\n",
        "    compute the signed distance map of binary mask\n",
        "    input: segmentation, shape = (batch_size, class, x, y, z)\n",
        "    output: the Signed Distance Map (SDM)\n",
        "    sdm(x) = 0; x in segmentation boundary\n",
        "             -inf|x-y|; x in segmentation\n",
        "             +inf|x-y|; x out of segmentation\n",
        "    \"\"\"\n",
        "    # print(type(segmentation), segmentation.shape)\n",
        "\n",
        "    segmentation = segmentation.astype(np.uint8)\n",
        "    if len(segmentation.shape) == 4:  # 3D image\n",
        "        segmentation = np.expand_dims(segmentation, 1)\n",
        "    normalized_sdf = np.zeros(segmentation.shape)\n",
        "    if segmentation.shape[1] == 1:\n",
        "        dis_id = 0\n",
        "    else:\n",
        "        dis_id = 1\n",
        "    for b in range(segmentation.shape[0]):  # batch size\n",
        "        for c in range(dis_id, segmentation.shape[1]):  # class_num\n",
        "            # ignore background\n",
        "            posmask = segmentation[b][c]\n",
        "            if np.max(posmask) == 0:\n",
        "                continue\n",
        "            negmask = ~posmask\n",
        "            posdis = distance(posmask)\n",
        "            negdis = distance(negmask)\n",
        "            boundary = skimage_seg.find_boundaries(\n",
        "                posmask, mode='inner').astype(np.uint8)\n",
        "            sdf = negdis / np.max(negdis) - posdis / np.max(posdis)\n",
        "            sdf[boundary > 0] = 0\n",
        "            normalized_sdf[b][c] = sdf\n",
        "    return normalized_sdf\n",
        "\n",
        "\n",
        "def compute_fore_dist(segmentation):\n",
        "    \"\"\"\n",
        "    compute the foreground of binary mask\n",
        "    input: segmentation, shape = (batch_size, class, x, y, z)\n",
        "    output: the Signed Distance Map (SDM)\n",
        "    sdm(x) = 0; x in segmentation boundary\n",
        "             -inf|x-y|; x in segmentation\n",
        "             +inf|x-y|; x out of segmentation\n",
        "    \"\"\"\n",
        "    # print(type(segmentation), segmentation.shape)\n",
        "\n",
        "    segmentation = segmentation.astype(np.uint8)\n",
        "    if len(segmentation.shape) == 4:  # 3D image\n",
        "        segmentation = np.expand_dims(segmentation, 1)\n",
        "    normalized_sdf = np.zeros(segmentation.shape)\n",
        "    if segmentation.shape[1] == 1:\n",
        "        dis_id = 0\n",
        "    else:\n",
        "        dis_id = 1\n",
        "    for b in range(segmentation.shape[0]):  # batch size\n",
        "        for c in range(dis_id, segmentation.shape[1]):  # class_num\n",
        "            # ignore background\n",
        "            posmask = segmentation[b][c]\n",
        "            posdis = distance(posmask)\n",
        "            normalized_sdf[b][c] = posdis / np.max(posdis)\n",
        "    return normalized_sdf\n",
        "\n",
        "\n",
        "def sum_tensor(inp, axes, keepdim=False):\n",
        "    axes = np.unique(axes).astype(int)\n",
        "    if keepdim:\n",
        "        for ax in axes:\n",
        "            inp = inp.sum(int(ax), keepdim=True)\n",
        "    else:\n",
        "        for ax in sorted(axes, reverse=True):\n",
        "            inp = inp.sum(int(ax))\n",
        "    return inp\n",
        "\n",
        "\n",
        "def AAAI_sdf_loss(net_output, gt):\n",
        "    \"\"\"\n",
        "    net_output: net logits; shape=(batch_size, class, x, y, z)\n",
        "    gt: ground truth; (shape (batch_size, 1, x, y, z) OR (batch_size, x, y, z))\n",
        "    \"\"\"\n",
        "    smooth = 1e-5\n",
        "    axes = tuple(range(2, len(net_output.size())))\n",
        "    shp_x = net_output.shape\n",
        "    shp_y = gt.shape\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if len(shp_x) != len(shp_y):\n",
        "            gt = gt.view((shp_y[0], 1, *shp_y[1:]))\n",
        "\n",
        "        if all([i == j for i, j in zip(net_output.shape, gt.shape)]):\n",
        "            # if this is the case then gt is probably already a one hot encoding\n",
        "            y_onehot = gt\n",
        "        else:\n",
        "            gt = gt.long()\n",
        "            y_onehot = torch.zeros(shp_x)\n",
        "            if net_output.device.type == \"cuda\":\n",
        "                y_onehot = y_onehot.cuda(net_output.device.index)\n",
        "            y_onehot.scatter_(1, gt, 1)\n",
        "        gt_sdm_npy = compute_sdf1_1(y_onehot.cpu().numpy())\n",
        "        if net_output.device.type == \"cuda\":\n",
        "            gt_sdm = torch.from_numpy(gt_sdm_npy).float().cuda(\n",
        "                net_output.device.index)\n",
        "        else:\n",
        "            gt_sdm = torch.from_numpy(gt_sdm_npy).float()\n",
        "    intersect = sum_tensor(net_output * gt_sdm, axes, keepdim=False)\n",
        "    pd_sum = sum_tensor(net_output**2, axes, keepdim=False)\n",
        "    gt_sum = sum_tensor(gt_sdm**2, axes, keepdim=False)\n",
        "    L_product = (intersect + smooth) / (intersect + pd_sum + gt_sum)\n",
        "    # print('L_product.shape', L_product.shape) (4,2)\n",
        "    L_SDF_AAAI = -L_product.mean() + torch.norm(net_output - gt_sdm,\n",
        "                                                1) / torch.numel(net_output)\n",
        "\n",
        "    return L_SDF_AAAI\n",
        "\n",
        "\n",
        "def sdf_kl_loss(net_output, gt):\n",
        "    \"\"\"\n",
        "    net_output: net logits; shape=(batch_size, class, x, y, z)\n",
        "    gt: ground truth; (shape (batch_size, 1, x, y, z) OR (batch_size, x, y, z))\n",
        "    \"\"\"\n",
        "    smooth = 1e-5\n",
        "    axes = tuple(range(2, len(net_output.size())))\n",
        "    shp_x = net_output.shape\n",
        "    shp_y = gt.shape\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if len(shp_x) != len(shp_y):\n",
        "            gt = gt.view((shp_y[0], 1, *shp_y[1:]))\n",
        "\n",
        "        if all([i == j for i, j in zip(net_output.shape, gt.shape)]):\n",
        "            # if this is the case then gt is probably already a one hot encoding\n",
        "            y_onehot = gt\n",
        "        else:\n",
        "            gt = gt.long()\n",
        "            y_onehot = torch.zeros(shp_x)\n",
        "            if net_output.device.type == \"cuda\":\n",
        "                y_onehot = y_onehot.cuda(net_output.device.index)\n",
        "            y_onehot.scatter_(1, gt, 1)\n",
        "        # print('y_onehot.shape', y_onehot.shape)\n",
        "        gt_sdf_npy = compute_sdf(y_onehot.cpu().numpy())\n",
        "        gt_sdf = torch.from_numpy(gt_sdf_npy + smooth).float().cuda(\n",
        "            net_output.device.index)\n",
        "    # print('net_output, gt_sdf', net_output.shape, gt_sdf.shape)\n",
        "    # exit()\n",
        "    sdf_kl_loss = F.kl_div(net_output,\n",
        "                           gt_sdf[:, 1:2, ...],\n",
        "                           reduction='batchmean')\n",
        "\n",
        "    return sdf_kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "-GiUgprmeQ97"
      },
      "outputs": [],
      "source": [
        "eps = 1e-6\n",
        "\n",
        "\n",
        "def soft_dice_loss(outputs, targets, per_image=False, per_channel=False):\n",
        "    \"\"\"\n",
        "        If per_image = False, then the function calculates dice loss for a single image-mask pair.\n",
        "    \"\"\"\n",
        "    batch_size, n_channels = outputs.size(0), outputs.size(1)\n",
        "\n",
        "    eps = 1e-6\n",
        "    n_parts = 1\n",
        "    if per_image:\n",
        "        n_parts = batch_size\n",
        "    if per_channel:\n",
        "        n_parts = batch_size * n_channels\n",
        "\n",
        "    dice_target = targets.contiguous().view(n_parts, -1).float()\n",
        "    dice_output = outputs.contiguous().view(n_parts, -1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1)\n",
        "    loss = (1 - (2 * intersection + eps) / (union + eps)) # returns a tensor of size [8]\n",
        "    return loss.mean() # returns a tensor of size [1].\n",
        "\n",
        "def dice_metric(preds, trues, per_image=False, per_channel=False):\n",
        "    preds = preds.float()\n",
        "    return 1 - soft_dice_loss(preds, trues, per_image, per_channel)\n",
        "\n",
        "\n",
        "def jaccard(outputs, targets, per_image=False, non_empty=False, min_pixels=5):\n",
        "    batch_size = outputs.size()[0]\n",
        "    eps = 1e-3\n",
        "    if not per_image:\n",
        "        batch_size = 1\n",
        "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
        "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
        "    target_sum = torch.sum(dice_target, dim=1)\n",
        "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
        "    losses = 1 - (intersection + eps) / (torch.sum(dice_output + dice_target, dim=1) - intersection + eps)\n",
        "    if non_empty:\n",
        "        assert per_image == True\n",
        "        non_empty_images = 0\n",
        "        sum_loss = 0\n",
        "        for i in range(batch_size):\n",
        "            if target_sum[i] > min_pixels:\n",
        "                sum_loss += losses[i]\n",
        "                non_empty_images += 1\n",
        "        if non_empty_images == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return sum_loss / non_empty_images\n",
        "\n",
        "    return losses.mean()\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        dice_loss = soft_dice_loss(input, target, per_image=self.per_image)\n",
        "        dice_losses.append(dice_loss.item())\n",
        "        return dice_loss\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, per_image=False, non_empty=False, apply_sigmoid=False,\n",
        "                 min_pixels=5):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.per_image = per_image\n",
        "        self.non_empty = non_empty\n",
        "        self.apply_sigmoid = apply_sigmoid\n",
        "        self.min_pixels = min_pixels\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if self.apply_sigmoid:\n",
        "            input = torch.sigmoid(input)\n",
        "        return jaccard(input, target, per_image=self.per_image, non_empty=self.non_empty, min_pixels=self.min_pixels)\n",
        "\n",
        "class StableBCELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StableBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        bce_loss_with_logits = nn.BCEWithLogitsLoss(reduction='mean') # mean is taken across batches\n",
        "        bce_loss = bce_loss_with_logits(logits, target)\n",
        "        bce_losses.append(bce_loss.item())\n",
        "        return bce_loss # returns a tensor of size [1]\n",
        "\n",
        "\n",
        "class ComboLoss(nn.Module):\n",
        "    def __init__(self, weights, per_image=True, channel_weights=[1, 0.5, 0.5], channel_losses=None):\n",
        "        super().__init__()\n",
        "        self.weights = weights\n",
        "        self.bce = StableBCELoss()\n",
        "        self.dice = DiceLoss(per_image=True)\n",
        "        self.jaccard = JaccardLoss(per_image=True)\n",
        "        self.lovasz = LovaszLoss(per_image=per_image)\n",
        "        self.lovasz_sigmoid = LovaszLossSigmoid(per_image=per_image)\n",
        "        self.focal = FocalLoss2d()\n",
        "        self.mapping = {'bce': self.bce,\n",
        "                        'dice': self.dice,\n",
        "                        'focal': self.focal,\n",
        "                        'jaccard': self.jaccard,\n",
        "                        'lovasz': self.lovasz,\n",
        "                        'lovasz_sigmoid': self.lovasz_sigmoid}\n",
        "        self.expect_sigmoid = {'dice', 'focal', 'jaccard', 'lovasz_sigmoid'}\n",
        "        self.per_channel = {'dice', 'jaccard', 'lovasz_sigmoid'}\n",
        "        self.values = {}\n",
        "        self.channel_weights = channel_weights\n",
        "        self.channel_losses = channel_losses\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        loss = 0\n",
        "        weights = self.weights\n",
        "        sigmoid_input = torch.sigmoid(outputs)\n",
        "        for k, v in weights.items():\n",
        "            if not v:\n",
        "                continue\n",
        "            val = 0\n",
        "            if k in self.per_channel:\n",
        "                channels = targets.size(1)\n",
        "                for c in range(channels):\n",
        "                    if not self.channel_losses or k in self.channel_losses[c]:\n",
        "                        val += self.channel_weights[c] * self.mapping[k](sigmoid_input[:, c, ...] if k in self.expect_sigmoid else outputs[:, c, ...],\n",
        "                                               targets[:, c, ...])\n",
        "\n",
        "            else:\n",
        "                val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n",
        "\n",
        "            self.values[k] = val\n",
        "            loss += self.weights[k] * val\n",
        "        return loss.clamp(min=1e-5)\n",
        "\n",
        "\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    p = len(gt_sorted)\n",
        "    gts = gt_sorted.sum()\n",
        "    intersection = gts.float() - gt_sorted.float().cumsum(0)\n",
        "    union = gts.float() + (1 - gt_sorted).float().cumsum(0)\n",
        "    jaccard = 1. - intersection / union\n",
        "    if p > 1:  # cover 1-pixel case\n",
        "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
        "    return jaccard\n",
        "\n",
        "\n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
        "                    for log, lab in zip(logits, labels))\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "    if len(labels) == 0:\n",
        "        # only void pixels, the gradients should be 0\n",
        "        return logits.sum() * 0.\n",
        "    signs = 2. * labels.float() - 1.\n",
        "    errors = (1. - logits * Variable(signs))\n",
        "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
        "    perm = perm.data\n",
        "    gt_sorted = labels[perm]\n",
        "    grad = lovasz_grad(gt_sorted)\n",
        "    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = scores.view(-1)\n",
        "    labels = labels.view(-1)\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = (labels != ignore)\n",
        "    vscores = scores[valid]\n",
        "    vlabels = labels[valid]\n",
        "    return vscores, vlabels\n",
        "\n",
        "\n",
        "def lovasz_sigmoid(probas, labels, per_image=False, ignore=None):\n",
        "    \"\"\"\n",
        "    Multi-class Lovasz-Softmax loss\n",
        "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n",
        "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
        "      only_present: average only on classes present in ground truth\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class labels\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        loss = mean(lovasz_sigmoid_flat(*flatten_binary_scores(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
        "                          for prob, lab in zip(probas, labels))\n",
        "    else:\n",
        "        loss = lovasz_sigmoid_flat(*flatten_binary_scores(probas, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def lovasz_sigmoid_flat(probas, labels):\n",
        "    \"\"\"\n",
        "    Multi-class Lovasz-Softmax loss\n",
        "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
        "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
        "      only_present: average only on classes present in ground truth\n",
        "    \"\"\"\n",
        "    fg = labels.float()\n",
        "    errors = (Variable(fg) - probas).abs()\n",
        "    errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
        "    perm = perm.data\n",
        "    fg_sorted = fg[perm]\n",
        "    loss = torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))\n",
        "    return loss\n",
        "\n",
        "def symmetric_lovasz(outputs, targets, ):\n",
        "    return (lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1 - targets)) / 2\n",
        "\n",
        "def mean(l, ignore_nan=False, empty=0):\n",
        "    \"\"\"\n",
        "    nanmean compatible with generators.\n",
        "    \"\"\"\n",
        "    l = iter(l)\n",
        "    if ignore_nan:\n",
        "        l = ifilterfalse(np.isnan, l)\n",
        "    try:\n",
        "        n = 1\n",
        "        acc = next(l)\n",
        "    except StopIteration:\n",
        "        if empty == 'raise':\n",
        "            raise ValueError('Empty mean')\n",
        "        return empty\n",
        "    for n, v in enumerate(l, 2):\n",
        "        acc += v\n",
        "    if n == 1:\n",
        "        return acc\n",
        "    return acc / n\n",
        "\n",
        "\n",
        "class LovaszLoss(nn.Module):\n",
        "    def __init__(self, ignore_index=255, per_image=True):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        return symmetric_lovasz(outputs, targets)\n",
        "\n",
        "class LovaszLossSigmoid(nn.Module):\n",
        "    def __init__(self, ignore_index=255, per_image=True):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.per_image = per_image\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        return lovasz_sigmoid(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n",
        "\n",
        "class FocalLoss2d(nn.Module):\n",
        "    def __init__(self, alpha=0.65, gamma=2,n_parts=BATCH_SIZE):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.n_parts = n_parts\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        n_parts = self.n_parts\n",
        "        outputs = outputs.contiguous()\n",
        "        targets = targets.contiguous()\n",
        "        eps = 1e-6\n",
        "        # non_ignored = targets.view(n_parts, -1) != self.ignore_index\n",
        "        targets = targets.view(n_parts, -1).float()\n",
        "        outputs = outputs.view(n_parts, -1)\n",
        "        # clamp just makes sure the values of the tensor is within the given range.\n",
        "        outputs = torch.clamp(outputs, eps, 1. - eps)\n",
        "        targets = torch.clamp(targets, eps, 1. - eps)\n",
        "        \"\"\" pt = predicted probability for the true class\n",
        "        when targets = 1, pt = outputs which means pt now has the predicted probability of positive class.\n",
        "        when tagets = 0, pt = 1 - outputs which means pt has the predicted probability of negative class.\n",
        "        \"\"\"\n",
        "        pt = (1 - targets) * (1 - outputs) + targets * outputs\n",
        "        alpha_t = self.alpha * targets + (1 - self.alpha)*(1 - targets)\n",
        "        pt_proc = -(alpha_t*((1. - pt) ** self.gamma * torch.log(pt))) # torch.log is natural log\n",
        "        focal_loss = pt_proc.mean(dim=1).mean()\n",
        "        focal_losses.append(focal_loss.item())\n",
        "        return focal_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-y8ovXEK2St"
      },
      "source": [
        "## Config - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQG4nEiq9UOu",
        "outputId": "5d8d805e-0430-4e4f-a909-f035a39a7102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(arch='BAT', gpu='1', net_layer=50, seg_loss=0, pre=0, trans=1, point_pred=1, ppl=6, cross=0)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--arch', type=str, default='BAT')\n",
        "parser.add_argument('--gpu', type=str, default='1')\n",
        "parser.add_argument('--net_layer', type=int, default=50)\n",
        "# parser.add_argument('--dataset', type=str, default='isic2016')\n",
        "# parser.add_argument('--exp_name', type=str, default='')\n",
        "# parser.add_argument('--fold', type=str, default='0')\n",
        "# parser.add_argument('--lr_seg', type=float, default=1e-4)  #0.0003\n",
        "# parser.add_argument('--n_epochs', type=int, default=200)  #100\n",
        "# parser.add_argument('--bt_size', type=int, default=8)  #36\n",
        "parser.add_argument('--seg_loss', type=int, default=0, choices=[0, 1])\n",
        "# parser.add_argument('--aug', type=int, default=1)\n",
        "# parser.add_argument('--patience', type=int, default=10)  #50\n",
        "\n",
        "# pre-train\n",
        "parser.add_argument('--pre', type=int, default=0)\n",
        "\n",
        "# transformer\n",
        "parser.add_argument('--trans', type=int, default=1)\n",
        "\n",
        "# point constrain\n",
        "parser.add_argument('--point_pred', type=int, default=1)\n",
        "parser.add_argument('--ppl', type=int, default=6)\n",
        "\n",
        "# cross-scale framework\n",
        "parser.add_argument('--cross', type=int, default=0)\n",
        "\n",
        "parse_config , unknown = parser.parse_known_args()\n",
        "print(parse_config )\n",
        "\n",
        "# if parse_config.arch == 'BAT':\n",
        "#     parse_config.exp_name += '_{}_{}_{}_e{}'.format(parse_config.trans,\n",
        "#                                                     parse_config.point_pred,\n",
        "#                                                     parse_config.cross,\n",
        "#                                                     parse_config.ppl)\n",
        "# exp_name = parse_config.dataset + '/' + parse_config.exp_name + '_loss_' + str(\n",
        "#     parse_config.seg_loss) + '_aug_' + str(parse_config.aug) + '/fold_' + str(\n",
        "#         parse_config.fold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "hz72Jtwr9UOu"
      },
      "outputs": [],
      "source": [
        "def ce_loss(pred, gt):\n",
        "    pred = torch.clamp(pred, 1e-6, 1 - 1e-6)\n",
        "    return (-gt * torch.log(pred) - (1 - gt) * torch.log(1 - pred)).mean()\n",
        "\n",
        "\n",
        "def structure_loss(pred, mask):\n",
        "    \"\"\"            TransFuse train loss        \"\"\"\n",
        "    \"\"\"            Without sigmoid             \"\"\"\n",
        "    weit = 1 + 5 * torch.abs(\n",
        "        F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
        "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduction='none')\n",
        "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
        "\n",
        "    pred = torch.sigmoid(pred)\n",
        "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
        "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
        "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
        "    return (wbce + wiou).mean()\n",
        "\n",
        "\n",
        "def focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.6,  #0.8\n",
        "    gamma: float = 2,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    p = inputs\n",
        "    ce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"mean\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t)**gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "CRITERION = [focal_loss, ce_loss][parse_config.seg_loss]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SW4ViKfK2St"
      },
      "source": [
        "The [ComboLoss](https://github.com/sneddy/pneumothorax-segmentation/blob/master/unet_pipeline/Losses.py#L104) function used in CRITERION below also comes from the winning solution by Anuar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "g3__XL68K2St"
      },
      "outputs": [],
      "source": [
        "# CRITERION        = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})\n",
        "\n",
        "# # Use During Inference Stage to store images of predicted segmentation masks\n",
        "# # PREDICTION_PATH  = \"/content/drive/MyDrive/Colab_Notebooks/datasets/archive_png_siim_acr/Predicted_masks/tests\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od_pZDUoK2Su"
      },
      "source": [
        "\n",
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JuSwxSqK2Su"
      },
      "source": [
        "General utility functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "3LcOlxiwK2Sv"
      },
      "outputs": [],
      "source": [
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    fig,ax = plt.subplots(figsize=(10,6))\n",
        "    ax.imshow(img.permute(1,2,0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "LkS16buBK2Sv"
      },
      "outputs": [],
      "source": [
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    images = {k:v.numpy() for k,v in images.items() if isinstance(v, torch.Tensor)} #convert tensor to numpy\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    image, mask = images['image'], images['mask']\n",
        "    plt.imshow(image.transpose(1,2,0), vmin=0, vmax=1)\n",
        "    if mask.max()>0:\n",
        "        plt.imshow(mask.squeeze(0), alpha=0.25)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "5KOb07Zlb3GN"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"Saving checkpoint...\")\n",
        "  torch.save(state, filename)\n",
        "  print(\"Checkpoint saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cIcBkgjOcPWQ",
        "outputId": "16a9f8ec-ef55-49b6-d704-de22e0878f08"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef load_checkpoint(checkpoint):\\n  print(\"Loading checkpoint...\")\\n  model.load_state_dict(checkpoint[\\'state_dict\\'])\\n  optimizer.load_state_dict(checkpoint[\\'optimizer\\'])\\n'"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "def load_checkpoint(checkpoint):\n",
        "  print(\"Loading checkpoint...\")\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "NrindR018hyO"
      },
      "outputs": [],
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item() # Calculates where two tensors are equal\n",
        "  acc = (correct / len(y_pred) ) * 100\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqxMQpqB5JFV"
      },
      "source": [
        "# ---------------------- DL Workflow -----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLEHf3QK2Sv"
      },
      "source": [
        "## 1. Data -> Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wbgxfpGK2Sv"
      },
      "source": [
        "### Create five-fold splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDyy5r93K2Sv",
        "outputId": "91c875a1-d860-4ca2-9e1f-1c5f7f8a30ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8570, 2142)"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# single fold training for now, rerun notebook to train for multi-fold\n",
        "DF       = pd.read_csv(DATA_FRAME_PATH)\n",
        "TRAIN_DF = DF.query(f'kfold!={FOLD_ID}').reset_index(drop=True)\n",
        "VAL_DF   = DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)\n",
        "len(TRAIN_DF), len(VAL_DF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njKNWpqFK2Sw"
      },
      "source": [
        "### Dataset and DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Boundary-Supervised Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdU-NOG0G2yq",
        "outputId": "280e82eb-d5b1-4e4c-ef2a-2ec8501e814f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-111-f662d1877941>:13: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform\n",
            "  albu.ElasticTransform(alpha=120, sigma=6.0, alpha_affine=10, p=0.5),\n",
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Define Joint Transformations (applied to both image and mask)\n",
        "# -----------------------------------------------------------------------------\n",
        "# These transforms include brightness/contrast adjustment, gamma correction,\n",
        "# elastic transformation, grid distortion, optical distortion, and affine shifts.\n",
        "joint_tfms = albu.Compose(\n",
        "    [\n",
        "        albu.OneOf([\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.OneOf([\n",
        "            albu.ElasticTransform(alpha=120, sigma=6.0, alpha_affine=10, p=0.5),\n",
        "            albu.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), p=0.5),\n",
        "            albu.OpticalDistortion(distort_limit=(-2, 2), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45,\n",
        "                              border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
        "        albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "    ],\n",
        "    # Here \"mask\" is declared as an additional target so that it receives the same augmentations.\n",
        "    additional_targets={\"mask\": \"mask\"}\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Define Image-Only Transformations\n",
        "# -----------------------------------------------------------------------------\n",
        "# Since the image is already scaled in [0, 1], we need to use max_pixel_value=1.0.\n",
        "# Normalize will only be applied on the image.\n",
        "image_tfms = albu.Compose([\n",
        "    albu.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                   std=[0.229, 0.224, 0.225],\n",
        "                   max_pixel_value=1.0,  # Use 1.0 because the image is in [0, 1]\n",
        "                   p=1.0),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Define Mask-Only Transformations\n",
        "# -----------------------------------------------------------------------------\n",
        "# For masks, we usually just need to convert them to tensors without normalization.\n",
        "mask_tfms = albu.Compose([\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Combined Function to Process Sample\n",
        "# -----------------------------------------------------------------------------\n",
        "def transform_sample(image, mask):\n",
        "    \"\"\"\n",
        "    Apply the joint transformations to both image and mask, then apply\n",
        "    image-specific normalization and tensor conversion only on the image,\n",
        "    and tensor conversion on the mask.\n",
        "    \"\"\"\n",
        "    # Apply joint transformations so that image and mask stay aligned.\n",
        "    sample = joint_tfms(image=image, mask=mask)\n",
        "    image_trans = sample['image']\n",
        "    mask_trans = sample['mask']\n",
        "\n",
        "    # Now, normalize image and convert both to tensor.\n",
        "    image_final = image_tfms(image=image_trans)['image']\n",
        "    mask_final = mask_tfms(image=mask_trans)['image']\n",
        "\n",
        "    return image_final, mask_final\n",
        "\n",
        "# Test transforms\n",
        "TEST_TFMS = albu.Compose([\n",
        "    albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1),\n",
        "    albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=1.0, p=1.0),\n",
        "    ToTensorV2(),\n",
        "],\n",
        "    is_check_shapes=True,\n",
        "    p=1.0,\n",
        ")\n",
        "\n",
        "# New Train Transforms - Aggressive Augmentations to avoid overfitting.\n",
        "TFMS =  albu.Compose(\n",
        "    [\n",
        "        albu.OneOf([\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.OneOf([\n",
        "            albu.ElasticTransform(alpha=120, sigma=6.0, p=0.5),\n",
        "            albu.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), p=0.5),\n",
        "            albu.OpticalDistortion(distort_limit=(-2, 2), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, p=0.5),\n",
        "        albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "        albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=1.0, p=1.0),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    p=1.0\n",
        ")\n",
        "\n",
        "MASK_TFMS =  albu.Compose(\n",
        "    [\n",
        "        albu.OneOf([\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.OneOf([\n",
        "            albu.ElasticTransform(alpha=120, sigma=6.0, p=0.5),\n",
        "            albu.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), p=0.5),\n",
        "            albu.OpticalDistortion(distort_limit=(-2, 2), p=0.5),\n",
        "        ], p=0.3),\n",
        "        albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, p=0.5),\n",
        "        albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    p=1.0\n",
        ")\n",
        "\n",
        "mask_transform = albu.Compose([\n",
        "    albu.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
        "    ToTensorV2()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "9ywRGbXTK2Sw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Key-Patch Map Generation\n",
        "# =============================================================================\n",
        "\n",
        "def resize_and_clip(img, target_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Resize image or mask to target_size using nearest neighbor interpolation\n",
        "    and clip its pixel values to [0, 255].\n",
        "    \"\"\"\n",
        "    resized = cv2.resize(img, target_size, interpolation=cv2.INTER_NEAREST)\n",
        "    resized = np.clip(resized, 0, 255)\n",
        "    return resized\n",
        "\n",
        "def draw_msra_gaussian(heatmap, center, sigma):\n",
        "    \"\"\"\n",
        "    Draw a gaussian blob onto the heatmap centered at `center` with standard deviation `sigma`.\n",
        "    \"\"\"\n",
        "    tmp_size = sigma * 3\n",
        "    mu_x = int(center[0] + 0.5)\n",
        "    mu_y = int(center[1] + 0.5)\n",
        "    h, w = heatmap.shape[0], heatmap.shape[1]\n",
        "    ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
        "    br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
        "    if ul[0] >= w or ul[1] >= h or br[0] < 0 or br[1] < 0:\n",
        "        return heatmap\n",
        "    size = 2 * tmp_size + 1\n",
        "    x = np.arange(0, size, 1, np.float32)\n",
        "    y = x[:, np.newaxis]\n",
        "    x0 = y0 = size // 2\n",
        "    g = np.exp(-((x - x0)**2 + (y - y0)**2) / (2 * sigma**2))\n",
        "    g_x = max(0, -ul[0]), min(br[0], w) - ul[0]\n",
        "    g_y = max(0, -ul[1]), min(br[1], h) - ul[1]\n",
        "    img_x = max(0, ul[0]), min(br[0], w)\n",
        "    img_y = max(0, ul[1]), min(br[1], h)\n",
        "    heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]] = np.maximum(\n",
        "        heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]],\n",
        "        g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
        "    )\n",
        "    return heatmap\n",
        "\n",
        "def compute_key_patch_map_dt(mask_orig, sigma=8, window_size=3, min_distance=1):\n",
        "    \"\"\"\n",
        "    Compute the key-patch (ground truth point) map using Distance Transform and Local Maxima Detection.\n",
        "\n",
        "    Steps:\n",
        "      1. Resize the input mask to 512x512.\n",
        "      2. Threshold the resized mask to obtain a binary mask.\n",
        "      3. Compute the Euclidean distance transform of the binary mask.\n",
        "      4. Detect local maxima via a simple non-maximum suppression approach (using dilation).\n",
        "      5. Draw a Gaussian blob on a heatmap at each detected local maximum.\n",
        "\n",
        "    Returns a 512x512 heatmap representing the key patch map.\n",
        "    \"\"\"\n",
        "    # Resize the original mask to 512x512\n",
        "    mask_resized = resize_and_clip(mask_orig, (512, 512))\n",
        "\n",
        "    # Threshold mask to binary (pixels > 127 become foreground)\n",
        "    _, mask_bin = cv2.threshold(mask_resized, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Ensure binary mask is in uint8 format for distanceTransform\n",
        "    mask_bin_uint8 = np.uint8(mask_bin)\n",
        "\n",
        "    # Compute the Euclidean distance transform\n",
        "    dt = cv2.distanceTransform(mask_bin_uint8, cv2.DIST_L2, 5)\n",
        "\n",
        "    # Detect local maxima using dilation.\n",
        "    # Create a kernel (window) for the non-maximum suppression.\n",
        "    kernel = np.ones((window_size, window_size), dtype=np.float32)\n",
        "    dt_dilated = cv2.dilate(dt, kernel)\n",
        "\n",
        "    # A pixel is a local maximum if it equals the dilated version and exceeds a minimal distance.\n",
        "    local_max = (dt == dt_dilated) & (dt > min_distance)\n",
        "\n",
        "    # Initialize the heatmap of size 512x512\n",
        "    point_heatmap = np.zeros((512, 512), dtype=np.float32)\n",
        "\n",
        "    # Retrieve the coordinates of the local maxima.\n",
        "    # np.argwhere returns (row, col) corresponding to (y, x)\n",
        "    coords = np.argwhere(local_max)\n",
        "\n",
        "    # Draw a Gaussian at each detected local maximum on the heatmap.\n",
        "    for pt in coords:\n",
        "        y, x = pt  # Note: (y, x) order from np.argwhere\n",
        "        point_heatmap = draw_msra_gaussian(point_heatmap, (x, y), sigma)\n",
        "\n",
        "    return point_heatmap\n",
        "\n",
        "# =============================================================================\n",
        "# Modified Dataset Class (Using Distance Transform Approach)\n",
        "# =============================================================================\n",
        "\n",
        "class Dataset():\n",
        "    def __init__(self, rle_df, image_base_dir, masks_base_dir, augmentation=None, mask_augmentation=None):\n",
        "        self.df                 = rle_df\n",
        "        self.image_base_dir     = image_base_dir\n",
        "        self.masks_base_dir     = masks_base_dir\n",
        "        self.image_ids          = rle_df.ImageId.values\n",
        "        self.augmentation       = augmentation\n",
        "        self.mask_augmentation  = mask_augmentation\n",
        "\n",
        "    def __image_ids__(self):\n",
        "        print(self.image_ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image_id  = self.image_ids[i]\n",
        "        img_path  = os.path.join(self.image_base_dir, Path(image_id + '.png'))\n",
        "        mask_path = os.path.join(self.masks_base_dir, Path(image_id + '.png'))\n",
        "\n",
        "        # Load image and mask using OpenCV.\n",
        "        image     = cv2.imread(img_path, 1)\n",
        "        mask      = cv2.imread(mask_path, 0)\n",
        "\n",
        "        # Normalize image (scale to [0, 1]) and convert mask to binary.\n",
        "        image = (image / 255.0).astype(np.float32)\n",
        "        mask = (mask > 0).astype(np.float32)\n",
        "\n",
        "        # Generate the ground truth key-patch map using the distance transform approach.\n",
        "        # Multiply mask by 255 if required by the helper functions.\n",
        "        point = compute_key_patch_map_dt(mask * 255)\n",
        "\n",
        "        # Apply the transformations.\n",
        "        image, mask = transform_sample(image, mask)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'mask' : mask,\n",
        "            'point': torch.unsqueeze(torch.Tensor(point), axis=0)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "4DKpUR1QK2Sw"
      },
      "outputs": [],
      "source": [
        "# train dataset\n",
        "train_dataset = Dataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TFMS, MASK_TFMS)\n",
        "val_dataset   = Dataset(VAL_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TEST_TFMS, mask_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zno_0Q4eVHv",
        "outputId": "b7277a3d-a099-403b-94e7-88b959ec6458"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8570"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "8SW1GpbQK2Sw"
      },
      "outputs": [],
      "source": [
        "# # plot one with mask\n",
        "# visualize(**train_dataset[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-dcysMmK2Sw"
      },
      "source": [
        "### Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "Qx49nrt4K2Sw"
      },
      "outputs": [],
      "source": [
        "class PneumoSampler(Sampler):\n",
        "    def __init__(self, train_df, positive_perc=0.8):\n",
        "        assert positive_perc > 0, 'percentage of positive pneumothorax images must be greater then zero'\n",
        "        self.train_df = train_df\n",
        "        self.positive_perc = positive_perc\n",
        "        self.positive_idxs = self.train_df.query('has_mask==1').index.values\n",
        "        self.negative_idxs = self.train_df.query('has_mask!=1').index.values\n",
        "        self.n_positive = len(self.positive_idxs)\n",
        "        self.n_negative = int(self.n_positive * (1 - self.positive_perc) / self.positive_perc)\n",
        "\n",
        "    def __iter__(self):\n",
        "        negative_sample = np.random.choice(self.negative_idxs, size=self.n_negative)\n",
        "        shuffled = np.random.permutation(np.hstack((negative_sample, self.positive_idxs)))\n",
        "        return iter(shuffled.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_positive + self.n_negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "kzJerUgtK2Sx"
      },
      "outputs": [],
      "source": [
        "SAMPLER = PneumoSampler(TRAIN_DF, positive_perc=POSTIVE_PERC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asd-jMF9K2Sx"
      },
      "source": [
        "### DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Saav3bv6K2Sx",
        "outputId": "dca3cc25-47e3-4001-9971-514f0c5b0932"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE,\n",
        "                              shuffle=True if not USE_SAMPLER else False,\n",
        "                              num_workers=NUM_WORKERS,\n",
        "                              sampler=SAMPLER if USE_SAMPLER else None)\n",
        "val_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9nGVmHlj6z5",
        "outputId": "eec9651a-f6f3-4688-d054-96e4055925c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x795b35361ad0>, <torch.utils.data.dataloader.DataLoader object at 0x795a14533b90>)\n",
            "Length of train dataloader: 298 batches of 8\n",
            "length of validation dataloader: 268 batches of 8\n"
          ]
        }
      ],
      "source": [
        "print(f\"Dataloaders: {train_dataloader , val_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"length of validation dataloader: {len(val_dataloader)} batches of {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jmBiDQJwFTN"
      },
      "source": [
        "for batch_index, data in enumerate(train_dataloader):\n",
        "    for z in range(3):\n",
        "        print(\"Image: \", data['image'].shape)\n",
        "        print(\"Mask: \", data['mask'].shape)\n",
        "        print(\"data['mask'].unsqueeze(1)\", data['mask'].unsqueeze(1).shape)\n",
        "    break\n",
        "\n",
        "Output\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])\n",
        "Image:  torch.Size([8, 3, 512, 512])\n",
        "Mask:  torch.Size([8, 1, 512, 512])\n",
        "data['mask'].unsqueeze(1) torch.Size([8, 1, 1, 512, 512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "D4Wrs4Z4D6dj",
        "outputId": "a52142cc-114b-49ec-db60-ac964736496f"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-4b585b43f4ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'point'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# for data in train_dataloader:\n",
        "#   continue\n",
        "#   image = data['image']\n",
        "#   mask = data['mask']\n",
        "#   point = data['point']\n",
        "#   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHpinHeKHNIe"
      },
      "outputs": [],
      "source": [
        "# fig, axs = plt.subplots(4, 2, figsize=(20, 20))  # axs is a 2D array with shape (4, 2)\n",
        "\n",
        "# # Row 0\n",
        "# axs[0, 0].imshow(point[0][0])\n",
        "# axs[0, 1].imshow(mask[0][0])\n",
        "\n",
        "# # Row 1\n",
        "# axs[1, 0].imshow(point[1][0])\n",
        "# axs[1, 1].imshow(mask[1][0])\n",
        "\n",
        "# # Row 2\n",
        "# axs[2, 0].imshow(point[2][0])\n",
        "# axs[2, 1].imshow(mask[2][0])\n",
        "\n",
        "# # Row 3\n",
        "# axs[3, 0].imshow(point[3][0])\n",
        "# axs[3, 1].imshow(mask[3][0])\n",
        "\n",
        "# plt.tight_layout()  # Adjust spacing between subplots\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPWAK3BK9UPJ"
      },
      "source": [
        "## NonBlockND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "EJdpZrHW9UPJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class _NonLocalBlockND(nn.Module):\n",
        "    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n",
        "        \"\"\"\n",
        "        :param in_channels:\n",
        "        :param inter_channels:\n",
        "        :param dimension:\n",
        "        :param sub_sample:\n",
        "        :param bn_layer:\n",
        "        \"\"\"\n",
        "\n",
        "        super(_NonLocalBlockND, self).__init__()\n",
        "\n",
        "        assert dimension in [1, 2, 3]\n",
        "\n",
        "        self.dimension = dimension\n",
        "        self.sub_sample = sub_sample\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.inter_channels = inter_channels\n",
        "\n",
        "        if self.inter_channels is None:\n",
        "            self.inter_channels = in_channels // 2\n",
        "            if self.inter_channels == 0:\n",
        "                self.inter_channels = 1\n",
        "\n",
        "        if dimension == 3:\n",
        "            conv_nd = nn.Conv3d\n",
        "            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "            bn = nn.BatchNorm3d\n",
        "        elif dimension == 2:\n",
        "            conv_nd = nn.Conv2d\n",
        "            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "            bn = nn.BatchNorm2d\n",
        "        else:\n",
        "            conv_nd = nn.Conv1d\n",
        "            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n",
        "            bn = nn.BatchNorm1d\n",
        "\n",
        "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                         kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        if bn_layer:\n",
        "            self.W = nn.Sequential(\n",
        "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
        "                        kernel_size=1, stride=1, padding=0),\n",
        "                bn(self.in_channels)\n",
        "            )\n",
        "            nn.init.constant_(self.W[1].weight, 0)\n",
        "            nn.init.constant_(self.W[1].bias, 0)\n",
        "        else:\n",
        "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
        "                             kernel_size=1, stride=1, padding=0)\n",
        "            nn.init.constant_(self.W.weight, 0)\n",
        "            nn.init.constant_(self.W.bias, 0)\n",
        "\n",
        "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                             kernel_size=1, stride=1, padding=0)\n",
        "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                           kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        if sub_sample:\n",
        "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
        "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
        "\n",
        "    def forward(self, x, y, return_nl_map=False):\n",
        "        \"\"\"\n",
        "        :param x: (b, c, h, w)\n",
        "        :param y: (b, c, 1)\n",
        "        :param return_nl_map: if True return z, nl_map, else only return z.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        h, w = x.shape[2:]\n",
        "\n",
        "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
        "        #g_x = g_x.permute(0, 2, 1)\n",
        "\n",
        "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
        "        theta_x = theta_x.permute(0, 2, 1)\n",
        "\n",
        "        #phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
        "        phi_x = self.phi(y.unsqueeze(-1)).view(batch_size, self.inter_channels, -1)\n",
        "        f = torch.matmul(theta_x, phi_x)\n",
        "        #f_div_C = F.softmax(f, dim=-1)\n",
        "        f_div_C = torch.sigmoid(f)\n",
        "        f_div_C = f_div_C.permute(0,2,1).contiguous()\n",
        "\n",
        "        #y = torch.matmul(f_div_C, g_x)\n",
        "        #y = y.permute(0, 2, 1).contiguous()\n",
        "        y = g_x * f_div_C\n",
        "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
        "        W_y = self.W(y)\n",
        "        z = W_y + x\n",
        "\n",
        "        if return_nl_map:\n",
        "            return z, f_div_C.view(batch_size, 1, h, w)\n",
        "        return z\n",
        "\n",
        "class NONLocalBlock2D(_NonLocalBlockND):\n",
        "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
        "        super(NONLocalBlock2D, self).__init__(in_channels,\n",
        "                                              inter_channels=inter_channels,\n",
        "                                              dimension=2, sub_sample=sub_sample,\n",
        "                                              bn_layer=bn_layer,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rpAsnr89UPL"
      },
      "source": [
        "## BAT utility modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "_mqFXmim9UPL"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=512,\n",
        "                 dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "\n",
        "    def forward(self, tgt, src):\n",
        "        \"tgt shape: Batch_size, C, H, W \"\n",
        "        \"src shape: Batch_size, 1, C    \"\n",
        "\n",
        "        B, C, h, w = tgt.shape\n",
        "        tgt = tgt.view(B, C, h*w).permute(2,0,1)  # shape: L, B, C\n",
        "\n",
        "        src = src.permute(1,0,2)  # shape: Q:1, B, C\n",
        "\n",
        "        fusion_feature = self.cross_attn(query=tgt,\n",
        "                                         key=src,\n",
        "                                         value=src)[0]\n",
        "        tgt = tgt + self.dropout1(fusion_feature)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt1 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout2(tgt1)\n",
        "        tgt = self.norm2(tgt)\n",
        "        return tgt.permute(1, 2, 0).view(B, C, h, w)\n",
        "\n",
        "class BoundaryCrossAttention(CrossAttention):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=True,\n",
        "                 dim_feedforward=512,\n",
        "                 dropout=0.0):\n",
        "        super().__init__(d_model, nhead, dim_feedforward, dropout)\n",
        "\n",
        "        #self.BAG = nn.Sequential(\n",
        "        #    nn.Conv2d(d_model, d_model, kernel_size=3, padding=1, bias=False),\n",
        "        #    nn.BatchNorm2d(d_model),\n",
        "        #    nn.ReLU(inplace=False),\n",
        "        #    nn.Conv2d(d_model, d_model, kernel_size=3, padding=1, bias=False),\n",
        "        #    nn.BatchNorm2d(d_model),\n",
        "        #    nn.ReLU(inplace=False),\n",
        "        #    nn.Conv2d(d_model, 1, kernel_size=1))\n",
        "        self.BAG_type = BAG_type\n",
        "        if self.BAG_type == '1D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous1D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate1D(d_model)\n",
        "        elif self.BAG_type == '2D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous2D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate2D(d_model)\n",
        "\n",
        "    def forward(self, tgt, src):\n",
        "        \"tgt shape: Batch_size, C, H, W \"\n",
        "        \"src shape: Batch_size, 1, C    \"\n",
        "\n",
        "        B, C, h, w = tgt.shape\n",
        "        tgt = tgt.view(B, C, h*w).permute(2,0,1)  # shape: L, B, C\n",
        "\n",
        "        src = src.permute(1,0,2)  # shape: Q:1, B, C\n",
        "\n",
        "        fusion_feature = self.cross_attn(query=tgt,\n",
        "                                         key=src,\n",
        "                                         value=src)[0]\n",
        "        tgt = tgt + self.dropout1(fusion_feature)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt1 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout2(tgt1)\n",
        "        tgt = self.norm2(tgt)\n",
        "\n",
        "        if self.BAG_type == '1D':\n",
        "            tgt = tgt.permute(1,2,0)\n",
        "            tgt, weights = self.BAG(tgt)\n",
        "            tgt = tgt.view(B, C, h, w).contiguous()\n",
        "            weights = weights.view(B, 1, h, w)\n",
        "        elif self.BAG_type == '2D':\n",
        "            tgt = tgt.permute(1,2,0).view(B, C, h, w)\n",
        "            tgt, weights = self.BAG(tgt)\n",
        "            tgt = tgt.contiguous()\n",
        "        return tgt, weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements a multi head attention module like proposed in:\n",
        "    https://arxiv.org/abs/2005.12872\n",
        "    \"\"\"\n",
        "    def __init__(self, query_dimension: int = 64, hidden_features: int = 64, number_of_heads: int = 16,\n",
        "                 dropout: float = 0.0) -> None:\n",
        "        \"\"\"\n",
        "        Constructor method\n",
        "        :param query_dimension: (int) Dimension of query tensor\n",
        "        :param hidden_features: (int) Number of hidden features in detr\n",
        "        :param number_of_heads: (int) Number of prediction heads\n",
        "        :param dropout: (float) Dropout factor to be utilized\n",
        "        \"\"\"\n",
        "        # Call super constructor\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Save parameters\n",
        "        self.hidden_features = hidden_features\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.dropout = dropout\n",
        "        # Init layer\n",
        "        self.layer_box_embedding = nn.Linear(in_features=query_dimension, out_features=hidden_features, bias=True)\n",
        "        # Init convolution layer\n",
        "        self.layer_image_encoding = nn.Conv2d(in_channels=query_dimension, out_channels=hidden_features,\n",
        "                                              kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=True)\n",
        "        # Init normalization factor\n",
        "        self.normalization_factor = torch.tensor(self.hidden_features / self.number_of_heads, dtype=torch.float).sqrt()\n",
        "\n",
        "        # Linear\n",
        "        self.linear = nn.Linear(in_features=number_of_heads, out_features=1)\n",
        "\n",
        "    def forward(self, input_box_embeddings: torch.Tensor, input_image_encoding: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        :param input_box_embeddings: (torch.Tensor) Bounding box embeddings\n",
        "        :param input_image_encoding: (torch.Tensor) Encoded image of the transformer encoder\n",
        "        :return: (torch.Tensor) Attention maps of shape (batch size, n, m, height, width)\n",
        "        \"\"\"\n",
        "        # Map box embeddings\n",
        "        output_box_embeddings = self.layer_box_embedding(input_box_embeddings)\n",
        "        # Map image features\n",
        "        output_image_encoding = self.layer_image_encoding(input_image_encoding)\n",
        "        # Reshape output box embeddings\n",
        "        output_box_embeddings = output_box_embeddings.view(output_box_embeddings.shape[0],\n",
        "                                                           output_box_embeddings.shape[1],\n",
        "                                                           self.number_of_heads,\n",
        "                                                           self.hidden_features // self.number_of_heads)\n",
        "        # Reshape output image encoding\n",
        "        output_image_encoding = output_image_encoding.view(output_image_encoding.shape[0],\n",
        "                                                           self.number_of_heads,\n",
        "                                                           self.hidden_features // self.number_of_heads,\n",
        "                                                           output_image_encoding.shape[-2],\n",
        "                                                           output_image_encoding.shape[-1])\n",
        "        # Combine tensors and normalize\n",
        "        output = torch.einsum(\"bqnc,bnchw->bqnhw\",\n",
        "                              output_box_embeddings * self.normalization_factor,\n",
        "                              output_image_encoding)\n",
        "        # Apply softmax\n",
        "        output = F.softmax(output.flatten(start_dim=2), dim=-1).view_as(output)\n",
        "\n",
        "        # Linear: to generate one map\n",
        "        b, _, _, h, w = output.shape\n",
        "        output = torch.sigmoid(self.linear(output.flatten(start_dim=3).permute(0,1,3,2))).view(b,1,h,w)\n",
        "\n",
        "        # Perform dropout if utilized\n",
        "        if self.dropout > 0.0:\n",
        "            output = F.dropout(input=output, p=self.dropout, training=self.training)\n",
        "#         print(\"MultiHead Attention\",output.shape)\n",
        "        return output.contiguous()\n",
        "\n",
        "\n",
        "class BoundaryWiseAttentionGateAtrous2D(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "\n",
        "        super(BoundaryWiseAttentionGateAtrous2D,self).__init__()\n",
        "\n",
        "        modules = []\n",
        "\n",
        "        if hidden_channels == None:\n",
        "            hidden_channels = in_channels // 2\n",
        "\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=1, dilation=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=2, dilation=2, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=4, dilation=4, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=6, dilation=6, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "\n",
        "        self.convs = nn.ModuleList(modules)\n",
        "\n",
        "        self.conv_out = nn.Conv2d(5 * hidden_channels, 1, 1, bias=False)\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, H, W \"\n",
        "        \" return: feature, weight (B,C,H,W) \"\n",
        "        res = []\n",
        "        for conv in self.convs:\n",
        "            res.append(conv(x))\n",
        "        res = torch.cat(res, dim=1)\n",
        "        weight = torch.sigmoid(self.conv_out(res))\n",
        "        x = x * weight + x\n",
        "        return x, weight\n",
        "\n",
        "class BoundaryWiseAttentionGateAtrous1D(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "\n",
        "        super(BoundaryWiseAttentionGateAtrous1D,self).__init__()\n",
        "\n",
        "        modules = []\n",
        "\n",
        "        if hidden_channels == None:\n",
        "            hidden_channels = in_channels // 2\n",
        "\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 1, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=1, dilation=1, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=2, dilation=2, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=4, dilation=4, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv1d(in_channels, hidden_channels, 3, padding=6, dilation=6, bias=False),\n",
        "            nn.BatchNorm1d(hidden_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "\n",
        "        self.convs = nn.ModuleList(modules)\n",
        "\n",
        "        self.conv_out = nn.Conv1d(5 * hidden_channels, 1, 1, bias=False)\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, L \"\n",
        "        \" return: feature, weight (B,C,L) \"\n",
        "        res = []\n",
        "        for conv in self.convs:\n",
        "            res.append(conv(x))\n",
        "        res = torch.cat(res, dim=1)\n",
        "        weight = torch.sigmoid(self.conv_out(res))\n",
        "        x = x * weight + x\n",
        "        return x, weight\n",
        "\n",
        "class BoundaryWiseAttentionGate2D(nn.Sequential):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "        super(BoundaryWiseAttentionGate2D,self).__init__(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels, 1, kernel_size=1))\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, H, W \"\n",
        "        \" return: feature, weight (B,C,H,W) \"\n",
        "        weight = torch.sigmoid(super(BoundaryWiseAttentionGate2D,self).forward(x))\n",
        "        x = x * weight + x\n",
        "        return x, weight\n",
        "\n",
        "class BoundaryWiseAttentionGate1D(nn.Sequential):\n",
        "    def __init__(self, in_channels, hidden_channels = None):\n",
        "        super(BoundaryWiseAttentionGate1D,self).__init__(\n",
        "            nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(in_channels),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv1d(in_channels, 1, kernel_size=1))\n",
        "    def forward(self, x):\n",
        "        \" x.shape: B, C, L \"\n",
        "        \" return: feature, weight (B,C,L) \"\n",
        "        weight = torch.sigmoid(super(BoundaryWiseAttentionGate1D,self).forward(x))\n",
        "        x = x * weight + x\n",
        "        return x, weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wyBin1-klDc"
      },
      "source": [
        "## transformer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7bRaHbhklDd"
      },
      "outputs": [],
      "source": [
        "# Based on: https://github.com/facebookresearch/detr/blob/master/models/transformer.py\n",
        "import copy\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers,\n",
        "                                          encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        #         print(\"Trans Encoder\",memory.shape)\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers=6,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = BoundaryAwareTransformerEncoderLayer(\n",
        "            d_model, nhead, BAG_type, Atrous, dim_feedforward, dropout,\n",
        "            activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = BoundaryAwareTransformerEncoder(point_pred_layers,\n",
        "                                                       encoder_layer,\n",
        "                                                       num_encoder_layers,\n",
        "                                                       encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory, weights = self.encoder(src,\n",
        "                                       src_key_padding_mask=mask,\n",
        "                                       pos=pos_embed,\n",
        "                                       height=h,\n",
        "                                       width=w)\n",
        "\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h,\n",
        "                                                                w), weights\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask,\n",
        "                           pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers,\n",
        "                 encoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None):\n",
        "        super().__init__()\n",
        "        self.point_pred_layers = point_pred_layers\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        output = src\n",
        "        weights = []\n",
        "\n",
        "        for layer_i, layer in enumerate(self.layers):\n",
        "            if layer_i > self.num_layers - self.point_pred_layers - 1:\n",
        "                output, weight = layer(\n",
        "                    True,\n",
        "                    output,\n",
        "                    src_mask=mask,\n",
        "                    src_key_padding_mask=src_key_padding_mask,\n",
        "                    pos=pos,\n",
        "                    height=height,\n",
        "                    width=width)\n",
        "                weights.append(weight)\n",
        "            else:\n",
        "                output = layer(False,\n",
        "                               output,\n",
        "                               src_mask=mask,\n",
        "                               src_key_padding_mask=src_key_padding_mask,\n",
        "                               pos=pos,\n",
        "                               height=height,\n",
        "                               width=width)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src2,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoderLayer(TransformerEncoderLayer):\n",
        "    \"    Add Boundary-wise Attention Gate to Transformer's Encoder\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=True,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation,\n",
        "                         normalize_before)\n",
        "        if BAG_type == '1D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous1D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate1D(d_model)\n",
        "        elif BAG_type == '2D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous2D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate2D(d_model)\n",
        "        self.BAG_type = BAG_type\n",
        "\n",
        "    def forward(self,\n",
        "                use_bag,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        if self.normalize_before:\n",
        "            features = self.forward_pre(src, src_mask, src_key_padding_mask,\n",
        "                                        pos)\n",
        "            if use_bag:\n",
        "                b, c = features.shape[1:]\n",
        "                if self.BAG_type == '1D':\n",
        "                    features = features.permute(1, 2, 0)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.permute(2, 0, 1).contiguous()\n",
        "                    weights = weights.view(b, 1, height, width)\n",
        "                elif self.BAG_type == '2D':\n",
        "                    features = features.permute(1, 2,\n",
        "                                                0).view(b, c, height, width)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.flatten(2).permute(2, 0,\n",
        "                                                           1).contiguous()\n",
        "                return features, weights\n",
        "            else:\n",
        "                return features\n",
        "        features = self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "        if use_bag:\n",
        "            b, c = features.shape[1:]\n",
        "            if self.BAG_type == '1D':\n",
        "                features = features.permute(1, 2, 0)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.permute(2, 0, 1).contiguous()\n",
        "                weights = weights.view(b, 1, height, width)\n",
        "            elif self.BAG_type == '2D':\n",
        "                features = features.permute(1, 2, 0).view(b, c, height, width)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.flatten(2).permute(2, 0, 1).contiguous()\n",
        "            return features, weights\n",
        "        else:\n",
        "            return features\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 decoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None,\n",
        "                 return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           memory,\n",
        "                           tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos,\n",
        "                           query_pos=query_pos)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model,\n",
        "                                                    nhead,\n",
        "                                                    dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     tgt,\n",
        "                     memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    tgt,\n",
        "                    memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt2,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask,\n",
        "                                    memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask,\n",
        "                                 pos, query_pos)\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args.hidden_dim,\n",
        "        dropout=args.dropout,\n",
        "        nhead=args.nheads,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        num_encoder_layers=args.enc_layers,\n",
        "        num_decoder_layers=args.dec_layers,\n",
        "        normalize_before=args.pre_norm,\n",
        "        return_intermediate_dec=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"leaky relu\":\n",
        "        return F.leaky_relu\n",
        "    if activation == \"selu\":\n",
        "        return F.selu\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(\n",
        "        F\"activation should be relu, gelu, glu, leaky relu or selu, not {activation}.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLaynkwn9UPM"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "vsyy9B33klD8"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50, resnet34, resnet18, ResNet50_Weights, ResNet18_Weights, ResNet34_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "rlV1_wDH9UPM"
      },
      "outputs": [],
      "source": [
        "# For ResNet18, use layers up to layer3 so that the output has OS16.\n",
        "def ResNet18_OS16(multi_scale=False):\n",
        "    resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "    # Original resnet18 children:\n",
        "    # [conv1, bn1, relu, maxpool, layer1, layer2, layer3, layer4, avgpool, fc]\n",
        "    # Removing layer4, avgpool and fc gives a feature map of size roughly (B, 256, H/16, W/16)\n",
        "    features = nn.Sequential(*list(resnet.children())[:-3])\n",
        "    return features\n",
        "\n",
        "# For ResNet50, adjust layer4 to preserve OS16.\n",
        "def ResNet50_OS16(multi_scale=False):\n",
        "    resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "    # The children of resnet50 are:\n",
        "    # [conv1, bn1, relu, maxpool, layer1, layer2, layer3, layer4, avgpool, fc]\n",
        "    # To achieve OS16 instead of OS32, modify layer4 so that it does not downsample:\n",
        "    #  - Change the stride in the first Bottleneck of layer4 from 2 to 1.\n",
        "    resnet.layer4[0].conv2.stride = (1, 1)\n",
        "    if resnet.layer4[0].downsample:  # adjust the downsampling layer as well\n",
        "        resnet.layer4[0].downsample[0].stride = (1, 1)\n",
        "    # Additionally, increase dilation in layer4 so that its receptive field remains large.\n",
        "    for m in resnet.layer4.modules():\n",
        "        if isinstance(m, nn.Conv2d) and m.kernel_size == (3, 3):\n",
        "            m.dilation = (2, 2)\n",
        "            m.padding = (2, 2)\n",
        "    # Now remove the avgpool and fc layers.\n",
        "    features = nn.Sequential(*list(resnet.children())[:-2])\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFIKfzTH9UPN"
      },
      "source": [
        "## Atrous Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "KhAMspTBQ0N4"
      },
      "outputs": [],
      "source": [
        "# camera-ready\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, num_classes, head = True):\n",
        "        super(ASPP, self).__init__()\n",
        "\n",
        "        self.conv_1x1_1 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.conv_1x1_2 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        if head:\n",
        "            self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "        self.head = head\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # (feature_map has shape (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16. If self.resnet instead is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8))\n",
        "        #print(f\"Shape of feature_map: {feature_map.shape}\")\n",
        "\n",
        "        feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "        feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out_1x1: {out_1x1.shape}\")\n",
        "        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out_3x3_1: {out_3x3_1.shape}\")\n",
        "        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out_3x3_2: {out_3x3_2.shape}\")\n",
        "        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out_3x3_3: {out_3x3_3.shape}\")\n",
        "\n",
        "        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "        #print(f\"Shape of out_img after avg_pool: {out_img.shape}\")\n",
        "        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "        #print(f\"Shape of out_img after conv_1x1_2: {out_img.shape}\")\n",
        "        out_img = F.interpolate(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out_img after interpolate: {out_img.shape}\")\n",
        "\n",
        "        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "        #print(f\"Shape of out after concatenation: {out.shape}\")\n",
        "        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out after conv_1x1_3: {out.shape}\")\n",
        "        if self.head:\n",
        "            out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "            #print(f\"Shape of out after conv_1x1_4 (head): {out.shape}\")\n",
        "\n",
        "        return out\n",
        "\n",
        "class ASPP_Bottleneck(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ASPP_Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv_1x1_1 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_1 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_2 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_3 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.conv_1x1_2 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # (feature_map has shape (batch_size, 4*512, h/16, w/16))\n",
        "        #print(f\"Shape of feature_map: {feature_map.shape}\")\n",
        "\n",
        "        feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "        feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out_1x1: {out_1x1.shape}\")\n",
        "        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out_3x3_1: {out_3x3_1.shape}\")\n",
        "        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out_3x3_2: {out_3x3_2.shape}\")\n",
        "        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out_3x3_3: {out_3x3_3.shape}\")\n",
        "\n",
        "        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "        #print(f\"Shape of out_img after avg_pool: {out_img.shape}\")\n",
        "        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "        #print(f\"Shape of out_img after conv_1x1_2: {out_img.shape}\")\n",
        "        out_img = F.interpolate(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out_img after interpolate: {out_img.shape}\")\n",
        "\n",
        "        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "        #print(f\"Shape of out after concatenation: {out.shape}\")\n",
        "        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        #print(f\"Shape of out after conv_1x1_3: {out.shape}\")\n",
        "        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "        #print(f\"Shape of out after conv_1x1_4: {out.shape}\")\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCLcdf0B9UPN"
      },
      "source": [
        "## DeepLabV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "xVGsTLZxklEK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import sys\n",
        "# sys.path.insert(0, '../')\n",
        "\n",
        "# from Ours.resnet import ResNet18_OS16, ResNet34_OS16, ResNet50_OS16, ResNet101_OS16, ResNet152_OS16, ResNet18_OS8, ResNet34_OS8\n",
        "# from Ours.ASPP import ASPP, ASPP_Bottleneck\n",
        "\n",
        "# class DeepLabV3(nn.Module): | \"DeepLabV3\" is called as \"base\" in the BAT class\n",
        "# class base(nn.Module):\n",
        "#     def __init__(self, num_classes, num_layers):\n",
        "#         super(base, self).__init__()\n",
        "\n",
        "#         self.num_classes = num_classes\n",
        "#         layers = num_layers\n",
        "#         # NOTE! specify the type of ResNet here\n",
        "#         # NOTE! if you use ResNet50-152, set self.aspp = ASPP_Bottleneck(num_classes=self.num_classes) instead\n",
        "#         if layers == 18:\n",
        "#             self.resnet = ResNet18_OS16()\n",
        "#             self.aspp = ASPP(num_classes=self.num_classes)\n",
        "#         elif layers == 50:\n",
        "#             self.resnet = ResNet50_OS16()\n",
        "#             self.aspp = ASPP_Bottleneck(num_classes=self.num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # (x has shape (batch_size, 3, h, w))\n",
        "#         h = x.size()[2]\n",
        "#         w = x.size()[3]\n",
        "#         feature_map = self.resnet(x)\n",
        "\n",
        "#         # (shape: (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16.\n",
        "#         # If self.resnet is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8).\n",
        "#         # If self.resnet is ResNet50-152, it will be (batch_size, 4*512, h/16, w/16))\n",
        "#         output = self.aspp(\n",
        "#             feature_map)  # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "#         output = F.upsample(\n",
        "#             output, size=(h, w),\n",
        "#             mode=\"bilinear\")  # (shape: (batch_size, num_classes, h, w))\n",
        "#         return output\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# The \"base\" model acts like a DeepLabV3 backbone.\n",
        "# It instantiates a segmentation network using a ResNet backbone (with OS16)\n",
        "# plus an ASPP module. (Ensure that ASPP and ASPP_Bottleneck are defined/imported.)\n",
        "# -------------------------------------------------------------------\n",
        "class base(nn.Module):\n",
        "    def __init__(self, num_classes, num_layers):\n",
        "        super(base, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        layers = num_layers\n",
        "\n",
        "        if layers == 18:\n",
        "            self.resnet = ResNet18_OS16()\n",
        "            self.aspp = ASPP(num_classes=self.num_classes)  # ASPP for BasicBlock variant\n",
        "        elif layers == 50:\n",
        "            self.resnet = ResNet50_OS16()\n",
        "            self.aspp = ASPP_Bottleneck(num_classes=self.num_classes)  # ASPP_Bottleneck for Bottleneck variant\n",
        "        else:\n",
        "            raise ValueError(\"Only 18 and 50 are supported in this example.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is of shape (batch_size, 3, h, w)\n",
        "        h, w = x.size(2), x.size(3)\n",
        "        feature_map = self.resnet(x)  # Expected shape: [B, C, h/16, w/16]\n",
        "        # Process through the ASPP module:\n",
        "        output = self.aspp(feature_map)  # Expected shape: (batch_size, num_classes, h/16, w/16)\n",
        "        # Upsample the output to original image resolution.\n",
        "        output = F.interpolate(output, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZWZ7jTw9UPO"
      },
      "source": [
        "## Base Transformer (BAT class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "N6jR8NwS9UPO"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "\n",
        "# root_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..')\n",
        "# sys.path.insert(0, os.path.join(root_path))\n",
        "# sys.path.insert(0, os.path.join(root_path, 'lib'))\n",
        "# sys.path.insert(0, os.path.join(root_path, 'lib/Cell_DETR_master'))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from Ours.base import DeepLabV3 as base\n",
        "\n",
        "# from src.BAT_Modules import BoundaryCrossAttention, CrossAttention\n",
        "# from src.BAT_Modules import MultiHeadAttention as Attention_head\n",
        "# from src.transformer import BoundaryAwareTransformer, Transformer\n",
        "\n",
        "\n",
        "class BAT(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_classes,\n",
        "            num_layers,\n",
        "            point_pred,\n",
        "            decoder=False,\n",
        "            transformer_type_index=0,\n",
        "            hidden_features=128,  # 256\n",
        "            number_of_query_positions=1,\n",
        "            segmentation_attention_heads=8):\n",
        "\n",
        "        super(BAT, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.point_pred = point_pred\n",
        "        self.transformer_type = \"BoundaryAwareTransformer\" if transformer_type_index == 0 else \"Transformer\"\n",
        "        self.use_decoder = decoder\n",
        "\n",
        "        self.deeplab = base(num_classes, num_layers)\n",
        "\n",
        "        in_channels = 2048 if num_layers == 50 else 512\n",
        "\n",
        "        self.convolution_mapping = nn.Conv2d(in_channels=in_channels,\n",
        "                                             out_channels=hidden_features,\n",
        "                                             kernel_size=(1, 1),\n",
        "                                             stride=(1, 1),\n",
        "                                             padding=(0, 0),\n",
        "                                             bias=True)\n",
        "\n",
        "        self.query_positions = nn.Parameter(data=torch.randn(\n",
        "            number_of_query_positions, hidden_features, dtype=torch.float),\n",
        "                                            requires_grad=True)\n",
        "\n",
        "        self.row_embedding = nn.Parameter(data=torch.randn(100,\n",
        "                                                           hidden_features //\n",
        "                                                           2,\n",
        "                                                           dtype=torch.float),\n",
        "                                          requires_grad=True)\n",
        "        self.column_embedding = nn.Parameter(data=torch.randn(\n",
        "            100, hidden_features // 2, dtype=torch.float),\n",
        "                                             requires_grad=True)\n",
        "\n",
        "        self.transformer = [\n",
        "            Transformer(d_model=hidden_features),\n",
        "            BoundaryAwareTransformer(d_model=hidden_features)\n",
        "        ][point_pred]\n",
        "\n",
        "        if self.use_decoder:\n",
        "            self.BCA = BoundaryCrossAttention(hidden_features, 8)\n",
        "\n",
        "        self.trans_out_conv = nn.Conv2d(in_channels=hidden_features,\n",
        "                                        out_channels=in_channels,\n",
        "                                        kernel_size=(1, 1),\n",
        "                                        stride=(1, 1),\n",
        "                                        padding=(0, 0),\n",
        "                                        bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x.size()[2]\n",
        "        w = x.size()[3]\n",
        "        feature_map = self.deeplab.resnet(x)\n",
        "\n",
        "        features = self.convolution_mapping(feature_map)\n",
        "        height, width = features.shape[2:]\n",
        "        batch_size = features.shape[0]\n",
        "        positional_embeddings = torch.cat([\n",
        "            self.column_embedding[:height].unsqueeze(dim=0).repeat(\n",
        "                height, 1, 1),\n",
        "            self.row_embedding[:width].unsqueeze(dim=1).repeat(1, width, 1)\n",
        "        ],\n",
        "                                          dim=-1).permute(\n",
        "                                              2, 0, 1).unsqueeze(0).repeat(\n",
        "                                                  batch_size, 1, 1, 1)\n",
        "\n",
        "        if self.transformer_type == 'BoundaryAwareTransformer':\n",
        "            latent_tensor, features_encoded, point_maps = self.transformer(\n",
        "                features, None, self.query_positions, positional_embeddings)\n",
        "        else:\n",
        "            latent_tensor, features_encoded = self.transformer(\n",
        "                features, None, self.query_positions, positional_embeddings)\n",
        "            point_maps = []\n",
        "\n",
        "        latent_tensor = latent_tensor.permute(2, 0, 1)\n",
        "        # shape:(bs, 1 , 128)\n",
        "\n",
        "        if self.use_decoder:\n",
        "            features_encoded, point_dec = self.BCA(features_encoded,\n",
        "                                                   latent_tensor)\n",
        "            point_maps.append(point_dec)\n",
        "\n",
        "        trans_feature_maps = self.trans_out_conv(\n",
        "            features_encoded.contiguous())  #.contiguous()\n",
        "\n",
        "        trans_feature_maps = trans_feature_maps + feature_map\n",
        "\n",
        "        output = self.deeplab.aspp(\n",
        "            trans_feature_maps\n",
        "        )  # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "        output = F.interpolate(\n",
        "            output, size=(h, w),\n",
        "            mode=\"bilinear\")  # (shape: (batch_size, num_classes, h, w))\n",
        "\n",
        "        if self.point_pred == 1:\n",
        "            return output, point_maps\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIIdw0tLK2S4"
      },
      "source": [
        "## 2. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "svv3B6UJ9UPP"
      },
      "outputs": [],
      "source": [
        "model = BAT(1, parse_config.net_layer, parse_config.point_pred,\n",
        "                    parse_config.ppl).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ag6qGuR9UPP"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "20fGhStqikfl"
      },
      "outputs": [],
      "source": [
        "# Based on: https://github.com/facebookresearch/detr/blob/master/models/transformer.py\n",
        "import copy\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "# from .BAT_Modules import BoundaryWiseAttentionGate2D, BoundaryWiseAttentionGate1D, BoundaryWiseAttentionGateAtrous2D, BoundaryWiseAttentionGateAtrous1D\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers,\n",
        "                                          encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        #         print(\"Trans Encoder\",memory.shape)\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers=6,\n",
        "                 d_model=512,\n",
        "                 nhead=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=2,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False,\n",
        "                 return_intermediate_dec=False,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = BoundaryAwareTransformerEncoderLayer(\n",
        "            d_model, nhead, BAG_type, Atrous, dim_feedforward, dropout,\n",
        "            activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = BoundaryAwareTransformerEncoder(point_pred_layers,\n",
        "                                                       encoder_layer,\n",
        "                                                       num_encoder_layers,\n",
        "                                                       encoder_norm)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead,\n",
        "                                                dim_feedforward, dropout,\n",
        "                                                activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer,\n",
        "            num_decoder_layers,\n",
        "            decoder_norm,\n",
        "            return_intermediate=return_intermediate_dec)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory, weights = self.encoder(src,\n",
        "                                       src_key_padding_mask=mask,\n",
        "                                       pos=pos_embed,\n",
        "                                       height=h,\n",
        "                                       width=w)\n",
        "\n",
        "        hs = self.decoder(tgt,\n",
        "                          memory,\n",
        "                          memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed,\n",
        "                          query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h,\n",
        "                                                                w), weights\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask,\n",
        "                           pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 point_pred_layers,\n",
        "                 encoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None):\n",
        "        super().__init__()\n",
        "        self.point_pred_layers = point_pred_layers\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        output = src\n",
        "        weights = []\n",
        "\n",
        "        for layer_i, layer in enumerate(self.layers):\n",
        "            if layer_i > self.num_layers - self.point_pred_layers - 1:\n",
        "                output, weight = layer(\n",
        "                    True,\n",
        "                    output,\n",
        "                    src_mask=mask,\n",
        "                    src_key_padding_mask=src_key_padding_mask,\n",
        "                    pos=pos,\n",
        "                    height=height,\n",
        "                    width=width)\n",
        "                weights.append(weight)\n",
        "            else:\n",
        "                output = layer(False,\n",
        "                               output,\n",
        "                               src_mask=mask,\n",
        "                               src_key_padding_mask=src_key_padding_mask,\n",
        "                               pos=pos,\n",
        "                               height=height,\n",
        "                               width=width)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=src2,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class BoundaryAwareTransformerEncoderLayer(TransformerEncoderLayer):\n",
        "    \"    Add Boundary-wise Attention Gate to Transformer's Encoder\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 BAG_type='2D',\n",
        "                 Atrous=True,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation,\n",
        "                         normalize_before)\n",
        "        if BAG_type == '1D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous1D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate1D(d_model)\n",
        "        elif BAG_type == '2D':\n",
        "            if Atrous:\n",
        "                self.BAG = BoundaryWiseAttentionGateAtrous2D(d_model)\n",
        "            else:\n",
        "                self.BAG = BoundaryWiseAttentionGate2D(d_model)\n",
        "        self.BAG_type = BAG_type\n",
        "\n",
        "    def forward(self,\n",
        "                use_bag,\n",
        "                src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                height: int = 32,\n",
        "                width: int = 32):\n",
        "        if self.normalize_before:\n",
        "            features = self.forward_pre(src, src_mask, src_key_padding_mask,\n",
        "                                        pos)\n",
        "            if use_bag:\n",
        "                b, c = features.shape[1:]\n",
        "                if self.BAG_type == '1D':\n",
        "                    features = features.permute(1, 2, 0)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.permute(2, 0, 1).contiguous()\n",
        "                    weights = weights.view(b, 1, height, width)\n",
        "                elif self.BAG_type == '2D':\n",
        "                    features = features.permute(1, 2,\n",
        "                                                0).view(b, c, height, width)\n",
        "                    features, weights = self.BAG(features)\n",
        "                    features = features.flatten(2).permute(2, 0,\n",
        "                                                           1).contiguous()\n",
        "                return features, weights\n",
        "            else:\n",
        "                return features\n",
        "        features = self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "        if use_bag:\n",
        "            b, c = features.shape[1:]\n",
        "            if self.BAG_type == '1D':\n",
        "                features = features.permute(1, 2, 0)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.permute(2, 0, 1).contiguous()\n",
        "                weights = weights.view(b, 1, height, width)\n",
        "            elif self.BAG_type == '2D':\n",
        "                features = features.permute(1, 2, 0).view(b, c, height, width)\n",
        "                features, weights = self.BAG(features)\n",
        "                features = features.flatten(2).permute(2, 0, 1).contiguous()\n",
        "            return features, weights\n",
        "        else:\n",
        "            return features\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 decoder_layer,\n",
        "                 num_layers,\n",
        "                 norm=None,\n",
        "                 return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output,\n",
        "                           memory,\n",
        "                           tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos,\n",
        "                           query_pos=query_pos)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 activation=nn.LeakyReLU,\n",
        "                 normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model,\n",
        "                                                    nhead,\n",
        "                                                    dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     tgt,\n",
        "                     memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self,\n",
        "                    tgt,\n",
        "                    memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q,\n",
        "                              k,\n",
        "                              value=tgt2,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory,\n",
        "                                   attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self,\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask,\n",
        "                                    memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask,\n",
        "                                 pos, query_pos)\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args.hidden_dim,\n",
        "        dropout=args.dropout,\n",
        "        nhead=args.nheads,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        num_encoder_layers=args.enc_layers,\n",
        "        num_decoder_layers=args.dec_layers,\n",
        "        normalize_before=args.pre_norm,\n",
        "        return_intermediate_dec=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"leaky relu\":\n",
        "        return F.leaky_relu\n",
        "    if activation == \"selu\":\n",
        "        return F.selu\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(\n",
        "        F\"activation should be relu, gelu, glu, leaky relu or selu, not {activation}.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzze2VhIK2S5"
      },
      "source": [
        "### Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "TbTO94ePK2S5"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.0001):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.mode = mode\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        self.BEST_MODEL_PATH = \"\"\n",
        "        if self.mode == \"min\":\n",
        "            self.val_score = np.inf\n",
        "        else:\n",
        "            self.val_score = -np.inf\n",
        "\n",
        "    def __call__(self, epoch, epoch_score, model, optimizer, loss, model_path):\n",
        "        if self.mode == \"min\":\n",
        "            score = -1.0 * epoch_score\n",
        "        else:\n",
        "            score = np.copy(epoch_score)\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch, epoch_score, model, optimizer, loss, model_path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(\n",
        "                \"EarlyStopping counter: {} out of {}\".format(\n",
        "                    self.counter, self.patience\n",
        "                )\n",
        "            )\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch, epoch_score, model, optimizer, loss, model_path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch, epoch_score, model, optimizer, loss, model_path):\n",
        "        model_path = Path(model_path)\n",
        "        parent = model_path.parent\n",
        "        os.makedirs(parent, exist_ok=True)\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
        "            print(\n",
        "                \"Validation score improved ({} --> {}). Model saved at {}!\".format(\n",
        "                    self.val_score, epoch_score, model_path\n",
        "                )\n",
        "            )\n",
        "            torch.save({\"Epoch\": epoch, \"Train Loss\": loss, \"Validation Dice Score\": self.val_score, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, model_path)\n",
        "            self.BEST_MODEL_PATH = model_path\n",
        "            print(f\"Checkpoint saved on epoch - {epoch} with dice score - {epoch_score}\")\n",
        "        self.val_score = epoch_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "JwQu_LDS9UPS"
      },
      "outputs": [],
      "source": [
        "criterion = CRITERION\n",
        "es = EarlyStopping(patience=EARLY_STOPPING_PATIENCE, mode='max')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOlM9jTEK2S5"
      },
      "source": [
        "### Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "_amog1GHK2S5"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "xbn3pKTaK2S5"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(train_loader, model, optimizer, loss_fn, epoch, accumulation_steps=int(EFFECTIVE_BATCH_SIZE/BATCH_SIZE), device='cuda'):\n",
        "    losses = AverageMeter()\n",
        "    # Lists to store batch-to-batch progress details within the epoch while training\n",
        "    batch_count_train = []\n",
        "    batch_train_loss = []\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    if accumulation_steps > 1:\n",
        "      optimizer.zero_grad()\n",
        "    tk0 = tqdm(train_loader, total=len(train_loader))\n",
        "    for b_idx, data in enumerate(tk0):\n",
        "      #print(data['image'].shape) # print(data['image'].shape) -> torch.Size([8, 3, 512, 512])\n",
        "      #print(data['mask'].shape) # print(data['mask'].shape) -> torch.Size([8, 1, 512, 512])\n",
        "      if (b_idx + 1) % accumulation_steps == 0:\n",
        "        batch_count_train.append(b_idx)\n",
        "\n",
        "      # moves image tensor and mask tensor to gpu\n",
        "      for key, value in data.items():\n",
        "        data[key] = value.to(\"cuda\")\n",
        "      point = (data['point'] > 0).cuda().float()\n",
        "\n",
        "      if parse_config.net_layer == 18:\n",
        "          point_c4 = nn.functional.max_pool2d(point,\n",
        "                                              kernel_size=(16, 16),\n",
        "                                              stride=(16, 16))\n",
        "          point = nn.functional.max_pool2d(point,\n",
        "                                          kernel_size=(8, 8),\n",
        "                                          stride=(8, 8))\n",
        "      else:\n",
        "          point_c5 = nn.functional.max_pool2d(point,\n",
        "                                              kernel_size=(32, 32),\n",
        "                                              stride=(32, 32))\n",
        "\n",
        "          point_c4 = nn.functional.max_pool2d(point,\n",
        "                                              kernel_size=(16, 16),\n",
        "                                              stride=(16, 16))\n",
        "\n",
        "      if accumulation_steps == 1 and b_idx == 0:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "      if parse_config.point_pred == 1:\n",
        "            output, point_maps_pre = model(data['image'])\n",
        "            output = torch.sigmoid(output)\n",
        "\n",
        "            #print(\"point_maps_pre[-1] shape:{}, point_c4 shape:{}\".format(point_maps_pre[-1].shape,point_c4.shape))\n",
        "            assert (output.shape == data['mask'].float().shape)\n",
        "            loss_dc = dice_loss(output, data['mask'].float())\n",
        "            #print(point_maps_pre[-1].shape, point_c4.shape)\n",
        "            assert (point_maps_pre[-1].shape == point_c4.shape)\n",
        "\n",
        "            point_loss = 0.\n",
        "            for i in range(len(point_maps_pre)):\n",
        "                point_loss += criterion(point_maps_pre[i], point_c4)\n",
        "            point_loss = point_loss / len(point_maps_pre)\n",
        "\n",
        "            loss = loss_dc + point_loss  # point_loss weight: 3\n",
        "\n",
        "            with torch.set_grad_enabled(True):\n",
        "              loss.backward()\n",
        "              # if (b_idx + 1) % accumulation_steps == 0:\n",
        "              #   if GRADIENT_CLIPPING:\n",
        "              #     clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING_THRESHOLD)\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "      ###############################################################################################\n",
        "      # out  = model(data['image']) # out.shape = torch.Size([8, 1, 512, 512])\n",
        "      # loss = loss_fn(out, data['mask'].float()) # mask.shape = torch.Size([8, 1, 512, 512])\n",
        "      # with torch.set_grad_enabled(True):\n",
        "      #   loss.backward()\n",
        "      #   if (b_idx + 1) % accumulation_steps == 0:\n",
        "      #     if GRADIENT_CLIPPING:\n",
        "      #       clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING_THRESHOLD)\n",
        "      #     optimizer.step()\n",
        "      #     optimizer.zero_grad()\n",
        "      losses.update(loss.item(), train_loader.batch_size)\n",
        "      if (b_idx + 1) % accumulation_steps == 0:\n",
        "        batch_train_loss.append(loss.item())\n",
        "      tk0.set_postfix(loss=losses.avg, learning_rate=optimizer.param_groups[0]['lr'])\n",
        "    return losses.avg, batch_count_train, batch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FKL6S336-unk",
        "outputId": "46550c13-e9fd-4337-9b95-a2e46a5e178b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>focal_loss</b><br/>def focal_loss(inputs: torch.Tensor, targets: torch.Tensor, alpha: float=0.6, gamma: float=2, reduction: str=&#x27;mean&#x27;) -&gt; torch.Tensor</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-103-defc6de845ef&gt;</a>&lt;no docstring&gt;</pre></div>"
            ],
            "text/plain": [
              "<function __main__.focal_loss(inputs: torch.Tensor, targets: torch.Tensor, alpha: float = 0.6, gamma: float = 2, reduction: str = 'mean') -> torch.Tensor>"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K136b3eqK2S5"
      },
      "source": [
        "### Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RY2NxC-unk"
      },
      "source": [
        "#### Mask Binarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "V4my02Uf-unk"
      },
      "outputs": [],
      "source": [
        "class MaskBinarization():\n",
        "    def __init__(self):\n",
        "        self.thresholds = 0.5\n",
        "    def transform(self, predicted):\n",
        "        yield predicted > self.thresholds\n",
        "\n",
        "class SimpleMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, score_thresholds):\n",
        "        super().__init__()\n",
        "        self.thresholds = score_thresholds\n",
        "    def transform(self, predicted):\n",
        "        for thr in self.thresholds:\n",
        "            yield predicted > thr\n",
        "\n",
        "class DupletMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, duplets, with_channels=True):\n",
        "        super().__init__()\n",
        "        self.thresholds = duplets\n",
        "        self.dims = (2,3) if with_channels else (1,2)\n",
        "    def transform(self, predicted):\n",
        "        for score_threshold, area_threshold in self.thresholds:\n",
        "            mask = predicted > score_threshold\n",
        "            mask[mask.sum(dim=self.dims) < area_threshold] = 0\n",
        "            yield mask\n",
        "\n",
        "class TripletMaskBinarization(MaskBinarization):\n",
        "    def __init__(self, triplets, with_channels=True):\n",
        "        super().__init__()\n",
        "        self.thresholds = triplets\n",
        "        self.dims = (2,3) if with_channels else (1,2) # dims should be HxW, basically it should ignore batch_size and no_of_channels in the general format of BxCxHxW\n",
        "    def transform(self, predicted):\n",
        "        for top_score_threshold, area_threshold, bottom_score_threshold in self.thresholds:\n",
        "            clf_mask = (predicted > top_score_threshold).float()\n",
        "            pred_mask = (predicted > bottom_score_threshold).float()\n",
        "            pred_mask[clf_mask.sum(dim=self.dims) < area_threshold] = 0\n",
        "            yield pred_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joGnfZgw-unl"
      },
      "source": [
        "#### Metric used in validation and evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "zlWOcwQdK2S5"
      },
      "outputs": [],
      "source": [
        "def metric(probability, truth):\n",
        "    probability = torch.from_numpy(probability)\n",
        "    truth = torch.from_numpy(truth)\n",
        "    # print(probability.shape, truth.shape)\n",
        "    if probability.shape[0] == truth.shape[0]: # checking for batch size mismatches in the code for image & mask\n",
        "        batch_size = probability.shape[0]\n",
        "    with torch.no_grad():\n",
        "        probability = probability.view(batch_size, -1) # probability's size is [8, 1*512*512]\n",
        "        truth = truth.view(batch_size, -1)             # truth's size is [8, 1*512*512]\n",
        "        assert(probability.shape == truth.shape)\n",
        "\n",
        "        p = probability.float() # prob_preds already comes in binarized.\n",
        "        t = (truth > 0.5).float()\n",
        "\n",
        "        t_sum = t.sum(-1) # t_sum size is 8 # Each value in the vector represents the sum of all pixels in one mask\n",
        "        p_sum = p.sum(-1) # p_sum size is 8 # Each value in the vector represents the sum of all elements in one pred_probs\n",
        "        neg_index = torch.nonzero(t_sum == 0) # indices of masks which are negative.\n",
        "        pos_index = torch.nonzero(t_sum >= 1) # indices of masks which are positive.\n",
        "\n",
        "        dice_neg = (p_sum == 0).float() # tensor of size 8\n",
        "        \"\"\"\n",
        "        if t_sum = torch.tensor([0.0, 1000.0, 0.0, 600.0, 720.0, 420.0, 0.0, 0.0]), then dice_neg = tensor([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0])\n",
        "        \"\"\"\n",
        "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1)) # tensor of size 8\n",
        "\n",
        "        dice_neg = dice_neg[neg_index] # selects elements of dice_neg acc to the indices in neg_index, it can have more than one element.\n",
        "        dice_pos = dice_pos[pos_index] # similar to the above code line.\n",
        "        dice = torch.cat([dice_pos, dice_neg])\n",
        "\n",
        "        num_neg = len(neg_index) # no. of negative masks in a batch\n",
        "        num_pos = len(pos_index) # no. of positive masks in a batch\n",
        "\n",
        "    return dice.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "u3ovO3f9XHQt"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_best_thresholds(epoch, b_idx, best_metric, best_threshold, filepath=Path(save_best_thresholds_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch,\n",
        "        'batch_number': b_idx,\n",
        "        'best_metric': best_metric,\n",
        "        'best_threshold': best_threshold\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "7opE5TuMRCvg"
      },
      "outputs": [],
      "source": [
        "epoch_count_thr = []\n",
        "batch_indices = []\n",
        "best_metrics_list = []\n",
        "best_thresholds_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "ww6ftfKmK2S5"
      },
      "outputs": [],
      "source": [
        "def evaluate(valid_loader, model, epoch, device=DEVICE, metric=metric, loss_fn=criterion):\n",
        "    losses = AverageMeter()\n",
        "    combolosses = AverageMeter()\n",
        "    metrics = defaultdict(float)\n",
        "    # Lists to store batch-to-batch progress details within the epoch while training\n",
        "    batch_count_val = []\n",
        "    batch_val_loss_values = []\n",
        "################################# top\n",
        "    dice_value = 0\n",
        "    iou_value = 0\n",
        "    dice_average = 0\n",
        "    iou_average = 0\n",
        "    numm = 0\n",
        "################################# bottom\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    tk0 = tqdm(valid_loader, total=len(valid_loader))\n",
        "    with torch.inference_mode():\n",
        "        for b_idx, input_data in enumerate(tk0):\n",
        "            batch_count_val.append(b_idx)\n",
        "################################################################################ top\n",
        "            data = input_data['image'].cuda().float()\n",
        "            label = input_data['mask'].cuda().float()\n",
        "            point = (input_data['point'] > 0).cuda().float()\n",
        "            point_c5 = nn.functional.max_pool2d(point,\n",
        "                                                kernel_size=(32, 32),\n",
        "                                                stride=(32, 32))\n",
        "            point_c4 = nn.functional.max_pool2d(point,\n",
        "                                                kernel_size=(16, 16),\n",
        "                                                stride=(16, 16))\n",
        "################################################################################### bottom\n",
        "\n",
        "#################################################################################### top\n",
        "            if parse_config.arch == 'transfuse':\n",
        "                _, _, output = model(data)\n",
        "                loss_fuse = structure_loss(output, label)\n",
        "            elif parse_config.point_pred == 0:\n",
        "                output = model(data)\n",
        "            elif parse_config.cross == 1 and parse_config.point_pred == 1:\n",
        "                output, point_maps_pre_1, point_maps_pre_2 = model(data)\n",
        "                point_loss_c4 = 0.\n",
        "                for i in range(len(point_maps_pre_1) - 1):\n",
        "                    point_loss_c4 += criterion(point_maps_pre_1[i], point_c4)\n",
        "                point_loss_c4 = 1.0 / len(point_maps_pre_1) * (\n",
        "                    point_loss_c4 + criterion(point_maps_pre_1[-1], point_c4))\n",
        "                point_loss_c5 = 0.\n",
        "                for i in range(len(point_maps_pre_2) - 1):\n",
        "                    point_loss_c5 += criterion(point_maps_pre_2[i], point_c5)\n",
        "                point_loss_c5 = 1.0 / len(point_maps_pre_2) * (\n",
        "                    point_loss_c5 + criterion(point_maps_pre_2[-1], point_c5))\n",
        "                point_loss = 0.5 * (point_loss_c4 + point_loss_c5)\n",
        "            elif parse_config.point_pred == 1:\n",
        "                output, point_maps_pre = model(data)\n",
        "                point_loss = 0.\n",
        "                for i in range(len(point_maps_pre) - 1):\n",
        "                    point_loss += criterion(point_maps_pre[i], point_c4)\n",
        "                point_loss = 1.0 / len(point_maps_pre) * (\n",
        "                    point_loss + criterion(point_maps_pre[-1], point_c4))\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "\n",
        "            loss_dc = dice_loss(output, label)\n",
        "\n",
        "            if parse_config.arch == 'transfuse':\n",
        "                loss = loss_fuse\n",
        "            elif parse_config.arch == 'transunet':\n",
        "                loss = 0.5 * loss_dc + 0.5 * ce_loss(output, label)\n",
        "            elif parse_config.point_pred == 0:\n",
        "                loss = loss_dc\n",
        "            elif parse_config.cross == 1 and parse_config.point_pred == 1:\n",
        "                loss = loss_dc + point_loss\n",
        "            elif parse_config.point_pred == 1:\n",
        "                loss = loss_dc + 3 * point_loss\n",
        "\n",
        "            output = output.cpu().numpy() > 0.5\n",
        "\n",
        "            label = label.cpu().numpy()\n",
        "            assert (output.shape == label.shape)\n",
        "            dice_ave = metric(output, label)\n",
        "            # iou_ave = jc(output, label)\n",
        "            dice_value += dice_ave\n",
        "            # iou_value += iou_ave\n",
        "            numm += 1\n",
        "\n",
        "            tk0.set_description('score: {:.5f}'.format(dice_ave))\n",
        "\n",
        "            epoch_count_thr.append(epoch)\n",
        "            batch_indices.append(b_idx)\n",
        "            best_metrics_list.append(dice_ave)\n",
        "\n",
        "            # if .item() is used then .cpu() is NOT required. .item() will itself return a float value.\n",
        "            losses.update(dice_ave, valid_loader.batch_size)\n",
        "            combolosses.update(loss.item(), valid_loader.batch_size)\n",
        "            batch_val_loss_values.append(loss.item())\n",
        "            tk0.set_postfix(dice_score=losses.avg, val_loss=combolosses.avg)\n",
        "    dice_average = dice_value / numm\n",
        "    # iou_average = iou_value / numm\n",
        "    print(\"Average dice value of evaluation dataset = \", dice_average)\n",
        "    # print(\"Average iou value of evaluation dataset = \", iou_average)\n",
        "    return losses.avg, batch_count_val, batch_val_loss_values, combolosses.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE7TO5J9XHQt"
      },
      "source": [
        "### Optimizer & Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "XGkO2pA_9UPW"
      },
      "outputs": [],
      "source": [
        "if PRETRAINED:\n",
        "  checkpoint = torch.load(TRAINING_MODEL_PATH)\n",
        "  model.to(DEVICE)\n",
        "  model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=1)\n",
        "elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "Fn0XAwlyK2S6"
      },
      "outputs": [],
      "source": [
        "# if PRETRAINED:\n",
        "#   checkpoint = torch.load(TRAINING_MODEL_PATH)\n",
        "#   model.to(DEVICE)\n",
        "#   model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "#   optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay= L2_WEIGHT_DECAY)\n",
        "#   if OPTIMIZER_LOAD:\n",
        "#     optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "#   # Manually set the new learning rate for this stage of training as loading optimizer's state dict will\n",
        "#   # load parameters that was there while saving the previous checkpoint but loading the optimizer's state dict is\n",
        "#   # crucial\n",
        "#   for param_group in optimizer.param_groups:\n",
        "#     param_group['lr'] = LEARNING_RATE\n",
        "\n",
        "#   if SCHEDULER == \"ReduceLROnPlateau\":\n",
        "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=SCHEDULER_PARAMS[\"factor\"], patience=SCHEDULER_PARAMS[\"patience\"], threshold=SCHEDULER_PARAMS[\"threshold\"], min_lr=SCHEDULER_PARAMS[\"min_lr\"])\n",
        "#   elif SCHEDULER == \"CosineAnnealingWarmRestarts\":\n",
        "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=SCHEDULER_PARAMS[\"T_0\"], T_mult=SCHEDULER_PARAMS[\"T_mult\"], eta_min=SCHEDULER_PARAMS[\"eta_min\"])\n",
        "#   elif SCHEDULER == \"CosineAnnealingLR\":\n",
        "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=SCHEDULER_PARAMS[\"T_max\"], eta_min=SCHEDULER_PARAMS[\"eta_min\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "gK69cmOlRCvg"
      },
      "outputs": [],
      "source": [
        "# if not PRETRAINED:\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay= L2_WEIGHT_DECAY)\n",
        "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=SCHEDULER_PARAMS[\"factor\"], patience=SCHEDULER_PARAMS[\"patience\"], threshold=SCHEDULER_PARAMS[\"threshold\"], min_lr=SCHEDULER_PARAMS[\"min_lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqlFcFDSRCvh",
        "outputId": "3db3abc1-a5fa-4119-d78b-49a73506f869"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New learning rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "    print(f\"New learning rate: {param_group['lr']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "ig7E3afMOi-6"
      },
      "outputs": [],
      "source": [
        "# initial_lr_scheduler = scheduler.base_lrs[0]  # Assuming a single learning rate for all parameters\n",
        "# print(\"Initial learning rate of scheduler:\", initial_lr_scheduler)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOkP2HYpvsqr"
      },
      "source": [
        "### GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "ypne222KBjPe"
      },
      "outputs": [],
      "source": [
        "# # memory footprint support libraries/code\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isnt guaranteed\n",
        "# gpu = GPUs[0]\n",
        "# def printm():\n",
        "#  process = psutil.Process(os.getpid())\n",
        "#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "# printm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtHHjC45vsqr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcACWXgAXHQu"
      },
      "source": [
        "#### Saving Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "AzIGIAe2FmJQ"
      },
      "outputs": [],
      "source": [
        "def store_batch_training_details(epoch, batch_count_train, batch_train_loss):\n",
        "  # Directory where you want to save the CSV file\n",
        "  directory = store_batch_training_details_path\n",
        "\n",
        "  # Ensure the directory exists\n",
        "  os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "  # File path\n",
        "  file_path = os.path.join(directory, name_of_batch_training_details_csv + f\"{epoch}.csv\")\n",
        "\n",
        "  # Write to CSV file\n",
        "  with open(file_path, \"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      # Write header\n",
        "      writer.writerow([\"Batch Count Train\", \"Batch Train Loss\"])\n",
        "      # Write data rows\n",
        "      for batch_count, train_loss in zip(batch_count_train, batch_train_loss):\n",
        "          writer.writerow([batch_count, train_loss])\n",
        "\n",
        "  print(f\"CSV file for epoch-{epoch}has been created at: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "f5-0shprvsqr"
      },
      "outputs": [],
      "source": [
        "def store_batch_validation_details(epoch, batch_count_val, batch_val_score_values):\n",
        "  # Directory where you want to save the CSV file\n",
        "  directory = store_batch_validation_details_path\n",
        "\n",
        "  # Ensure the directory exists\n",
        "  os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "  # File path\n",
        "  file_path = os.path.join(directory, name_of_batch_validation_details_csv + f\"{epoch}.csv\")\n",
        "\n",
        "  # Write to CSV file\n",
        "  with open(file_path, \"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      # Write header\n",
        "      writer.writerow([\"Batch Count Validation\", \"Batch Validation Loss\"])\n",
        "      # Write data rows\n",
        "      for batch_count, batch_val_score in zip(batch_count_val, batch_val_score_values):\n",
        "          writer.writerow([batch_count, batch_val_score])\n",
        "\n",
        "  print(f\"CSV file for validation, epoch-{epoch}has been created at: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "ijkQ3VlU-unm"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_progress(epoch_count, loss_values, val_loss_values, filepath= Path(save_progress_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch_count,\n",
        "        'train_loss': loss_values,\n",
        "        'val_loss': [None] * (len(epoch_count) - len(val_loss_values)) + val_loss_values\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "PTogqmQgWkZC"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_dice_score(epoch_count, val_score_values, filepath= Path(save_dice_score_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'epoch': epoch_count,\n",
        "        'val_dice_scores': [None] * (len(epoch_count) - len(val_score_values)) + val_score_values\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "ntFd7ioHXHQv"
      },
      "outputs": [],
      "source": [
        "# Function to save the data dynamically\n",
        "def save_losses(bce_losses=bce_losses, dice_losses=dice_losses, focal_losses=focal_losses, filepath= Path(save_3losses_path)):\n",
        "    # Create a dictionary of lists\n",
        "    data = {\n",
        "        'bce': bce_losses,\n",
        "        'dice': dice_losses,\n",
        "        'focal': focal_losses\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to CSV (mode='w' to write from scratch each time, mode='a' to append)\n",
        "    df.to_csv(filepath, index=False, mode='w')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMFmkSOtXHQv"
      },
      "source": [
        "#### Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bPiRNBuK2S6",
        "outputId": "1438c0ed-8a26-477e-dc48-00e343ea92c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [08:03<00:00,  1.62s/it, learning_rate=0.001, loss=0.705]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-1has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_1.csv\n",
            "EPOCH: 1, TRAIN LOSS: 0.7052062827868749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.00187: 100%|| 268/268 [08:34<00:00,  1.92s/it, dice_score=tensor(0.1731), val_loss=0.798]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.1731)\n",
            "CSV file for validation, epoch-1has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics1.csv\n",
            "EPOCH: 1, TRAIN LOSS: 0.7052062827868749, VAL DICE: 0.1730841100215912\n",
            "Validation score improved (-inf --> 0.1730841100215912). Model saved at /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/m_512_CHECKPOINTS_epoch1_bst_model512_fold4_0.17309999465942383.tar!\n",
            "Checkpoint saved on epoch - 1 with dice score - 0.1730841100215912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [06:55<00:00,  1.40s/it, learning_rate=0.001, loss=0.574]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-2has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_2.csv\n",
            "EPOCH: 2, TRAIN LOSS: 0.574423877684862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.00039: 100%|| 268/268 [03:26<00:00,  1.30it/s, dice_score=tensor(0.0754), val_loss=0.941]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.0754)\n",
            "CSV file for validation, epoch-2has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics2.csv\n",
            "EPOCH: 2, TRAIN LOSS: 0.574423877684862, VAL DICE: 0.07541681081056595\n",
            "EarlyStopping counter: 1 out of 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [07:09<00:00,  1.44s/it, learning_rate=0.001, loss=0.516]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-3has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_3.csv\n",
            "EPOCH: 3, TRAIN LOSS: 0.5158566226495193\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.33360: 100%|| 268/268 [03:23<00:00,  1.32it/s, dice_score=tensor(0.2622), val_loss=0.742]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.2622)\n",
            "CSV file for validation, epoch-3has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics3.csv\n",
            "EPOCH: 3, TRAIN LOSS: 0.5158566226495193, VAL DICE: 0.262224018573761\n",
            "Validation score improved (0.1730841100215912 --> 0.262224018573761). Model saved at /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/m_512_CHECKPOINTS_epoch3_bst_model512_fold4_0.2621999979019165.tar!\n",
            "Checkpoint saved on epoch - 3 with dice score - 0.262224018573761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [07:01<00:00,  1.41s/it, learning_rate=0.001, loss=0.511]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-4has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_4.csv\n",
            "EPOCH: 4, TRAIN LOSS: 0.5105847410807673\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.16667: 100%|| 268/268 [03:29<00:00,  1.28it/s, dice_score=tensor(0.2192), val_loss=0.959]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.2192)\n",
            "CSV file for validation, epoch-4has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics4.csv\n",
            "EPOCH: 4, TRAIN LOSS: 0.5105847410807673, VAL DICE: 0.2192484438419342\n",
            "EarlyStopping counter: 1 out of 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [06:57<00:00,  1.40s/it, learning_rate=0.0005, loss=0.447]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-5has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_5.csv\n",
            "EPOCH: 5, TRAIN LOSS: 0.4474600589515379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.16667: 100%|| 268/268 [03:33<00:00,  1.26it/s, dice_score=tensor(0.4381), val_loss=0.647]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.4381)\n",
            "CSV file for validation, epoch-5has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics5.csv\n",
            "EPOCH: 5, TRAIN LOSS: 0.4474600589515379, VAL DICE: 0.43810462951660156\n",
            "Validation score improved (0.262224018573761 --> 0.43810462951660156). Model saved at /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/m_512_CHECKPOINTS_epoch5_bst_model512_fold4_0.43810001015663147.tar!\n",
            "Checkpoint saved on epoch - 5 with dice score - 0.43810462951660156\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [06:58<00:00,  1.40s/it, learning_rate=0.0005, loss=0.423]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-6has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_6.csv\n",
            "EPOCH: 6, TRAIN LOSS: 0.42279760314514175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.33333: 100%|| 268/268 [03:31<00:00,  1.27it/s, dice_score=tensor(0.3609), val_loss=0.669]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.3609)\n",
            "CSV file for validation, epoch-6has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics6.csv\n",
            "EPOCH: 6, TRAIN LOSS: 0.42279760314514175, VAL DICE: 0.3609122335910797\n",
            "EarlyStopping counter: 1 out of 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [06:58<00:00,  1.40s/it, learning_rate=0.00025, loss=0.397]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-7has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_7.csv\n",
            "EPOCH: 7, TRAIN LOSS: 0.39710908403132583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.33333: 100%|| 268/268 [03:30<00:00,  1.27it/s, dice_score=tensor(0.4851), val_loss=0.641]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.4851)\n",
            "CSV file for validation, epoch-7has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics7.csv\n",
            "EPOCH: 7, TRAIN LOSS: 0.39710908403132583, VAL DICE: 0.4850598871707916\n",
            "Validation score improved (0.43810462951660156 --> 0.4850598871707916). Model saved at /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/m_512_CHECKPOINTS_epoch7_bst_model512_fold4_0.48510000109672546.tar!\n",
            "Checkpoint saved on epoch - 7 with dice score - 0.4850598871707916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [06:59<00:00,  1.41s/it, learning_rate=0.00025, loss=0.378]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-8has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_8.csv\n",
            "EPOCH: 8, TRAIN LOSS: 0.37824016889469736\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.33333: 100%|| 268/268 [03:35<00:00,  1.24it/s, dice_score=tensor(0.5837), val_loss=0.625]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.5837)\n",
            "CSV file for validation, epoch-8has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics8.csv\n",
            "EPOCH: 8, TRAIN LOSS: 0.37824016889469736, VAL DICE: 0.5836879014968872\n",
            "Validation score improved (0.4850598871707916 --> 0.5836879014968872). Model saved at /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/m_512_CHECKPOINTS_epoch8_bst_model512_fold4_0.5837000012397766.tar!\n",
            "Checkpoint saved on epoch - 8 with dice score - 0.5836879014968872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [06:59<00:00,  1.41s/it, learning_rate=0.000125, loss=0.366]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-9has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_9.csv\n",
            "EPOCH: 9, TRAIN LOSS: 0.36616717123945286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.66667: 100%|| 268/268 [03:32<00:00,  1.26it/s, dice_score=tensor(0.5735), val_loss=0.582]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.5735)\n",
            "CSV file for validation, epoch-9has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics9.csv\n",
            "EPOCH: 9, TRAIN LOSS: 0.36616717123945286, VAL DICE: 0.5735166072845459\n",
            "EarlyStopping counter: 1 out of 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [06:57<00:00,  1.40s/it, learning_rate=0.000125, loss=0.351]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-10has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_10.csv\n",
            "EPOCH: 10, TRAIN LOSS: 0.35054058401576627\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.16667: 100%|| 268/268 [03:30<00:00,  1.27it/s, dice_score=tensor(0.5954), val_loss=0.582]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.5954)\n",
            "CSV file for validation, epoch-10has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics10.csv\n",
            "EPOCH: 10, TRAIN LOSS: 0.35054058401576627, VAL DICE: 0.5954049229621887\n",
            "Validation score improved (0.5836879014968872 --> 0.5954049229621887). Model saved at /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/m_512_CHECKPOINTS_epoch10_bst_model512_fold4_0.5953999757766724.tar!\n",
            "Checkpoint saved on epoch - 10 with dice score - 0.5954049229621887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [06:56<00:00,  1.40s/it, learning_rate=6.25e-5, loss=0.341]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-11has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_11.csv\n",
            "EPOCH: 11, TRAIN LOSS: 0.3414846821719368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.33333: 100%|| 268/268 [03:38<00:00,  1.23it/s, dice_score=tensor(0.6139), val_loss=0.574]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average dice value of evaluation dataset =  tensor(0.6139)\n",
            "CSV file for validation, epoch-11has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_vali_m512_b8/Project-2_512_b_8_Validation_Metrics11.csv\n",
            "EPOCH: 11, TRAIN LOSS: 0.3414846821719368, VAL DICE: 0.6139189600944519\n",
            "Validation score improved (0.5954049229621887 --> 0.6139189600944519). Model saved at /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/m_512_CHECKPOINTS_epoch11_bst_model512_fold4_0.6139000058174133.tar!\n",
            "Checkpoint saved on epoch - 11 with dice score - 0.6139189600944519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 298/298 [07:01<00:00,  1.42s/it, learning_rate=6.25e-5, loss=0.349]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file for epoch-12has been created at: /content/drive/MyDrive/Saved Models/Final_Model1024_from_0.8342/Project-2_EXP_Novel_Algo_1_14_04_2025/Novel_Algo_1_train_m512_b8/Project-2_TRAIN_512_b_8_epoch_12.csv\n",
            "EPOCH: 12, TRAIN LOSS: 0.34933905703509416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "score: 0.77445:  56%|    | 150/268 [02:02<01:13,  1.60it/s, dice_score=tensor(0.5346), val_loss=0.582]"
          ]
        }
      ],
      "source": [
        "till_epoch = EPOCHS\n",
        "model.to(DEVICE)\n",
        "model.train()\n",
        "# Lists to store epoch-to-epoch progress details\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "val_score_values = []\n",
        "val_loss_values = []\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, till_epoch+1):\n",
        "    epoch_count.append(epoch)\n",
        "    loss, batch_count_train, batch_train_loss = train_one_epoch(train_dataloader, model, optimizer, criterion, epoch=epoch)\n",
        "    loss_values.append(loss)\n",
        "    store_batch_training_details(epoch, batch_count_train, batch_train_loss)\n",
        "    if not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step()\n",
        "    # Storing Training details for plotting curves and inference\n",
        "    print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}\")\n",
        "\n",
        "    \"\"\"-------------------VALIDATION---------------------\"\"\"\n",
        "    dice, batch_count_val, batch_val_score_values, val_loss = evaluate(val_dataloader, model, epoch=epoch) # Evaluates model performance by calculating the dice coefficient\n",
        "    val_score_values.append(dice)\n",
        "    val_loss_values.append(val_loss)\n",
        "    # Storing Validation details for plotting curves and inference\n",
        "    store_batch_validation_details(epoch, batch_count_val, batch_val_score_values)\n",
        "    print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}, VAL DICE: {dice}\")\n",
        "\n",
        "    save_progress(epoch_count, loss_values, val_loss_values)\n",
        "    save_dice_score(epoch_count, val_score_values)\n",
        "    save_losses()\n",
        "# \"\"\"\" If it is necessary to save model's state dict as checkpoint, do the essential changes\n",
        "#      in Early Stopping class.\"\"\"\n",
        "\n",
        "    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step(dice)\n",
        "\n",
        "    es(epoch, dice, model, optimizer, loss, model_path= model_checkpoint_path + f\"_epoch{epoch}_bst_model{IMG_SIZE}_fold{FOLD_ID}_{np.round(dice,4)}.tar\")\n",
        "    if es.early_stop:\n",
        "        print('\\n\\n -------------- EARLY STOPPING -------------- \\n\\n')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO0V40DhG6TM"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import seaborn as snsewf\n",
        "# import matplotlib.pyplot as pltdsa\n",
        "# key_patch_map = np.load(\"C:/Users/nisha/Desktop/Research Project May-June/gt_keypatch/1.2.276.0.7230010.3.1.4.8323329.300.1517875162.258081.npy\")\n",
        "# sns.heatmap(key_patch_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRoO5PvbG6TM"
      },
      "outputs": [],
      "source": [
        "# plt.imshow(key_patch_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSUJdNHlG6TM"
      },
      "outputs": [],
      "source": [
        "# key_patch_map.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-LO3JIgG6TN"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# key_patch_map = torch.unsqueeze(torch.Tensor(key_patch_map), axis=0)\n",
        "# print(key_patch_map.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJFz9wAhzL4k"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgLZEbqwyAb4"
      },
      "source": [
        "20 mins for a single epoch of training and validation together with T4 GPU. If the no. of epoches is 50 and early stopping is 10, let's see how many hours it takes to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ptGWY9E2cn"
      },
      "source": [
        "Visualizations for the predictions!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KC5W67ovsqs"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")221d\n",
        "52\n",
        "# fig, ax = plt.subplots()awjk\n",
        "506\n",
        "132\n",
        "54\n",
        "# ax.plot(epoch_count, loss_values)\n",
        "# ax.set_title(\"Training Loss Curve [EPOCHS]\", fontsize=20)\n",
        "# ax.set_xlabel(\"epoch number\", fontsize=14)\n",
        "# ax.set_ylabel(\"loss value\", fontsize=14)\n",
        "# ax.tick_params(axis='both', labelsize=14)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8TB_fU0f1jh"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")\n",
        "# fig, ax = plt.subplots()\n",
        "\n",
        "# ax.plot(epoch_count, val_loss_values)\n",
        "# ax.set_title(\"Validation Loss Curve [EPOCHS]\", fontsize=20)\n",
        "# ax.set_xlabel(\"epoch number\", fontsize=14)\n",
        "# ax.set_ylabel(\"loss value\", fontsize=14)\n",
        "# ax.tick_params(axis='both', labelsize=14)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ2c6YzKSuFU"
      },
      "outputs": [],
      "source": [
        "# # Visualization of epoch to epoch progress while training\n",
        "# plt.style.use(\"seaborn-v0_8\")\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(epoch_count, loss_values)\n",
        "# plt.plot(epoch_count, val_loss_values)\n",
        "# plt.title(\"Training and Validation Loss Curves\", fontsize=20)\n",
        "# plt.xlabel(\"epoch number\", fontsize=14)\n",
        "# plt.ylabel(\"loss value\", fontsize=14)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2BXTyFrPxzy"
      },
      "outputs": [],
      "source": [
        "# Load data from CSV into a DataFrame\n",
        "file_path = save_progress_path  # Replace with your CSV file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Plotting the line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['epoch'], df['train_loss'], marker='o', linestyle='-', color='b', label='Train Loss')\n",
        "plt.plot(df['epoch'], df['val_loss'], marker='o', linestyle='-', color='r', label='Validation Loss')\n",
        "\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOQs9Sy2VBHR"
      },
      "source": [
        "# Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_QmgkWSGPsu"
      },
      "outputs": [],
      "source": [
        "# checkpoint = torch.load(es.BEST_MODEL_PATH)\n",
        "# print(es.best_score)\n",
        "# model.to(DEVICE)\n",
        "# model.load_state_dict(checkpoint[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnM0qtplSwyW"
      },
      "outputs": [],
      "source": [
        "# model.to(DEVICE)\n",
        "# model.eval()\n",
        "# idx = 1995\n",
        "# mask = val_dataset[idx][\"mask\"].unsqueeze(0)\n",
        "# print(\"Mask Shape: \",mask.shape)\n",
        "# image = val_dataset[idx][\"image\"].unsqueeze(0)\n",
        "# print(\"Image Shape: \",image.shape)\n",
        "# raw_output = model(image.to(DEVICE))\n",
        "# print(\"raw output shape: \", raw_output.shape)\n",
        "# print(\"------ Raw Output ------\")\n",
        "# print(raw_output)\n",
        "# print(\"------- Pred Probs -------\")\n",
        "# pred_probs = torch.sigmoid(raw_output)\n",
        "# print(pred_probs)\n",
        "# print(\"------- How does the Mask look like? ------\")\n",
        "# print(mask)\n",
        "# print(\"-------- Predicted Segmentation mask --------\")\n",
        "# # binarizer_fn = TripletMaskBinarization(triplets=[[0.7, 600, 0.3]])\n",
        "# # mask = binarizer_fn.transform(mask).float()\n",
        "# # print(segmentation_mask)\n",
        "# segmentation_mask = (pred_probs > 0.4).float()\n",
        "# print(segmentation_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cALrPDiZFyhu"
      },
      "outputs": [],
      "source": [
        "# mask.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prdvPG3XZiuL"
      },
      "outputs": [],
      "source": [
        "# segmentation_mask.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAzD4mVaTYJg"
      },
      "outputs": [],
      "source": [
        "# plt.style.use(\"classic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak3eAHrDdY_V"
      },
      "outputs": [],
      "source": [
        "# first_channel_tensor = image[0, 0, :, :]\n",
        "# print(first_channel_tensor.shape)  # torch.Size([512, 512])\n",
        "# second_channel_tensor = image[0, 1, :, :]\n",
        "# print(second_channel_tensor.shape)  # torch.Size([512, 512])\n",
        "# third_channel_tensor = image[0, 2, :, :]\n",
        "# print(third_channel_tensor.shape)  # torch.Size([512, 512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRJzqWT1drZv"
      },
      "outputs": [],
      "source": [
        "# plt.title(\"First Channel Image\")\n",
        "# plt.imshow(first_channel_tensor.detach().cpu().numpy(), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS4Lttr2TLad"
      },
      "outputs": [],
      "source": [
        "# plt.imshow(mask.squeeze().detach().cpu().numpy(), cmap = 'gray')\n",
        "# print(mask.squeeze().shape)\n",
        "# plt.title(\"Mask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X9nSoL1TNw_"
      },
      "outputs": [],
      "source": [
        "# print(segmentation_mask.squeeze().shape)\n",
        "# plt.imshow(segmentation_mask.squeeze().detach().cpu().numpy(), cmap='gray')\n",
        "# plt.title(\"Segmentation Mask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi-kZiELUV79"
      },
      "outputs": [],
      "source": [
        "# print(type(pred_probs), type(mask))\n",
        "# print(pred_probs.shape, mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKAY7_pFh_Vt"
      },
      "outputs": [],
      "source": [
        "# print(segmentation_mask.squeeze().squeeze().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm9Qm1_FiFbS"
      },
      "outputs": [],
      "source": [
        "# print(mask.squeeze().squeeze().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kiIFEw2TPZn"
      },
      "outputs": [],
      "source": [
        "# # \"\"\"\n",
        "# # Both methods aim to capture the overlap between predicted positive regions and actual positive regions in the ground truth.\n",
        "# # The dice_metric approach leverages the full range of predicted probabilities, while the  metric function relies on a binary classification based on a chosen threshold.\n",
        "# # \"\"\"\n",
        "# # Dice = metric(pred_probs.detach().cpu(), mask).item()\n",
        "# # print(f\"Dice coefficient: {Dice}\")\n",
        "# # dice_metric_score = dice_metric(pred_probs.detach().cpu(), mask)\n",
        "# # print(f\"Dice coefficient: {dice_metric_score}\")\n",
        "\n",
        "# \"\"\"\n",
        "# Both methods aim to capture the overlap between predicted positive regions and actual positive regions in the ground truth.\n",
        "# The dice_metric approach leverages the full range of predicted probabilities, while the  metric function relies on a binary classification based on a chosen threshold.\n",
        "# \"\"\"\n",
        "# Dice = metric(segmentation_mask.detach().cpu(), mask).item()\n",
        "# print(f\"Dice coefficient: {Dice}\")\n",
        "# dice_metric_score = dice_metric(segmentation_mask.squeeze().squeeze().detach().cpu(), mask.squeeze().squeeze(), per_image=False)\n",
        "# print(f\"Dice coefficient: {dice_metric_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLi3ZH-8ALZM"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P0OtlxdALZN"
      },
      "outputs": [],
      "source": [
        "# from torchvision.transforms import Resize\n",
        "# from torchvision.utils import make_grid\n",
        "\n",
        "\n",
        "# def preprocess_image(image, target_size):\n",
        "#     transform = Resize(target_size)\n",
        "#     image = transform(image)\n",
        "#     return image\n",
        "\n",
        "# def generate_predictions(model, images):\n",
        "#     with torch.inference_mode():\n",
        "#         predictions = model(images.to(DEVICE))\n",
        "#         predictions = torch.sigmoid(predictions)\n",
        "#         return predictions\n",
        "\n",
        "\n",
        "# def visualize_predictions(images, target_masks, preds):\n",
        "#     images = images.cpu() # shape: [8, 3, 1024, 1024] or [8, 3, 512, 512]\n",
        "#     target_masks = target_masks.cpu() # shape: [8, 1, 1024, 1024] or [8, 1, 512, 512]\n",
        "#     preds = preds.cpu() # shape: [8, 1, 1024, 1024] or [8, 1, 512, 512]\n",
        "\n",
        "#     fig, axes = plt.subplots(2, 3, figsize=(20, 30))\n",
        "\n",
        "#     for i in range(2):\n",
        "#         # axes[i, 0].imshow(images[i].permute(1, 2, 0))\n",
        "#         axes[i, 0].imshow(images[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 0].set_title(\"Input Image\")\n",
        "#         # axes[i, 0].axis(\"off\")\n",
        "\n",
        "#         axes[i, 1].imshow(target_masks[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 1].set_title(\"Target Mask\")\n",
        "#         # axes[i, 1].axis(\"off\")\n",
        "\n",
        "#         axes[i, 2].imshow(preds[i, 0], cmap=\"gray\")\n",
        "#         axes[i, 2].set_title(\"Model 512 prediction\")\n",
        "#         # axes[i, 2].axis(\"off\")\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPbHLrDvALZN"
      },
      "outputs": [],
      "source": [
        "# \"\"\"----------------------------- Inference --------------------------------\"\"\"\n",
        "\n",
        "# metrics512 = defaultdict(float)\n",
        "# for b_idx, data in enumerate(val_dataloader):\n",
        "#     batch_images = data[\"image\"] # shape: [8, 3, 1024, 1024]\n",
        "#     images_resized_512 = preprocess_image(batch_images, (512, 512))\n",
        "\n",
        "#     # Generate predictions\n",
        "#     preds = generate_predictions(model, images_resized_512)  # shape: [1, 1, 512, 512]\n",
        "#     print(\"preds - Shape: \", preds.shape, \" || device: \", preds.get_device())\n",
        "#     seg_mask_ensemble_512 = preds > 0.4\n",
        "#     # mask preprocessing\n",
        "#     batch_masks = data[\"mask\"]\n",
        "#     masks_resized_512 = preprocess_image(batch_masks, (512, 512))\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"---------------------------- Dice Scores -----------------------------------\")\n",
        "\n",
        "\n",
        "#     dice_scores_512 = dice_metric(seg_mask_ensemble_512, masks_resized_512.to(DEVICE), per_image=True)\n",
        "#     print(\"dice_scores_512 - Shape: \", dice_scores_512.shape, \" || type: \", type(dice_scores_512))\n",
        "\n",
        "#     # dice_scores_1024 = dice_metric(combined_pred, batch_images.to(DEVICE), per_image=True)\n",
        "#     # print(\"dice_scores_1024 - Shape: \", dice_scores_1024.shape, \" || type: \", type(dice_scores_1024))\n",
        "\n",
        "#     print(\"Dice scores of predictions on the 512x512 scale\\n\", dice_scores_512)\n",
        "#     print(\"Dice score of the batch 512x512 scale: \", dice_scores_512.mean())\n",
        "\n",
        "#     print(\"---------------------------- SCALE = 1024x1024 -----------------------------------\")\n",
        "#     visualize_predictions(batch_images, batch_masks, seg_mask_ensemble_512)\n",
        "#     print(\"-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBJzbhDrSnfh"
      },
      "source": [
        "ffgfhhff"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Y7HFiESzDWcY",
        "Xfcsl-YW_ck9",
        "3A4XlffPf1jV",
        "7xtZrBIJeLH_",
        "A-y8ovXEK2St",
        "od_pZDUoK2Su",
        "BNLEHf3QK2Sv",
        "6-dcysMmK2Sw",
        "QPWAK3BK9UPJ",
        "Ld_aLxHH9UPK",
        "DAar3gnt9UPK",
        "7rpAsnr89UPL",
        "3wyBin1-klDc",
        "jLaynkwn9UPM",
        "gFIKfzTH9UPN",
        "YCLcdf0B9UPN",
        "sZWZ7jTw9UPO",
        "_ag6qGuR9UPP",
        "Vzze2VhIK2S5",
        "kOlM9jTEK2S5",
        "K136b3eqK2S5",
        "pOkP2HYpvsqr",
        "LcACWXgAXHQu",
        "gJFz9wAhzL4k",
        "iOQs9Sy2VBHR",
        "cLi3ZH-8ALZM"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deeplearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
